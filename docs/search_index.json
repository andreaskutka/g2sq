[["index.html", "The LSMS Guide to Survey Quality Preface About this book How to use this book About The LSMS Guideline series About the authors callout box examples", " The LSMS Guide to Survey Quality Josefine Durazo, Kevin McGee, James Arthur Shaw, Andreas Kutka Preface Hello there. Thanks for dropping by. However, you have come a bit early. This book is still work in progress and in a pre-publish state. PLEASE DO NOT SHARE. We look forward to seeing you back here in a few months, when everything is ready. Version 0.1. Last updated on 05 May 2022. About this book What is survey quality and how can it be achieved? This book gives practical advice experiences and practices that have worked in. based upon our experience in designing and implementing large scale socio-economic surveys in a variety of contexts. recognises the multi-dimensionality of survey quality and follows the total survey error paradigm total survey quality optimization The book is targeted towards a range of survey practitioner roles. Survey designers, sponsors will find guidance on design decisions that are relevant for quality in chapters X and Y. They might also benefit from reading the introduction chapters to the following chapters. Survey implementer will find tips &amp; tricks and best practices in chapters Z and ZZ to maximize quality for all main steps of survey implementation. Individual sections provide details for individual key roles, including trainers, translators, etc. The book provides detailed steps and best practices for all survey phases. Survey practitioners are not encouraged to implement everything, as this would be beyond most surveys budget and time constraints, and as there might be alternative methods with equally good outcomes. Survey practitioners can use the detailed information for those components that form part of their quality assurance plan, or use the detail to reflect on or improve own practices. The book is a companion and go-to reference for survey practitioners aiming to improve the quality of the surveys they design and implement. The book follows the survey life cycle, and explaining the best practices and providing tips and tricks on how to achieve high quality surveys. The book is very comprehensive on each of the survey phases. Readers are not expected to implement all how-to-guidelines listed in the book. Instead, the book is meant to be a collection best practices to achive high quality in surveys. Survey practitioners are encouraged to evaluate which improvements can be implemented within the context and constraints of their respective survey. Follows the total survey quality framework, and touches base on the survey design, implementation and analysis. The book is written as companion for survey managers to help them take the right decisions, organize their team and works streams and how to obtain the right types of inputs in every phase of the survey to achieve high quality outputs. While providing a lot of detail in some parts, the book does not aim to be a comprehensive how-to-survey guide. Further guidelines and the input of experts are needed to achieve high quality surveys. The tips and processes described in the book are not necessarily the only way of doign things. They have, however, proven to be effective in conducting surveys that produce good quality data and are huzzle-free and without bad surprises. We welcome comments that point us into alternative. The book is divided into 6 parts. Part I provides a short introduction of key concepts of survey quality. It is the only theoretical part of the book. Parts II - IV follow the different stages of a survey life cycle from inception, design, preparation, training, fieldwork to post-field. Each part is comprised of several chapters. The first chapter to each part is an introduction that outlines the content of the remaining chapters and provides a cheatsheet version of our key recommendations, called TL;DR. References for each chapter are added to the bottom. In the Appendix, you can find the list of Abbreviations and Acronyms, as well as a complete list of all references used throughout the book. The book is a living document that is being expanded and revised on an ongoing basis. Whenever we become aware of new best practices, we will try to integrate them into the book. The last update was made on 05 May 2022. Book is often written with reference to a LSMS-style household surveys implemented in cAPI (Survey Solutions) as most surveys are now. However, almost everything in the book applies generally to most survey types or other software packages. How to use this book The book provides a lot of information and details that are probably difficult to absorb entirely and retain for the duration of the survey. For those who are managing one of their first surveys we recommend skim reading the document focusing on the introductions to each chapter and skipping the details. They summarize the key points and provide you with a general idea of what matters to achieve survey quality. Once the survey is underway, reading the relevant chapters prior to thinking about a phase will give you all the details you need at this point. Experienced survey managers who are curious about or aiming to improve certain aspects can read the relevant chapters only. The book is relatively extensive and contains a lot of detail. It is not written to be read in one go (we applaud your stamina if you do), but rather as go-to reference that can be visited during any stage of the survey to get relevant practical advice and learn about best practices. Chapters are written to be independent of each other. Links to other chapters are added to where relevant information is References are provided if they built upon other parts of the book or if more detail is provided elsewhere. Use the search function to quickly find all occurrences of a word or phrase. Open the search function by clicking on the search icon on top or by typing f on your keyboard. Use the up and down arrow keys to navigate between different search matches. You can edit the book! Yes, you read this correctly. If you come across anything that you think could be improved, from correcting typos to adding content to chapters, click on the edit icon on top. This will open the underlying file on github where you can make the modifications and submit them as a pull request (you will need a GitHub account). The book is written using Rmarkdown in bookdown. For most parts of the text, the syntax is following simple markup language, of which you can learn the basics in under 2 minutes. (For more experienced github &amp; Bookdown user, you can fork the depository and make commits). If accepted, your suggestions will be added with the next version. If you have any suggestions but prefer to not edit the book, please list an issue on GitHub. Thank you! You can share the book or any page you like using the sharing icons on the top right. Each chapter and subchapter have specfic links. Copy the link, either by right clicking on the blue # icon next to the heading and copying the link address, or by clicking on the (sub)chapter in the table of content and copying the url from the browser. About The LSMS Guideline series The LSMS Guidebook series offers information on best practices related to survey design and implementation. While the Guidebooks differ in scope, length, and style, they share a common objective: to provide statistical agencies, researchers, and practitioners with rigorous yet practical guidance on a range of issues related to designing and fielding high-quality household surveys. The series draws on experience accumulated from decades of LSMS survey implementation, the expertise of LSMS staff and other survey experts, and new research using LSMS data and methodological validation studies. About the authors Josefine Durazo  Kevin McGee  James Arthur Shaw  Andreas Kutka . callout box examples Did you know that  this book is work in progress and in a pre-publish state. something long and longer ahhds oh yeahaaahjkl Did you know that  this book is work in progress and in a pre-publish state. something long and longer ahhds oh yeahaaahjkl Warning  this book is work in progress and in a pre-publish state. something longer Warning  this book is work in progress and in a pre-publish state. something longer and longer ajetztabera test DO NOT SHARE! The book is work in progress and in a pre-publish state. "],["introduction-to-survey-quality.html", "Chapter 1 Introduction to Survey Quality", " Chapter 1 Introduction to Survey Quality Sample surveys are a crucial data source for official statistics, impact evaluations and social economic research. Survey data informs public policy and other important decisions. Quality in surveys is paramount for good policies and sound decisions and should be the aspiration of all survey practitioners. In Part I of this book we summarize key frameworks used by survey methodologists and national statistical offices (NSOs) to conceptualize and maximize quality in surveys. Skip this part if you are not interested in the theoretical background of survey quality. We will link back to the concepts throughout the remainder of the book, so you can read only the parts that are relevant. Quality can be defined as fitness for use. For something to be fit to use it must be free from deficiencies and respond to the users needs (Juran and Gryna 1980). In a survey context, freedom from deficiencies translates to the requirement of survey data to be accurate to allow inferences about the target population. In other words, a survey statistics must accurately describe the population for it to have any value. Responsiveness to users needs translates into aspects such as timeliness, usability and completeness that are important to the user of survey data. Even the most accurate data can be unusable if it is outdated, poorly documented or not rich enough for the intended analysis. Which user attributes matter, depend on survey. As we can already see, survey quality is a complex, multidimensional concept. In Chapter 2 we look at Total Survey Error (TSE). It is the dominant framework used by survey methodologists to maximize the accuracy of survey estimates within survey constraints. We describe different sources of survey error and provide real-world examples for each error source. We explain how error affects survey statistics and how the TSE can be used as a planning criterion to reduce overall error in survey estimates. In Chapter 3 we introduce the notion of Total Survey Quality (TSQ) that combines survey accuracy with user-specific dimensions of quality. The concept is the basis of multi-dimensional survey quality frameworks that are now used by many national statistical offices as a planning and evaluation tool. We present a general strategy how TSQ can be used to design surveys that maximizes survey quality. The chapters are based on the articles by Biemer (2010) and Groves and Lyberg (2010), and the books by Biemer and Lyberg (2003) and Weisberg (2005). They are good starting points for readers who are interested in learning more about the theory of survey quality. References "],["tse.html", "Chapter 2 Total Survey Error 2.1 Error sources 2.2 Survey-related Effects 2.3 The Impact of Error 2.4 Minimizing TSE", " Chapter 2 Total Survey Error In the survey context error refers to the deviation of a survey response from its underlying true value. Probably the best-known error type is sampling error that occurs when collecting information from a sample of the population rather than the full population. Early in the history of probability sample surveys, it was recognized that there is a variety of non-sampling error sources that can equally cause a survey statistic, often referred to as survey estimator, to deviate from its underlying true value in the population, referred to as population parameter. Over time, this knowledge evolved into the concept of Total Survey Error (TSE), which has become the dominant paradigm in the field of survey methodology Groves and Lyberg (2010). TSE acknowledges that error can arise from many different sources. For example, error might be introduced by researchers formulating questions wrongly, interviewers not probing sufficiently or respondents unwilling to respond. Error may occur at any point in the survey process, including the design, sampling, implementation, processing or analysis, and may be caused by any actor including researchers, interviewers or respondents. TSE denotes the aggregate of all error sources in an estimate and is thus the difference between a survey statistic and the population parameter. A good presentation of how TSE components link to the process of generating a survey statistic has been made by Groves et al. (2004), which is shown in Figure 2.1. Figure 2.1: TSE Components Linked to Steps in the Measurement and Representational Inference Process 2.1 Error sources The TSE framework decomposes survey error into different error sources. Survey methodologists have produced a number of similar classifications. They divide TSE into sampling error and non-sampling error, that in turn can be further decomposed. We follow the classification developed by Biemer and Lyberg (2003). We briefly introduce each error source and provide real-world examples that we have observed in our work with NSOs, international organisations and survey firms. 2.1.1 Sampling error Sampling error describes the error that occurs if a sample of the population is surveyed, instead of surveying the entire population, i.e. conducting a census. It is determined by the sampling scheme (simple, multi-stage, stratified, ), the sample size and the choice of estimator. If probability sampling is being used, the sample error is randomly distributed and can be mathematically computated. It is probably for that reason that sampling error is receiving a disproportionate high level of attention by researchers and survey designers. By definition, sampling error occurs in every sample survey. Sampling error becomes problematic if it is unnecessarily large due to faulty or inefficient sampling design or is minimized at too high expenses of other error sources. If non-probability sampling methods are being used, sampling error may also introduce bias. Examples include: The so-called Random walk is an oxymoron. Interviewers can easily tweak the sequence in which they list units, so that respondents are selected that meet certain characteristics, such as being available for an interview or being friendly. Random walk is a not a probability sampling method and introduces bias. Unfortunately, some surveys still use it for a fast and cheap second-stage sampling. A HIES used convenience sampling to select individuals within collective living quarters as respondents were unable to list all individuals living there. Interviewers selected whoever was available during their visit, effectively removing individuals from the sample that were unavailable during working hours, thus introducing bias. A probability sample method could have been used instead, for example, by listing all lockers or beds or other items associated with individuals, randomly selecting items from the list and putting effort into interviewing the associated individuals. A survey part of an impact evaluation study selected a varying number of households per community, depending on the community size. In the largest communities, up to 40 households were interviewed. While this sample design does not increase sampling error nor introduces bias, it is inefficient. Beyond 20, the marginal decrease of sampling error per additional household per community is negligible. The resources spent to interview them could have been used to decrease error from other sources instead, for example by extending training or increase monitoring. 2.1.2 Specification error Specification error (referred to as Validity in Figure 2.1 occurs when there is a misalignment between the concept a survey intends to measure and the concept that is actually being measured. As a consequence, the wrong parameter could be estimated by the survey and invalid inferences be made. It is often caused by insufficient definition of concepts, poor questionnaire design, lack of cognitive testing, absence of good manuals, or concepts not being preserved during questionnaire translation or CAPI scripting. Examples of specification error are: Surveys that aim to capture income from any kind of employment sometimes ask opening questions like In the past , did NAME work as an employee for a wage, salary, commission, or any payment in kind?. Designers and data users assume that this question captures informal jobs. Often however, respondents understand this question to ask about formal employment only and answer with No, even though they have worked as day laborers for a few days. The question does not capture the concept of employment as planned by designers or interpreted by analysts. Researchers or questionnaire designers developing questions without taking into consideration local circumstances or testing that underlying concepts are understood the same way in the population. For example, questions around saving behavior tend to be delicate and highly dependent on economic circumstances. Researchers assuming that goods bought on credit are included in a loan roster, while the questionnaire did not explicitly probe for them and trainers did not train interviewers to include them. Mistakes that happen when updating translations or instruments. last-minute change is made to the questionnaire during the training, in the local language that never finds its way into the original version in the design language which is being used as data reference. Data users will interpret the question differently to how it was being asked. Household to be translated as family in the local language, referring to a different set of people. Household refers to the economic unit and does not necessarily imply family ties. Many languages lack a word for household. An expression like those living with you would often be more appropriate. Sometimes, CAPI scripting can alter the nature and functionality of questions or modules to such a degree that it affects the way they are being administered and ultimately changes results. Researchers who do not observe the CAPI questionnaire being fielded remain unaware of such differences. For example, time use modules are often designed as a grid of activities and hours of the days into which lines are drawn for the primary activity done for a certain period. We have seen this module being scripted in CAPI as series of 48 questions asking the respondent for the main activity for each 30-minute interval. The responses produced are not at all comparable. DHS and Is it OK to beat your wife question NOTE: ARTHUR TO WRITE Different household definitions, different results? 2.1.3 Coverage error Coverage error or sometimes frame error occurs if the sampling frames that are used to select the survey sample include units that are not part of the target population, exclude units that are part of the population or duplicate them an unknown number of times. It also refers to errors in auxiliary variables associated with the frame units, or if they are missing. Frame error can cause parts of the population to be under- or over-covered, distorting the sample or impact evaluation design. Since building perfect frames is often practically not possible, most surveys suffer from frame error to some degree. More severe cases of frame error often stem from a lack of quality assurance of listing exercises, the use of unverified lists compiled by third parties, or the use of outdated lists. Examples are: During listing exercises listers sometimes focus on the population-dense village centers, as it is quick and easy to list a large number of households, but omit households that live further away or are difficult to reach. As a result, remote households are under-covered in the frame. Community lists are usually being used as frame for first-stage sampling in household surveys or to inform impact evaluation design. In some contexts, these lists can be of very poor quality. We have experienced field teams searching for non-existent communities or that community characteristics grossly mistmatch the auxiliary variables used for sampling or matching. A survey for an impact evaluation in Per√∫ used household lists for the second stage sampling in treatment areas that had been compiled by the project to be evaluated. Several local dynamics played into the creation of the lists. As a result, in many villages, several members of one household were listed as separate households on the list, friends and family of project officials from outside the village were included, or beneficiaries from the village excluded. 2.1.4 Nonresponse error Nonresponse error can occur on the unit level, when a sampled survey unit (e.g. household, individual, firm, etc.) cannot be interviewed for any reason, or on the item level, when parts of the questionnaire remain unanswered because the respondent did not answer some questions for any reason. Nonresponse error can severely affect the validity of survey data. Since the reasons for nonresponse are hardly ever randomly distributed, the actual respondents may no longer be representative of the population, causing the survey estimates to be biased. Nonresponse error has been plaguing survey methodologists in the global North, where nonresponse levels generally have been very high over the past decades Surveys in the global South have so far largely been spoiled with very high response rates, but nonresponse error is increasingly becoming an issue, especially in urban areas, where availability and willingness to participate in surveys is decreasing. While various approaches have been developed that aim to correct for nonresponse error, the best way to deal with it is to put solid measures in place to avoid it as much as possible. QUOTESNEEDED. Examples of nonresponse error are: Survey teams visiting communities during working/market hours and replacing unavailable households without making sufficient revisiting attempts on different days and hours. In a survey in urban South Africa, many interviewers were scared of dogs and replaced households if they owned a dog. Dog ownership was (initially) not observed and highly correlated with other household characteristics. The below average probability of Donald Trump supporters to respond to interview requests is thought to be the main reason that surveys systematically underestimated the support for Donald Trump ahead of the 2020 US presidential elections. In a COVID response phone survey, interviewers were unable to call those panel respondents who had not paid their phone bills and had their line (temporarily) cut off. Long questionnaires that cause respondent fatigue and lead to high rates of uncomplete interviews, affecting the last sections of the questionnaire. High Dont Know (DK) rate for income related question if interviewers do not probe and explain the question sufficiently. Respondents refusing to participate in the survey. This is happening increasingly often in some context, such as urban areas. Interviewers can be trained in best practices to convince respondents to participate. 2.1.5 Measurement error Measurement error often is one of the most damaging error sources in a survey. It occurs if the recorded value is an inaccurate measure of what was to be measured. They can be due to the interviewer, the respondent, the questionnaire, protocols or their interaction. They might be intentional or unintentional. Respondents might misinterpret a question, struggle to recall some information or deliberately give a wrong response. Interviewers might administer a question incorrectly, misunderstand questions or answer option, record typos, falsify responses or cause measurement error in many other ways. Poorly designed questionnaires, ambiguous questions, underdefined concepts often are big contributing factors to measurement error, as can be the interviewing mode. Some interviewers in an LSMS survey confused KG and Gram and selected the wrong answer option in the consumption unit. As a result, the food quantities they recorded were off by a factor of 1000. Respondents who purposefully under-report household members, crops, livestock or other items, knowing that each item they report will entail a long set of follow-up questions. Interviewers recording responses against the wrong questions or for wrong items due to confusing CAPI design that fails to provide them with a good overview of the questionnaire. A school survey conducted teacher interviews and recorded teacher attendance in classroom. Reason for absence was not recorded and no protocol was put in place to ensure both exercise were conducted during different hours. In a follow-up survey that corrected for both problems, 8% of the teachers were absent from classroom for being interviewed at the time. Agricultural surveys trying to capture very detailed information on the parcel-crop-season level can be too detailed for respondents to recall, or for interviewer and respondents to get lost in the conversation. 2.1.6 Processing error Processing error refers to any error occurring post-interview including error in data entry, editing, formatting and labeling, construction of indicators, calculation of survey weights, or tabulation of results. It is often caused by unclear interview rejection mechanisms, data editors who are not qualified enough, improper keeping of records or change logs. Examples of processing error we have observed are: Interviewers fixing issues in rejected interview files without recontacting the respondent or obtaining the correct answer. Wrong or non-systematic outlier detection, such as manually summarizing variables in statistical software and looking at the 5 highest and lowest values only. A NSO replaced the outlier values in any variable with its median value prior to data publication. Not correcting for changes to the instrument when appending the data from different versions. For example, f there has been a change to answer options or item lists, this can cause value labels to be assigned wrongly for parts of the sample. Wrongly label value codes, such as household assets. 2.2 Survey-related Effects Conducting a survey in the way it is being conducted, in its context, has itself effects on survey statistics. While these effects are often implicitly included in TSE and referred to as error, Weisberg (2005) uses the term survey-related effects and explicitly includes them into the framework. Examples of survey-related effects are: Interviewer effect: Interviewer characteristics such as mannerism, tone of voice, gender, ethnicity, etc. have shown to influence the type of response they elicit. This effect on survey statistics is not due to any mistakes or wrongdoing by the interviewers, and will exist for any survey using interviewers. Mode effect: Administering a question in face-to-face interviews can produce very different response patterns compared to asking the same question in telephone interviews or self-administered interviews. This, for example, can be due to respondents attitudes being different if they receive an interviewer at home (guest) to being called by phone (solicitor/marketer). In multi-mode surveys or with modality changing between rounds, mode effect can cause significant comparability issues. Questionnaire-related effects: Answers patterns can differ with the exact question phrasing. As long as there is no specification error, there is no question phrasing that is more correct than others. Similarly, the order in which questions are asked can change outcomes. Yet, they have to be aske in some order. The same is true for the phrasing and order of answer options. A well established example is the recency effect in phone surveys, where respondents select the last answer option because they remember it best. While some of the survey-related effects can be minimized, e.g. through randomization of question or answer option order or careful field team planning, they cannot ever be eliminated completely as they are an inevitable product of a survey itself. For example, since a survey requires interviews to be conducted somehow, mode effect will always exist in a survey. In some cases, it is possible to study and quantify effects in experimental design and to make ex-post adjustments. This, however is costly and requires provisions in the survey design. In practice, usually the best way to deal with survey-related effects is to be aware of them and keep them in mind during survey design and analysis. 2.3 The Impact of Error Sample surveys aggregate individual responses to obtain statistics for the sample, referred to as survey estimator, often with the aim to infer corresponding values in the population, referred to as population parameters. Survey error causes the survey estimate to deviate from the population parameter, diminishing the accuracy of the inferences derived from the survey data. In other words, with high levels of error a survey does not accurately describe the reality of the population. Error can affect survey estimators by increasing the variance of a variable or by introducing bias. For a survey estimator to be accurate it has to have a small bias and variance, which only happens if the TSE is low for the estimate. The ways in which error affects an estimate depend on whether the error is random or systematic, and whether it is uncorrelated or correlated. Error is considered random if it does not follow any pattern. For example, if respondents have difficulty recalling an item, some respondents might give higher values while others provide lower values. Across all observations, the error would have a mean of zero and would not affect the mean value of the variable. However, random error increases the variance of a variable, which in turn reduces the reliability of a survey estimator. The higher the variance, the higher the probability that the estimator would be different if the survey were to be repeated under the exact same conditions. The results would be less reliable. Increased variance furthermore reduces the magnitude of correlations with other variables and the statistical power in hypothesis tests. If error contains a systematic tendency we speak of systematic error or bias. As an example, if interviewers tend to under-report household members to shorten interview duration, the survey estimate of household size will underestimate the actual household size in the population. In statistical terms, the sample mean would be a negatively biased estimator of the population mean. Since systematic error directly affects the mean of a variable, it reduces the validity of the estimator, in other words the estimator is not accurately measuring what it is supposed to. If the systematic error is not constant, it may furthermore increase the variance of the variable, also reducing the reliability of the estimator. Figure 2.2 illustrates how variance and bias relate to the reliability and validity of a survey estimator. Imagine the midpoint of each image to represent the true population value and the black points to be the recorded responses. The points in A are scattered around the midpoint with no particular pattern, the error is random. We have high variance but no bias. The estimator is valid but not reliable. In B, the answers are offset in the same direction and by the same distance. The error is systematic, so there is bias, but variance is small. The estimator is reliable, but not valid. C shows systematic error that is not constant. We have high variance and bias. The estimator is neither valid nor reliable. In D, there is little error, we have low variance as answers are relatively close to the actual value, and there is no bias as the deviations show no pattern. The estimate is valid and reliable. Figure 2.2: Validity and realiability in survey estimators The magnitude of the effect of error depends on whether the error is uncorrelated or correlated. Error is uncorrelated if the error for different units (e.g. households) is unrelated. An example would be occasionally typos by interviewer when recording answers, if typos do not occur more frequently with certain types of households or some interviewers. Above discussion on the effects of random and systematic error was based on uncorrelated error. As we saw, the increased variance and bias can have serious effects on statistics. If the error for different units is related, we speak of correlated error. For example, interviews conducted by one interviewer who has misunderstood a question and administered it wrongly will contain the same error, while those of another interviewer may not. Correlated error has much more damaging effects on estimators as it multiplies the variance of a variable. Biemer and Lyberg (2003) show that only a moderate intra-interviewer correlation of \\(\\rho_{int} = 0.03\\), can result in an interviewer design effect \\(\\mathit{deff_{int}}\\) as large as 2.47. In other words, the intra-interviewer correlated error alone can cause the variance of a variable to be increased by 1.5 times. Error is typically also correlated for other survey roles, such as supervisors, data monitors or editors. One data monitor, for example, might thoroughly review interviews and provide useful feedback , reducing mistakes and measurement in their field teams while another data monitor only glancing over interviews will miss many of the issues and fail to reduce measurement error. 2.4 Minimizing TSE The TSE framework implies that the TSE affecting a survey estimate should be minimized in order to maximize its accuracy. Two points are important to note here. First, the total survey error needs to be minimized, that is the aggregate of all error sources. Errors from different sources do not occur in isolation, but are interdependent as they are all part of the overall survey process. Minimizing error from one source may negatively affect error from other sources. As an example, while increasing the sample size will reduce sampling error, it may not be a good overall strategy to increase the precision of a survey estimator. A larger sample can require more interviewers to be trained and monitored, increasing other non-sampling error types that can outweigh the gains from reduced sampling error. Another example can be complex parts of a questionnaire, such as a time-use module, that have been optimized for PAPI, but may cause significant error if implemented in CAPI without being carefully adapted to the functionality of the CAPI package being used. Understanding how decisions in one phase affect error in another requires a comprehensive view of the entire survey process. In order to effectively minimize TSE, different survey phases and components have to be integrated to operate as as a coherent whole. The second point to note is that TSE is to be minimized, meaning to be reduced as much as possible within survey constraints. All surveys face constraints such as the available budget, time and human resources or ethical considerations. Attempting to eliminate all error would exceed the constraints of any survey. Even with unlimited resources, some error sources could never be eliminated fully, such as respondents unwillingness to disclose some information causing item non-response. Instead, surveys should strive to avoid the most damaging errors and control others to the extent that they are mostly inconsequential and tolerable. How can survey practitioners prioritize which errors to address? A key challenge is that it is very hard to quantify survey error or its non-sampling components. This is only possible if the underlying true population value is known or if methodological experiments can be built into the survey design, which is an option for most surveys. In practice, minimizing TSE requires a detailed understanding of the main causes of survey error, their relative impact on accuracy, means to control them and the required effort. This knowledge is often anecdotal and based on experience in survey implementing institutions or individuals who have honed their error awareness, identification and correction over time. While there are some scientific publications on this topic, they tend to be theoretical or isolate and a address a single error source. Survey practitioners involved in the day-to-day implementation tend to not publish their practices. References "],["tsq.html", "Chapter 3 Total Survey Quality", " Chapter 3 Total Survey Quality As we have seen in Chapter 2, low levels of TSE are necessary for a survey estimator to be accurate. While accuracy is necessary for quality, it is not sufficient. Survey data must also respond to user needs in order to yield useful results. Data users often take data accuracy as given, assuming that it has been controlled by data producers. Instead they prioritize properties such as data being rich in detail, easily accessible and well documented. Data that is not sufficiently detailed, difficult to access or hard to interpret may be unfit for use and be perceived to have low quality by data users, even if being accurate. The concept of total survey quality (TSQ) considers the fitness of use of a survey estimate. It combines the accuracy of an estimate with non-statistical dimensions oriented towards the data user. The definitions of survey quality used by most NSOs in Europe, North America and Oceania, as well as that of international organisations such as Eurostat, IMF, and OECD explicitly acknowledge the multi-dimensional nature of survey quality. These definitions are referred to as survey quality frameworks. There is some (often minor) variation between organisations, but most frameworks include a subset of the quality dimensions summarized in Table 3.1 (Biemer 2010). Table 3.1: Common Dimensions of a Survey Quality Framework. Dimension Description Accuracy Total survey error is minimized Credibility Data are considered trustworthy by the survey community Comparability Demographic, spatial, and temporal comparisons are valid Usability/Interpretability Documentation is clear and metadata are well-managed Relevance Data satisfy users needs Accessibility Access to the data is user friendly Timeliness/Punctuality Data deliveries adhere to schedules Completeness Data are rich enough to satisfy the analysis objectives without undue burden on respondents Coherence Estimates from different sources can be reliably combined These frameworks are commonly used as the basis of survey quality reporting. Some of the dimensions are qualitative, making it difficult to generate a single metric to summarize the overall quality of a survey. Survey methodologists have not (yet) put forward a standard quality measure. Instead, evaluations tend to identify the strength and weaknesses of a survey by dimension and assess how well it achieved the goals of each dimension. Another very useful application of survey quality frameworks is as a design principle to maximize the TSQ in a survey. References "],["introduction-to-inception.html", "Introduction to Inception", " Introduction to Inception make a good plan, and adjust throughout the project. work in progress optimal survey design, reduces error make a plan to survey quality maximization, e.g minimize TSE while keeping the other dimensions acceptable, Biemer 2010 less can be more. A reduced questionnaire doing the key variable sof interest well can e better than trying to do many and ending up doing them worse. Fewer things to train, understand and monitor. Likewise, a smaller sample with a higher response rate might be more valuable. if there are too few resources available to do the survey well, try to either obtain more or question the feasibility of the survey. plan holistically. Efficiently allocating resources to maximize survey quality requires to look at the big picture and treat each activity as part of the whole that they are as each activity affects others. Some activities might be cut that have little or no effect on survey quality, to save resources for others where it matters. E.g. to afford a longer interviewer training one might reduce field team size and extend the duration of field work, if there are no time constraints or seasonality issues. good plan: to achieve timeliness. Overlap activities that can be done together. For example, with CAPi survey and a fixed data output format you can start working on the data documentation, cleaning, or even to some extend the analysis while the survey is still in the field. Do not overlap activities that depend on each other. For example, translation of instruments should only start once the instrument and CAPI coding have been finalised. Keepign track of change in overlapping activities is cumbersome, and leads in most cases to mistakes and survey erros. make gantt of key activities and draft timeline list of depending activities, e.g. only do training after instrument has been finished. Draw upon the experiences from other surveys in the same context, subject or modality. While every survey tends to be a different, the effect can often be generalised. In other words, what worked in one survey, often works in another survey with similar parameters. Make sure to have enough survey expereince in your team! "],["schedule.html", "Chapter 4 Schedule", " Chapter 4 Schedule Make sure your schedule keeps into consideration and works around the following forseeable events: Public holidays and vacations that might limit field workers or respondents availability or affect results, e.g. increased household expenses in the week of Christmas or Eid. Seasonality effects, that need to be catered for, such as such as harvest periods or that might affect your results. Weather patters such as rainy seasons that might limit field work operations. Allow margin to cater for unforseeable events that might affect your survey, such as abnormal weather events, unexpected field worker attrition, strikes or demonstrations "],["formulation.html", "Chapter 5 Formulation", " Chapter 5 Formulation work in progress "],["personel.html", "Chapter 6 Personel", " Chapter 6 Personel work in progress remove field team effects. As shown in Chapter LINK INTERVIEWER, interviewers, supervisors, data monitors or any other level can have huge effects on the TSE. As an example, interviewers of one supervisor who thoroughly back-checks interviews with respondents will be careful to not under report household members, parcels etc, while those of a more lenient supervisor may notice that they can get away with under reporting. that is centraize processes, so they can e monitored and streamlined, to remove supervisor, or data editor effects. Examples are: - instead of reviewing interview files in field y each supervisor, let them do only the basic completion checks, and do in depth review and feedback of the interviewed by a central team of editors, that regularly debrief, receie rotate staff, so that not always the same person enters data , reviews interviews, provides feedback to one interviewer only. review the work of each data monitor, editor, superviisor, for data entry, rotate data entry staff "],["contracting-considerations.html", "Chapter 7 Contracting considerations", " Chapter 7 Contracting considerations work in progress "],["introduction-to-preparation.html", "Introduction to preparation", " Introduction to preparation work in progress "],["fieldworker-recruitment.html", "Chapter 8 Fieldworker recruitment", " Chapter 8 Fieldworker recruitment general rule, smaller teams better, easier to keep track off, higher interviewer effect, so need to streamline and monitor. check DHS, quite comprehensive. for supervisor, data monitors, if new team, recommend to identify suitable candidates during the training when abilities and personality become clearer. Beware of experienced trainees coming from other surveys. Standards might have been lower in their previous training and they may have developed some undesirable habits that you need to untrain. Also, they might have a stronger sense of I already know that and engage less. Most surveys are significantly different for even experienced interviewers to learn the concepts and protocols. MICS:  Recruitment/Selection of pool of field staff It is key to identify individuals to invite to the training. The practise of recruitment or selection of staff to invite for training differ from country to country and from survey to survey. In some situations, the implementing agency can decide to recruit a brand-new set of staff and in others, select from an existing pool of staff, either from a previous survey or actual staff on payroll. Here follows some overall recommendations to select the adequate mixture of participants and screen individuals for the training:  Gender: The protocol demands that the individual questionnaires are administered by an Interviewer of the same sex as the respondent. Depending on the sample of individual men (all households, half, one third, etc.), one or more male Interviewers will be required on each team. If the survey does not include an Individual Questionnaire for Men, then all the Interviewers must be women. However, since it is also recommended, based on experience, that all teams include members of both sexes, it is advised that, the Supervisor or the Measurer is male (if no male interviewers). It is important that all interviews can be observed by the Supervisor, but Supervisors should be advised to leave an individual interview during the most sensitive subjects, such as sexual behaviour and victimisation or other questions where an observer of the opposite sex may make the respondent uncomfortable.  Education: Normally, secondary education of trainees is a good target to bear in mind when recruiting. There are mixed experiences with university graduates: Recent graduates are often highly motivated and can be excellent Interviewers, but in other cases graduates have proven problematic by developing their own protocols in the field (being too smart). In any case, monitor performance carefully for all field staff.  Experience: Having worked in the field on other surveys is certainly helpful for Supervisors and can be equally so for other staff. However, please be careful, as other surveys may not pay as much attention to quality as the MICS or may have instituted protocols that are not recommended for MICS and are difficult to unlearn. A typical example is for Interviewers that have worked on market research that is not always conducted according to the standards of a national statistical office or MICS.  Language: All field staff must be completely fluent in the language(s) used for the training, which is typically also the language of the survey documents, such as the instructions and manuals. When deciding on the number of Interviewers fluent in other languages, it is important to have the fieldwork plan in mind, as well as a complete understanding of languages necessary in the different parts of the country. For example, if the fieldwork plan requires that just one team is fluent in a particular language, it is important that more trainees are invited than needed with this particular language skill. The rule of 10% is a rule of thumb that applies to total number of trainees, whereas it may be appropriate to invite 1-2 extra with a special language skill.  Appearance: Fieldwork is demanding, not just on physical fitness, but also, for some, on the ability to dress appropriately.  Attitude: A good candidate will show a respectful attitude and maturity and take interest in the work.  Diversity: With the demands for languages above, there is a good chance that recruiting happens across the country. However, for various reasons, there may be a natural bias towards candidates from the capital city or other major urban areas. It is important to ensure that the opportunity to apply is given across the country, perhaps through advertising that covers all regions. Even if the language requirements can be met in the capital, some candidates tend to appear so sophisticated to a rural population that a good rapport can never be established.  Avoid: It is risky to use staff currently employed for example in the health sector, both because of the issues mentioned under education and experience above and because a large part of the survey is measuring performance of the health sector and thus there is a potential conflict of interest. An objective set of requirements, based on the above, should always be developed and be transparent applied, so that the pressure the survey managers may feel to hire certain individuals can be eliminated. It is equally important that all applicants that meet the requirements are interviewed and tested as part of the selection process. This takes a lot of work and therefore planning far in advance is necessary. Very simple testing can be applied, i.e. if the candidate reads well, writes correct answers to simple questions, and can communicate in whatever languages are necessary and indicated by the applicant. There is a further need to at least check if candidates can operate simple functions in a tablet computer. Advanced computer literacy is not necessary, but a certain comfort with computers or smart phones is valuable. Additionally, measures must be able to see well (with glasses if used) as they will be reading out measurements that may be unclear in certain lighting. "],["fieldworker-terms.html", "Chapter 9 Fieldworker terms", " Chapter 9 Fieldworker terms make it fair: pay fair. Mix of daily and piece rate. Only use peice rate if control meachnisms in place to counter act effects, only use daily rate if sufficent control that do sufficnet work. due diligence: insurance for health and accidents give incentives to do well, e.g. bonus at the end if still there (to combat attrition) and based on performance put in deterrents to bad behaviour, make sure you can fire if gross mis conduct, not hire again write up short terms and conditions, including pay, DSA allowances, contract, duration, selection process, possibility of not getting selected, required tasks, duration fo field work and places of work. Make every candidate read the terms and conditions. If they agree, they should sign a copy. This is important that expectations of all trainees are correct at the beginning of the training, so trainee attricition is reduced, and to avoid collective walkouts by trainees. "],["introduction.html", "Introduction", " Introduction work in progress "],["instruments.html", "Chapter 10 Instruments 10.1 Questionnaire 10.2 CAPI/CATI 10.3 Translation", " Chapter 10 Instruments 10.1 Questionnaire discourage Dont Know responses. Dont make them very obvious by adding them to all questions. They are a tempting solution for iinterviewers. There are different types of Dont know, often they are related to respondents not makeing an effort to recall something. Intervieers should be encouraged to get respondents to answer. Only add to questions where you accept them. Instead use special codes that are displayed once at eginning of the questionanire and trained to intervieiwers. 10.2 CAPI/CATI 10.3 Translation Many questionnaires are designed in one language but fielded in one or more other languages. The way questionnaires are translated affects results, see e.g. Seo, Chung, and Shumway (2014). Interviewers speak the language and just translate on-the-go. is a big source of measurement error and interviewer effects, so make sure to translate your instruments! Using CAPI/CATI makes it easy to provide the questionnaire to interviewers in different languages. One can normally switch language within the questionnaire. If managed well, even translating to multiple languages is neither too much effort nor very costly, and a low-hanging fruit to increase survey quality. Translations done badly can quickly spiral out of control, causing tremendous amounts of work and potential mistakes. 10.3.1 What should be translated? Into which languages should I translate? Translate into all languages in which a significant proportion of the sample will be interviewed. The size of the proportion depends on the context and available resources, but can be as low as 5%. Only translate into languages that can be read fluently by interviewers. Some local languages are spoken only or have no spelling convention, making them very hard to read. Do I have to translate the whole questionnaire? Always translate question text, answer options and instructions to the respondents, as they are being read out to the respondent, or help the interviewers record their response. Translate interviewing-facing parts such as interviewer questions, instructions or warning messages if the field teams are more comfortable using a language over the design language. If they are fluent in the design language, there are only marginal benefits. Keep questionnaire structure such as names for screens, sections, rosters or the outcome variable in the design language or translate to a common language. Having a common reference makes training, management and feedback easier as everyone is literally on the same page (e.g. Parcel Listing). Add interviewer variables at the end of the instrument to record the main language in which the interview was conducted, if an interpreter was used and what the level of understanding was. 10.3.2 Who should translate? There are a few professional firms and translators that specialize in survey translations and cover a range of common languages. For many local languages these are unfortunately not an option. A sound translation of survey instruments requires translators to: be fluent in the design/source language be native speaker in the target language have survey field experience (to know survey expressions and ways to phrase questions) have good understanding of the subject (e.g. WASH, health, education) local contextual knowledge (to know how things are referred to locally) This set of skills and experience is rarely combined in one person. Translations from professional translators can sound too formal or bulky, while translations from field staff might miss the essence of a question or a construct. In the absence of professional survey translation services, the best options are often local consultants or experienced field workers who have worked with similar types of surveys before. Ideally, translation is done by a group that combines the above listed experiences and receives input where they lack, e.g. to correctly translate main toilet types or drinking water sources. It is useful for questionnaire designers to work through the instruments with the translators to make sure complex or nuanced parts and constructs are understood. A common communication channel (e.g. Email to all, WhatsApp group,) for translators to ask questions and for designers to send clarifications to all translators is important to ensure consistent translations into multiple target languages. 10.3.3 Some translation guidelines Follow below guidelines when translating survey questionnaires. Share this list with the translation team. Preserve meaning. Literal or close translations are often inadequate, especially if the target and source language and culture are distant. Find the best way to express the same meaning in the target language. Be precise. Use expressions to describe a construct if there is no word for it in the target language. E.g. use those who live with you instead of family if there is no word for household Dont omit words that provide any type of reference, such as on average,in total, main. Keep it simple. Use language and expressions that are easy to understand for all respondents in the sample. Make sure the same parts of questions are stressed. E.g. in English, the reference period is put at the beginning of a sentence to highlight it. In other languages this might be elsewhere. Adjust to different customs and culture. Direct translations might sound overly positive, negative, polite, impolite, etc. E.g. you might have to drop or add the word please. Be consistent within and across instruments. Use the same words or expressions to translate one concept in the source language. Check consistency by searching for key expressions in the translation sheet (e.g. Ctrl+f household) and checking they have been translated the same. Use established (good) surveys as references. They have already been tested and fielded in your context and often are a useful source of nomenclature. Preserve formatting, it carries meaning. Whatever is bold, underlined or UPPERCASE in the source language should be the same in the target languages. Preserve code. CAPI/CATI text sometimes contains dynamic parts that are written in code and must not be translated, e.g. piped text in Survey Solutions %MEMBER% or &lt;html&gt; tags. Use the custom Stata command sursol transcheck to make sure all Survey Solution formatting and code is correct in the target languages. Stay local. More widely spoken languages such as Spanish or Arabic can differ significantly between countries in how they call or express certain things. Use local translators, and review and adapt if using (parts of) questionnaires from other countries. Use the CAPI/CATI or a paper questionnaire in the source language on the side as guidance. Translations will depend on the context that is often not given when working in translation sheets. Give feedback if you think the source language needs updating and get clarifications if you are not sure how to interpret something. For more practical translation guidelines, see Chapter 12 of the ESS Translation Guidelines (European Social Survey 2018). 10.3.4 Translation verification Verify translations of your instrument. Despite being much-cited and persistently being added to ToRs, back-translations are not a great means to verify translations. Most big organizations such as the European Social Survey (2018) or the US Census Bureau (Pan and Puente 2005) that run multilingual surveys and put a lot of effort into comparative survey design use and recommend the TRAPD method (Translation, Review, Adjudication, Pretesting and Documentation). It involves reviewing and comparing (at least) two independent draft translations and agreeing on one translation that is cognitively pre-tested. Insights from the pre-test feed back to the translation, and the whole process is documented. Translation, review and adjudication are normally done together by a team that combines translators, survey field experts and content subject experts. A team-based approach deals better with mistakes, idiosyncratic interpretations and translator blind spots, compared to individual translators working on their own. Surveys with tight budgets and time constraints can use a reduced version of TRAPD to verify translation. Save yourself the back translation. Instead, use independent translator(s) to systematically review all rows in your translation sheet. The reviewer(s) either approve a translation or suggest an alternative translation in a new column. Translators and reviewers discuss all discrepancies and choose a final translation. Using a translation management system as described further below, translation and review processes can overlap and do not necessarily add much time to survey preparations. Make sure to desk test your translation in the CAPI/CATI before the pre-test. Often, the context, the way questions are displayed in the interface or the sequence require translations to be updated. As a last defense, verify the translation during the training. During the questionnaire review part, project the questionnaire and read out loud each question in the design/common language. Trainees follow on their devices in the local languages they are fluent in and are likely to use in the field. For each question stress the meaning of the question and check with the trainees if the translation is accurate. Assign one person to make updates to the translation sheet on the go based on feedback from the trainees. Keep note of contentious items and address them with selected trainees on the side or after the session in order to not delay the training. 10.3.5 When to translate Translate a questionnaire too early, and juggling the inevitable many updates can quickly overtake your work and introduce mistakes. Translate too late and the instruments are not verified, poorly translated or simply not ready in the local languages at the time of the pre-test, destroying one of its main purposes. In the preparation phase, carve out enough time prior to the pre-test for translation and verification. Try to complete the questionnaire before that, so that it only undergoes minimal change once translations have started. Translating a normal socio-economic questionnaire can take several days, and so does the verification process. If there is little time, you can use a staggered approach to provide enough time for translation. Translators can already translate and verify completed sections while the remaining sections are being finalized or put into CAPI/CATI. Use a translation management system as described below to stay on top of the updates and avoid mistakes. If you are using a paper version to develop the questionnaire, do not directly translate the paper version, but wait for the electronic questionnaire to be developed. When building the CAPI/CATI tool, questions often have to be modified to function in the interface, especially around roster and dynamic question text. If already translated, these changes have to be made in many versions, and often in languages the questionnaire designer does not speak, making updates difficult and error prone (e.g. when trying to find the word NAME in local languages to replace it with dynamic text). It is a lot easier, faster and less error prone to translate and manage updates once the tool has been converted to CAPI/CATI. If you require translated paper versions e.g. for training or archiving purposes, create them using your translated CAPI instrument. Often this can happen during or after field work, once the questionnaire does not change any longer, and during less work-intense parts of the project. 10.3.6 Stay on top of updates Lets face it. Most questionnaires will still undergo some change after the translation process has started. Done wrongly, late updates to the questionnaire can easily turn into a managing nightmare and cause significant undetected differences between the source and target languages. Done correctly, one can relatively easily stay on top of questionnaire modifications, even if there are a considerable amount of change and several target languages. Do not: Do not translate in the CAPI/CATI tool directly. They tend to hold only one additional text field for each language and have no means to let you manage or document the translation and verification process. Do not translate or store translations directly in translation sheets exported from CAPI/CATI, as they will be outdated with the next update to the questionnaires. You will inevitably end up trying to juggle different parts of the questionnaires on different translations spreadsheets for different languages. Do not work with offline versions of translation sheets sent back and forth by email. As above, this quickly becomes unmanageable. Instead: Work with an online spreadsheet such as Excel or Google sheets that holds all the translations and allows questionnaire designers and translators to simultaneously collaborate on the same live document. Define clear processes, how questionnaire designers mark rows that need to be translated, reviewed or updated, and how translators record the translation status of each row. Make sure designers and translators follow the processes to make the system work. Examples are: Add columns status (fixed set of answers, e.g. to translate, to review, to update, translated, reviewed, dont translate) and comments. Designers change the column status for rows they added or updated in the source language and provide details in comments if necessary. The first translator modifies the target language and sets status to translated, the reviewer to reviewed. At the end of the process, all rows should be in the final status reviewed. Designers add a special symbol that does not occur in the questionnaire or code (e.g. @) to the beginning of the target language column for rows that need to be translated, updated, or reviewed. Translators remove the symbol after updating the target text. Use conditional colour coding to highlight rows of certain status, and filtered views to generate custom views, e.g. a view for translators containing all rows they need to work on. Updating translations If you are working with Survey Solutions, use the translate_questionnaires STATA tool to keep your translation sheet in sync with the questionnaires and to produce the translation templates to be uploaded to the Survey Solutions Designer. The tool can be adapted relatively easily to work with other CAPI/CATI packages that export and import tabular translation sheets. For CAPI/CATI software such as ODK or Kobo that use spreadsheets to build the instruments, one can integrate the translation process into the same spreadsheet used to build the instrument by adding additional columns. Use an online spreadsheet to allow simultaneous access to designers and translators, and define processes as described above. 10.3.7 On-the-fly translation As described above, translate into as many languages as possible and try to avoid on-the-fly translations as much as possible. Sometimes however, on-the-fly translations are inevitable, e.g. if local languages cover small parts of the sample population or are non-scripted. If this is the case, make sure that interviewers have practiced translating the questionnaire in front of others prior to the field, and that there has been a group consensus on how to translate the questionnaire into the local languages. Both significantly improve the quality of the translations and can be implemented using group practice during the training. For group practices, group together trainees according to the local languages they are likely to use in the field. One trainee playing interviewer administers a question in the local language, those playing respondents follow on their tablets in the source language and make corrections or suggestions on how to improve the translation. Trainees should rotate roles such that every trainee has practiced translating the entire questionnaire to others speaking the same language. If you will have to work with local interpreters to translate interviews on-the-fly, make sure that interviewers explain well the key concepts such as household or parcel, and that they are understood by the translator. Make sure that you have budgeted for local interpreters if they are likely to be needed. References "],["sample.html", "Chapter 11 Sample", " Chapter 11 Sample work in progress "],["manual.html", "Chapter 12 Manual", " Chapter 12 Manual work in progress "],["introduction-to-training.html", "Introduction to Training TL;DR 12.1 notes", " Introduction to Training Fieldwork training is arguably one of the most important activities of any survey and has one of the biggest impacts on data quality. It serves several purposes: Build fieldworker capacity. In-depth content training, extensive practicing and capacity-based selection ensure that fieldworkers are capable of implementing the survey as designed. This is key to limit measurement and nonresponse error, as well as coverage and sampling error if the survey includes listing and in-field sampling. Standardize fieldworker behavior. Repeated practicing under supervision and continuous feedback streamlines how fieldworkers implement the survey. Reducing idiosyncratic behavior is important to mitigate the very damaging fieldworker effects. Scrutinize methodology. Particularly in surveys with limited pre-testing and piloting, fieldwork training often effectively is the most detailed review of the questionnaires, translations, CAPI and survey processes. Identifying and correcting mistakes is key to reduce specification error. Unfortunately, in many surveys fieldwork training is not effective in producing fieldworkers that are fully capable of implementing the survey, or is inefficient in being too demanding on trainers and survey budget and timeline. This part of the book gives recommendations on how to implement fieldwork training that effectively prepares field workers while not while not overburdening trainers or overstretching survey resources. It assumes fieldwork training to be in-person, as is custom for most socio-economic surveys with personal interviews. For remote training, check XYZ ??QUESTION Do we have any resources? Readers who are interested in a cheatsheet version can read the key recommendations for fieldwork training in the TL;DR below. Those looking for more detail will benefit from reading the respective chapters. Chapter 13 Planning and Preparing is targeted at survey designers, managers, fieldwork managers, trainers or any other role that is involved in the planning, budgeting and preparation of the fieldwork training. It provides recommendations on the training location, timing, size, the trainers, training content and schedules, and includes checklists of the key items to prepare prior to the training. Chapter 14 Conducting the Training gives useful recommendations on how an effective and efficient fieldwork training can be conducted. It provides extensive details and step-by-step guides for different training modules. It is mainly targeted towards trainers and is best during training preparation and the training itself. LSMS recommends to continuously evaluate trainees throughout the training and to select fieldworkers based on capacity at the end. Chapter 15 Assessment &amp; Selection is mainly targeted towards trainers and gives step-by-step guide on how to effectively implement written tests, systematically evaluate skills and select fieldworkers. TL;DR Follow below key recommendation to ensure a successful interviewer training. Train an excess of 40% of fieldworkers to allow for trainee attrition, competency-based fieldworker selection and to have a fieldworker reserve. This can be reduced to 20% if you are working with experienced teams with low attrition. Anything less does not allow for a meaningful selection process. Select based on skills. Continuously evaluate trainee performance and capacity in written tests, role plays, questions in class and observing them while practicing. Select fieldworkers at the end of training in a transparent process based on their capacity and skills. Select roles based on skills__ who wil be supervisor, data monitors, etc Train an excess of 20-40% of field workers and select them based on skills at the end of the training, even if working with experienced teams. Trainees take the training more seriously, pay attention, and learn and the overall skill level of field workers is higher. Dont cut on duration! Allow for at least one full week of training per 1.5 hours questionnaire length. Anything less is normally not enough for the complex content of a socio-economic questionnaire to be learned and practiced. Keep it small. Try not to train more than 50 persons at a time, so you get to know the strengths and weaknesses of the trainees. Adjust field work to take longer to allow a smaller field work team. Make it interactive. Avoid one-directional presentations that overwhelm trainees with too much information. Involve trainees and frequently change the training modality. Practice. Frequent practicing is crucial for trainees to fully internalize the content and be able to apply in the field. Practice on-site and in the field. Be prepared. Make sure questionnaires, manuals and tests are truly ready prior to the start of the training. Have the venue fully set up and all admin and logistics sorted. Focus. Only teach what interviewers need to know to do their work well. Spend more time on things that are crucial to the survey. Instead of giving a 2hr presentation on the analysis of the data, give a short introduction and practice with interviewers how to introduce the study in the field. Enough trainers. Delivering good training is more than a full time job. Many things need to happen in parallel to the training being delivered. More trainers are usually better, and best if stakeholders, designers and analysts are among them. Engage. As a survey designer, do not do your emails on the side table and leave it to the survey firm, fieldwork manager or trainers alone to deliver the training. Actively participate, check that the content is correctly understood, explain what matters, etc. You have put a lot of effort into the design. Make sure it is implemented correctly. Trainers need expertise. Sending a junior colleague without much experience to be in charge of the training is not a good idea. You need solid survey, context and subject experience among the trainers to deliver a solid training. Assess readiness for field. At the end of the training, a final field test that mimics field conditions. where field workers can be observed and feedback given without using the actual sample. If team is not ready, e.g. there are common outstanding issues, extend the training by a few days to revisit and strengthen problematic areas. Interviewing skills are essential. Not all trainees possess them. Especially new recruits need to learn and practice how to introduce the survey, respondents to participate, maintain the repsondent responsive, elicit responses for difficult questions. Only select capable interviewers. Only select interviewers that have demonstrated full understanding and capability of satisfactory and reliable completion of required tasks. If there are more than needed and you have flexibility, you can have slightly more teams operate over shorter time as long as overall size is kept at bay. If not enough trainees are available, extend training. 12.1 notes debriefing fter interviews retraining, when quality assurance mechanisms keep notes to write training report, for documentation and to learn for future rounds. design training prrotocols "],["planning-and-preparing.html", "Chapter 13 Planning and Preparing 13.1 Training size 13.2 Trainers 13.3 Content 13.4 Timing &amp; Duration 13.5 Schedule 13.6 Location 13.7 Preparation 13.8 Notes:", " Chapter 13 Planning and Preparing Fieldwork training should be planned well in advance and be well prepared prior to the start of field It consist of the interviewer training, separate training for supervisors, data monitors or other roles. The chapter presents recommendations for survey designers, survey managers and trainers. Survey designers, take note of the chapters duration, training size and trainees, as they are budget and timeline relevant. Survey managers and trainers might chapters schedule, content and preparation mgiht eb relevant to allow for a solid preparation fof the training. 13.1 Training size LSMS strongly recommends to train more trainees than are ultimately needed as fieldworkers, i.e. as interviewers, supervisors, data monitors or other roles. This allows for a competency-based selection of fieldworkers at the end of the training. Training more people than needed also compensates any attrition of trainees during the training and builds a reserve of already trained persons that can be brought up to speed relatively quickly during fieldwork, should fieldworkers need to be replaced. Train at least 40% more fieldworkers than are required if you are working with unknown trainees, a new implementing partner or in a new context. For example, train at least 56 people if you are aiming for 40 fieldworkers. Even if some drop out before the end of the training, this still allows you to not select some of the trainees that are not up to the task. You can reduce the excess to 20% if you are working with experienced teams who you have worked with before and if you are expecting few trainees to drop out during the training. Training less than 20% additional people is not recommended as it prohibits a meaningful fieldworker selection process. Often, some trainees will drop out during the training, particularly after day 1 or 2 once there is a clearer understanding of the required effort. Mitigate early trainee attrition by keeping on standby some of the applicants that have not been invited to the training, so they can quickly replace any trainees that have not come back. You might have to bring them up to speed in extra hours and ask them to independently learn already covered material to catch up. It is not advised to bring in additional trainees after day 3, as they will have fallen behind too far. In some surveys, there have been walk-outs of trainees, often towards the end of the training, in which trainees collectively refuse to continue the training or fieldwork unless their demands are met. Usually, there is no time or budget to repeat the training with other trainees, leaving survey management in a very weak bargaining position and putting the survey itself at risk. Avoid walk-outs by setting fair fieldworker terms and conditions, making them clear to trainees prior to the training, and asking them to agree by signing a copy of the terms and conditions. In many circumstances it is beneficial to select supervisors, data monitors and any other fieldwork roles from the pool of trainees based on their performance and capabilities observed during the training (see Assessment &amp; Selection). If any of the fieldwork roles are predetermined, the designated persons should participate in the fieldwork training without being counted towards the trainees excess (and demonstrate full understanding of the trained material). Keep the number of trainees as small as possible, ideally below 50 trainees (around 36 fieldworkers) and maximum around 70 trainees (around 50 fieldworkers). Learning outcomes quickly decrease with increasing class size, as each trainee receives less supervision and personal feedback and as it is more difficult for trainers to keep track of trainees performance. If possible, adjust the fieldwork plan, so that fewer fieldworkers work over a longer period of time. See Chapter Fieldworker Recruitment for other benefits of having a smaller field team. If it is necessary to train more than 70 trainees (e.g. surveys in large country or with large sample) or a centralized training is not possible (e.g. different languages or geographically dispersed sample), it might be adequate or imperative to split fieldworker training into separate rooms or even locations. In those instances it is paramount to standardize the training between different rooms/locations as much as possible in order to minimize the training effect. This requires the training to be largely guided by standardized and extremely well-prepared training materials, and effective communication between the training rooms/locations, so that all trainees benefit from relevant feedback, comments, questions, etc. raised in one room/location. on top of that, there must be sufficient capable trainers who have received a comprehensive Training of Trainers (ToT). the ToT can be a dedicated exercise or integrated as an explicit outcome of the pilot training. Several scenarios are possible if a large number of fieldworkers need to be trained: Partitioned training. This is the first best option to train larger class sizes (70-150 trainees) that can be trained centrally. Split the class into groups of manageable size around 15-20 trainees and assign dedicated trainers to each group. Train all modules that are sensitive to class size in the groups, including the questionnaire reading and practicing. Trainers must keep notes of any additional clarifications they provided, questions they received or feedback they gave. Hold daily plenary sessions with the entire class in which feedback is given based on the notes from individual groups. Modules that are less size-sensitive, such as the opening module or fieldwork logistics can be trained in the full class. Parallel rooms. Some surveys have split trainees into groups that are independently trained at the same location. Each group has a resident trainer, responsible for the training in this group. Other trainers responsible for certain modules rotate between groups. Compared to the partitioned training described above, it is harder in this approach to harmonize training between groups. It is only desirable if significant input from expert trainers is needed. Consecutive training. Different groups of interviewers are trained one after the other. A core set of trainers repeats the same training for each group, providing consistency between the groups. Since each group should start fieldwork immediately after their training, this approach only works if a staggered field start is possible (e.g. if field work is sufficiently long). Additional trainers should attend the training for each group, so that they can supervise the field start of one group while the core trainers move on to train the next one. Parallel locations. It is not recommended to simultaneously train in multiple locations, as it is difficult to harmonize fieldwork training between different locations. If unavoidable, it is essential for trainers to keep good notes, frequently communicate and debrief with trainers from other locations. 13.2 Trainers The success of a fieldwork training hinges upon having enough and competent trainers. Conducting a fieldwork training is a substantial amount of work! Doing it well even more so. Prepare the training well to avoid significant reductions in quality or trainers getting overwhelmed. Make provisions for enough trainers so they can conduct the training well. For every room, there needs to be a minimum of two active trainers at any point in time, one to lead the training and another one to support the class or do any of the supporting activities that need to happen in parallel, such as updating manuals, compiling feedback, creating practicing scenarios, etc. Larger classes require more trainers as supervision and supporting activities increase. Ideally, there should be one active trainer for every 15-20 trainees. Trainers must have availability to actively engage in the training most of the time. Avoid the situation where trainers are physically present, but either are in meetings, stare at their laptops or do other tasks unrelated to the survey. Cater for more trainers if trainers have ongoing other responsibilities outside of the training (which is often the case). All trainers must have: A good understanding about the survey in general A detailed understanding of the questionnaires, definitions, protocols, manual, CAPI, etc. Ideally, they have been involved in the survey design process, participated in the pre-test or pilot and will be involved in managing and monitoring fieldwork. A general understanding of the survey subject, e.g. the regional agricultural systems for an agricultural survey. Good facilitation skills and the ability to lead and manage all components of the training. Collectively, the trainers of one room should also have: A good understanding of the indicators construction, the intended data use or analysis. Subject and context expertise, e.g. know in detail the local circumstances of child education, nutrition and health for a survey on early child development. Experience in implementing the survey type and implementing surveys in the context Experience in conducting fieldworker training Speak the trainees language. Do not train in a language the trainees are not fluent in. If some trainers (usually the survey designers or analysts) do not speak the training language, other trainers can translate for them. Particularly in surveys with limited pre-testing and piloting, often mismatches between the survey design and field reality are uncovered during the training and adjustments need to be made quickly. For example, a locally practiced exchange labor system might not be picked up accurately in the questionnaire. Trainers need the above expertise and experience to be able to identify those issues and take decisions that are analytically correct, in-line with the survey design and make sense in the local context. There is a tendency to send junior analysts or PhD students without sufficient survey or context experience to conduct the interviewer training. This can work, as long as other experienced trainers are present, but quickly leads to quality issues if they are the main training lead. Similarly, handing over a sample and questionnaire to a survey firm and letting them implement the training without monitoring them closely is usually a bad idea. Firms commonly lack the analytical background, do not know about the details of the survey design, are differently motivated, and might not have the capacity to conduct the training to the required standard. If working with a new firm, it is recommended to send your own trainers to be present at the training, support if necessary and ensure the training meets the quality standards described here. Some surveys advocate for subject experts to facilitate some of the modules. Unfortunately, they often have their own agenda and priorities and come with a varying capacity to conduct effective fieldworker training, making it sometimes a waste of time (e.g. long, theoretical presentations on details that are largely irrelevant for interviewers). If external subject experts will lead parts of your training, make sure they fully understand the relevant parts of the questionnaire and what knowledge they need to transmit to trainees. Prior to the training, discuss the scope of their engagement and review their contributions, such as presentations. Subject experts can also be given a backstopping role to answer any questions on the subject and contribute to relevvant disucssions. 13.3 Content To equip trainees with all the knowledge and skills necessary to do a successful job as interviewers, training must go beyond a simple discussion of the questionnaire and cover the following components: CAPI: Try to focus on the interviewer perspective and limit it to the functionality that is relevant for the interviewer in this study. Keep the theory to a limit and practice as much as possible. It is better to gradually introduce new functionality as needed rather than have an extensive theoretical session at the beginning. Questionnaire. Question types, questionnaire formatting, navigation, routing, understand all questions and answer options. How to administer each question and section. Underlying key concepts and definitions. Practicing. Repeated practicing, alone, in groups and with respondents is required, so that interviewers are fully familiar with the questionnaire content, know how to navigate it in the CAPI tool, can administer it correctly and get exposure to real respondents and scenarios during the training. Protocols. Exercises such as capacity tests, anthropometrics, plot measurement or behavioral games require strict adherence to protocols for comparable measurements. Trainees must fully internalize protocols and practice them until behavior across the entire field team has been homogenized. Assignment management. How to receive household assignments. How to find and locate households. When and how often to revisit. Replacement protocols. How to plan interviews and manage your workload. Interviewing skills. How to introduce yourself and the study as well as convince respondents to participate. How (not) to ask questions. How to not lead questions. How to probe, inquire, get clarifications. How to control the interview. Manage respondents time. How to deal with impatient respondents. Post-interview skills. How to check for completion. How to address in-consistencies. How to leave comments. How and when to submit files. How to receive feedback and respond to inquiries on submitted interviews. How to (not) make corrections. How to ask questions. A full dress rehearsal at the end of the training that mimics real field conditions and asks interviewers to track and interview households. 13.4 Timing &amp; Duration Schedule at least 1 full week (6 working days) of interviewer training per 1-1.5 hr questionnaire time. To some readers, this might sound long, and many survey firms, especially when conducting opinion surveys will suggest much shorter training times. In most cases it is impossible to train the field teams to a higher standard in a shorter amount of time. Do not save on training days, unless you are very confident that you all trainees will have fully internalized the required material and be able to perform well as field workers. Socio-economic questionnaires are long and complex. They require interviews to understand a large number of questions and to fully internalize complex concepts and definitions such as employment types, or land management systems, etc., and to correctly apply those in the field. On top of that, interviewers need to learn and practice a range of skills such as convincing respondents to participate or continue, troubleshooting inconsistencies, responding to feedback from supervisors, etc. In many surveys, trainees are not ready to go to the field at the end of the training and the training needs to be extended last minute. If extensions have not been budgeted for, firms will have to cut corners elsewhere to compensate for increased costs. Avoid this by setting out with a realistic training duration and putting contingencies for potential extensions into the budget and timelines. anthro 5-6 days questionnaire reading + practicing Fieldwork training should be held just prior to the start of fieldwork, with a maximum of 1-3 days between training and fieldwork to allow for admin, rest day and travel to the field. The shorter the break, the better interviewers remember the trained material and can solidify it by practicing it immediately in the field. If the break is longer than a few days, the risk of interviewers forgetting details and developing idiosyncratic behavior increases. Conduct a short refresher training just prior to the field start to refresh key points of the trained material. If the field start is being delayed further, a longer refresher training or complete retraining will be necessary. 13.5 Schedule Prepare a solid schedule and try to stick to it during the training. Track your progress on an on-going basis during the training. Make adjustments and organize extensions as soon as you notice that you are falling behind or need more time, before logistics make this impossible. If training progresses faster than anticipated, use the time for additional practicing. Only shorten the training if you are certain that the trainees are fully ready for the field. A few consideration for drawing up a functional training schedule: Do not exceed 6-7 hours of training time per day and give frequent breaks, without the breaks taking over the day. For example, a morning session from 8.30 AM - 12 PM, and an afternoon session from 1 PM - 5 PM, each with a flexible 15 minutes break. Allow for enough time for trainers to debrief and prepare for the following days. Frequently change training modality, e.g. by putting practicing sessions between questionnaire reading sessions. Avoid as much as possible long sessions of only questionnaire content, PowerPoint, theory only. Trainee attention levels tend to be lower after lunch and in the afternoon. During these times, schedule hands-on, more interactive sessions that require greater engagement from trainees, such as practicing or quizzes. Be efficient. Only train content that interviewers need to know. Allocate time on topics proportionate to the direct importance for the interviewers. Keep it together. Train material only in context, once it is needed and can be practiced. Example, practice How to introduce and ask for participation the day before the first field practice or agricultural key definitions when going through the respective questionnaire. Stay flexible. You could draw up a very detailed plan with exact timings for each session, but in reality the timing of sessions tends to shift a bit during the training. Use the schedule as guidance to track progress, but stay flexible to allow sufficient time to address issues, retrain topics, or dive into more detail as needed. Allocate time generously. It is better to finish training early one day rather than keeping trainees until late, being behind schedule or rushing content. If you are falling behind schedule during the training, extend the training as early as possible so all logistical arrangements can be adjusted. From theory to practice. During the first days, focus on the questionnaire content and conduct frequent on-site practices. In the latter half, schedule field practices, interviewing skills, feedback sessions. Specialize fieldworkers. If your survey requires a lot of different questionnaires (e.g. in school surveys) or special tasks to be conducted (e.g. medical samples), it is often too much material for all interviewers to learn everything. Instead, divide the field teams into different roles that each completes a different set of questionnaires/tasks. During the training, those questionnaires/tasks can be trained in parallel sessions, allowing for more in-depth training. expand on this one: long Parallel training only for dedicated other roles, supervisors and monitors need to attend everything. Train field supervisors data monitors or similar roles outside of the hours of the interviewer training, so they can attend the interviewer training in full and know everything interviewers are supposed to (not) do. Typical time slots are towards the end of the training once supervisors have been selected during the late afternoon or evenings, or during administrative days or rest days. Schedule classroom days after field days so that you can hold in-class debrief sessions with the big screen. Rest days. For training lasting over one week make sure to schedule one or two rest days per week. Trainees and trainers need regular breaks for good learning outcomes. Below is an example training and piloting schedule for a 1hr-long questionnaire collecting information on agriculture and a few other socio-economic indicators. The schedule allows for practicing of learned material, changes modality frequently, and includes daily quizzes and feedback sessions. Module Hrs Location Day 1 Welcome &amp; introduction to the project 1 on-site Introduction to Survey Solutions Interviewer 1 on-site Questionnaire: A: HOUSEHOLD 1 on-site Questionnaire: B: AGRICULTURE &amp; key concepts/definitions 2 on-site Quiz A 1 on-site Day 2 Review Quiz A (0.5 - 1 hr) 0.5 on-site Questionnaire: B: AGRICULTURE 2 on-site Questionnaire: C: LIVESTOCK 2 on-site Group practicing sections A - C (can be split) 2 on-site Quiz B 0.5 on-site Day 3 Review Quiz B (0.5 -1 hr) 0.5 on-site Questionnaire: D: EMPLOYMENT &amp; BUSINESS 2 on-site Questionnaire: E: OTHER INCOME 1 on-site Questionnaire: F: HOUSING 1 on-site Questionnaire: G: SHOCKS 0.5 on-site Questionnaire: H: NUTRITION &amp; FS 0.5 on-site Group practicing sections D - H (can be split) 2 on-site Quiz C 0.5 on-site Day 4 Review Quiz C (0.5 - 1 hr) 0.5 on-site Questionnaire: I: DISABILITY 0.5 on-site Questionnaire: J: GROUPS 0.5 on-site Questionnaire: K: CONTACT 0.5 on-site Questionnaire: R: RESULT 0.5 on-site Practicing with respondents on-site 3 on-site Quiz D 0.5 on-site Day 5 Review Quiz D 0.5 on-site Questionnaire: S: START 1 on-site Role play: Introduce the study and ask for participation 1 on-site Quiz E 0.5 on-site Field practice (afternoon, 1 interview per team of 2) 4 field Day 6 Review Quiz E 0.5 on-site Debrief field practice 1 on-site Identifying households, replacement protocols 1 on-site Role play: Difficult situations e.g. respond to being asked to leave, ask for income, etc. 1 on-site Responding to rejected interviews 2 on-site Interviewer, supervisor and data monitor selection Day 7 Rest day Day 8 Pilot day 1 (at least 1 interview per interviewer) 6 field Debrief 1 field Afterwards: Supervisor Training 2 on-site Day 9 Pilot day 2 (at least 1 interview per interviewer) 6 field Debrief 1 field Afterwards: Supervisor Training 2 on-site Day 10 Debrief field test on-site Administrative day 4 on-site Supervisor training 4 on-site You can download the sample schedule as an Excel from here. MICS train for 25 working days. efficiency, good organisation can have a good learning outcomes in shorter time. too long, fatigue and forget parallel sessions for measurers, specialized tasks. can start from days one if no overlap in tasks, little awareness and roels are already clear e.g. if trained nurses are needed. Otehrwise, often useful to identify specialised trainees during training, after one has workked with trainees for a few days, to get to know their capabilities and personality. E.g. measurers need ot eb good with kids and practice, good number rading skills, but do not need to understand all concepts in a long hh questionnaire. Allows to trainees that perfroma ccordingly in the training. 13.6 Location Fieldwork training is best held in a location that: Has sufficient logistically capacity: Is easy to reach for trainers and trainees, has a suitable training venue, sufficient accommodation and food options, good network coverage, print shops, etc. facilitates field testing: Is close to communities that are similar to but outside of the sample. If the survey is limited to a specific region or area, it is best to conduct the fieldwork training there. facilitates field start: Allows field teams to start field work shortly after the training and trainers to supervise the first few days of field work. Find a training venue with a big hall large enough to comfortably fit the entire team (or multiple halls if parallel sessions are held) and space to breakout into smaller teams for group exercises, such as a garden, hallways, the cantine or additional rooms. It is crucial that there is good internet connectivity at the venue, either through on-site Wifi, or general network connectivity. The venue should be either able to accommodate trainees and cater for food, or be easily accessible and close to restaurants or eateries. It should be quiet and free from distractions. Good venues tend to be hotels or conference centers with garden or open space. Avoid universities, schools, towns halls or other places that are busy and become easily unavailable. Book the venue early. Check for availability, in case the training needs to be extended on short notice. 13.7 Preparation Avoid wasting precious training time by preparing everything that can be prepared prior to fieldwork training. This includes: Make sure they have learned about the survey details and terms before the training, including the areas to be visited, the population to be interviewed, the transport and lodging arrangements during field work, the length of field work, the interviewer selection process, their remuneration package, insurance, contract details etc. Provide them in written form and have each trainee sign it prior to the training. Schedule any contract signing for a time or day that does not interfere with the training. Pulling trainees in and out during a session reduces its effectiveness. Projector with a BIG screen. The projected content must be visible and legible to all trainees. You should be able to connect a PC and tablet/phone to the projector. Ideally, it should be possible to quickly link trainees devices to the projector, to facilitate exercises and practicing. Test the set-up prior to the start of the training. Stable internet connection for all devices. Trainees have to sync their devices several times per day. Usually, the WIFI connection of the training venue is not sufficient to connect all devices at the same time. Often, additional mobile network routers or WIFI hotspots are required. One option is to already provide the internet connection that is planned for field work, e.g. the dongles, mobile phone routers or sim cards. Electricity and extension cords to charge all devices. Food and refreshments for the breaks. If food is provided by the venue, make sure it is ready in time for the break. If trainees eat out for lunch, set reasonable break times that allow trainees to finish lunch and to be back in time. Incentivize punctuality, e.g. those coming back late from the break have to sing a song. referral forms for anemia or severe undernutrition. equipment if needed, measuring boards, scales GPS equipment One tablet/phone per trainee. It is crucial for all trainees to learn how to navigate the questionnaire CAPI and use the device. If you do not have enough devices (since there are more trainees than interviewers) borrow devices or ask some trainees to use their private devices. Devices must meet the minimum specifications for the CAPI software (see here for Survey Solutions) and have the correct CAPI version installed. Consider using an app blocker to allow using specific apps only. Add shortcut/icons on the main screen for CAPI, the softcopy of the manual, the calculator or any other relevant app. Fully set-up CAPI software. For Survey Solutions this includes the installation of Survey Solutions on a server, interviewer accounts for each trainee, questionnaires imported to the server and assignments made. For training, often open assignments with infinite quantity (-1) are useful. Create several assignments with different scenarios if the questionnaire includes substantial pre-loading (e.g. household rosters in a panel survey). printed questionnaires if needed. One at the beginning, and further throughout the training. additional copies of household memer roster, plot or parcel rosters etc, for practicing. printer in venue good idea samples of documents or items that interviewers need to interpret, such as birth certificates, immunization cards, school timetables, anti-malaria medications, for hand-son practicing. logistical tools such as household tracking lists, village maps, etc. community completion sheets Finalized questionnaires. The questionnaires should be reviewed, extensively pre-tested or piloted, translated and the translations verified. The CAPI version of the questionnaire should be extensively desk-tested and error free. Mistakes can cause significant issues or delays during the training. Provide printed copies of the paper questionnaire if needed. Fieldworker manuals. All manuals for interviewer, supervisor or other fieldworker roles have to be made available to trainees from the beginning of the training, so trainees can use them and get used to using them as a reference. Modifications and additions should be made to the manuals during the training wherever it was unclear or incorrect. Either hand out a printed version of the manual at the beginning and a revised version at the end, or provide regularly updated soft copies in pdf to the trainees which they can look at on their devices (which tends to work very well). Facilitation material. Microphones and speakers including batteries and fully tested, flip charts with non-permanent markers in multiple colors for drawings, name tags/stickers for all participants, printed attendance lists and feedback forms, for trainees notebooks, pens in 2 colors to allow marking and clipboards, etc. Field work material: bag packs, rain coats, ID cards, battery packs Organize in-field practicing. Select practice areas/communities well in advance. They must not be included in the sample. Make all necessary administrative arrangements such as getting permissions, introduction to village officials, etc. If necessary, arrange respondents for field practices, so trainees can spend more time practicing than looking for respondents to participate (dont do this in areas/communities for the final field test). Organize reliable transport for the trainees to travel to the field practice. Make sure the transport waits for the team, not the other way round. Organize on-site practicing. Arrange respondents for on-site practicing to visit the training venue, including transport, food/refreshment and remuneration. Logistical/administrative support. Get a dedicated person to support with all the logistical and administrative work that has to be done during the training, e.g. setting up the venue, organizing transport, contacting local leaders, printing, purchasing equipment, signing field worker contracts, etc. If these activities have to be done by the trainers, they become a substantial distraction and can significantly affect training quality and pace. 13.8 Notes: MICS: Preparing for the training Training should be planned well ahead of time. Before training the field staff, the following must be in place:  Other field procedure documents, such as the Cluster Control Sheets, necessary accounting documents, or any specific ethical guidelines , must also be prepared in advance.  Logistics for the fieldwork are key and should allow for the fieldwork to commence immediately after the training completes: Payments, accommodation, transport, insurance, etc. are among some of the absolutely key components to have established protocols for. It is well known that unless fieldwork payments are paid on time, the quality of the entire fieldwork is jeopardised.  Similarly, logistics for the training itself is key, including all monetary matters.  Identified and arranged locations/institutions and transportation for anthropometric training (pre-schools, day care centres, hospitals, etc.)  Prepared equipment, visuals, and tools for use in demonstrations during training and for practice and pilot. See appendix A for a typical list of materials.  Field equipment, i.e. tested tablets and accessories, anthropometry and water quality testing items, first aid kits, flashlights, rainboots, umbrellas, bags, caps, t-shirts, response cards with literacy sentences, smilies, etc., must all be available for distribution.  Agreed upon agenda and methodology  also for selection of field teams.  Secretariat support for the duration of the training.  "],["conducting-the-training.html", "Chapter 14 Conducting the Training 14.1 General considerations 14.2 Opening module 14.3 CAPI use 14.4 Questionnaire content 14.5 Practicing 14.6 Interviewing techniques 14.7 Special tasks 14.8 Responding to inquiries 14.9 Final field test (often called pilot) 14.10 Supervisor Training 14.11 Data Monitor Training 14.12 Trainer meeting", " Chapter 14 Conducting the Training This chapter provides details on how to deliver an effective and efficient interviewer training that facilitates learning without overstretching survey resources. The chapter assumes in-person training as is the case for most surveys conducted with in person interviews. The first subchapter covers general training considerations. The following chapters are organised by training module, provide details on the rational, optimal timing and good ways of conducting each respective module. See the field plan on how modules relate and when to deliver them. intended to be used by trainers fieldwork training recommendations Before you start, make sure the training is fully set up LINK PREPARATION. 14.1 General considerations In general, consider the following points for the training. Eliminate language barriers. Train in the language trainees are fluent in. Dont force them to speak your language if they do not fully understand it. If you dont speak the local language, either work with an interpreter, or have your local colleagues reiterate your points in the local language, e.g. the fieldwork managers. Support. Especially during the first few days, trainees often get lost in the CAPI or questionnaire and are unable to follow the sessions. Trainers not actively leading a session can walk around the class, check that trainees are in the right place and support those falling behind. Probe for understanding. It is wrong to assume things have been understood if you just ask Any questions? and there are none. There hardly ever are any. Probe using common examples or scenarios. Ask variations of your test questions. For example, think of working profiles/jobs that are typical in the surveyed population and probe trainees if these are wage employed or self-employed. Encourage questions. Often trainees do not dare to ask if they have questions. Make it clear to trainees from the beginning that asking questions is much better than not knowing something during the tests and that they should ask if they have any doubt. During sessions, they can raise their hand and be visited by a trainer. Involve trainees. Asking frequent questions. Let trainees read aloud questions and manuals. Ask them to correct one another. Dont always pick the most engaged trainees but try to involve everyone. Select a trainee using a random name picker such as https://wheelofnames.com, so it can be anybodys turn to answer your questions at any moment. They make it fun if displayed in front of class. Alternatively, select the person that is closest to where the paper ball lands. Select one trainee for a couple of questions to speed up things. Collect feedback. At the end of the day, use feedback forms to collect anonymous feedback from trainees on how clear different sessions or trainers were, topics they did not understand or questions they have. Review feedback forms prior to the next day and adjust your training schedule or methods if needed. Break up and mix seating arrangements. If you start noticing groups of trainees sitting together who are not paying attention or are in the bottom end of performance, distribute them across the classroom. Ask low performers to sit next to high performers, so they can help when falling behind. Essentials only. Only train interviewers on things that are relevant for them and that they are required to do. For example, there is no need for them to learn details about the analysis or experimental design. Likewise, if only supervisors collect GPS coordinates, this is better trained only to them. Confirm with designers/analysts. For solid results, the interpretation of a question must be aligned all the way from how it has been conceptualized to how it is fielded. This is not always the case. Note anything you are uncertain about or is underdefined and run it by designers/analysts on a daily basis. Ask them to be on standby so you receive timely responses. Make it fun! Let the last person entering the venue late and unexcused sing a song in front of the class to help with punctuality (Youtube is full of Karaoke videos). Play fun and exciting group games like Protect the egg or let trainees get to know each other in ice breakers. Use video &amp; audio. Some things are best demonstrated to interviewers, so they can see/hear what should and should not be done and learn from examples. Examples can be introduction to household, take anthropocentric measures, or other practical or non-scripted tasks. Use existing material, or collect video/audio material during pre-test, pilots and field practices to demonstrate good and bad practices to the class. Control the group. Set rules on talking use of mobile phones, punctuality and enforce them. Ask if you dont know. Keep notes of any scenarios that you do not know how to interpret and run them by relevant experts or decision makers in the evening. Provide feedback to the class once you have received the answer (ideally the following day). If theTrainers should compile survey designers are not present, questions need to be compiled, sent to them and answered quickly, so feedback can be given the following day. Trainees should attend all training days and must catch up in other ways if it has been inevitable to miss one session or day. 14.2 Opening module speaches, importace, introduction big shots, try to keep short as much as political introduction of team, sure, can roatate, but people will not get to know ech other, will do in group practices and lunch etc. instead a short ice breaker At the beginning, introduce the survey, sketch out the training and field work plan and set the training rules. Keep it short so you can focus on content during this day. Introduction to organization, the survey, the project/study, etc. Do not go into too much detail here and limit it to the essential. Interviewers do not need to know about the details of the analysis plan or the full history of surveys in the country. You can provide more details in the manual and refer trainees to it. Training and field work plan. Run through the training plan, where to be when, when the selection will take place and where and when field work will take place. Importantly, this should be a refresher, NOT be the first time trainees learn details about their pay, other terms or the field work. Make sure all trainees have understood and signed a copy of the general terms prior to the training. Ask trainees to write their name on a name tag and carry it during the training. You have little time to learn who is who and being able to connect names with faces helps to get an overview quicker. Set training rules. You can also print/write them on a poster. Examples: Be on time. Trainees must be on time in the morning and after breaks. Trainers should respect the schedule and not run over time. Late comers in the morning or after breaks could sing a song in front of class or donate to the cookie jar. No phones, no side activities. Trainees must pay attention and should not use their phones or have unnecessary side conversations. Phone users could donate to the cookie jar or you could collect a phone and keep them until the end of the day if used repeatedly. Trainees are responsible for their learning. Trainees can read the manual, practice outside the class, learn from peers etc to make sure they understand everything. It is no problem - and is actually encouraged - to ask questions during or after the training, but it will be a problem to not understand something and not say anything. There is a transparent selection process and they must pass it. Zero tolerance for cheating. This is a high quality survey that collects valid data. Anybody being caught making up data or purposefully breaking protocols will be immediately dismissed. There is nothing to fear as long as one follows what has been taught during training and communicates special cases to the survey management. MICS: The opening day should be kept light, but also set the stage for the task ahead: - All trainers and members of the survey management should be introduced. - One or more high level officials should be invited to give opening speeches. - Each participant should introduce themselves (keep it short). - The survey should be introduced along with key concepts and norms for the training and the agenda. - Trainees should be briefed on all logistical arrangements and payments, so that they are not distracted by these details during the training. 14.3 CAPI use You can train the questionnaire directly in CAPI if you are using Survey Solutions or another CAPI software that provides a good overview over the questionnaire, makes routing behavior clearly understandable and allows for free navigation. This has the advantage that trainees do not need to learn how to navigate in CAPI the questionnaire they learned in a different format. They only learn it in the format they will use in the field, and will practice CAPI along the way, making them much earlier confident in using it. Train CAPI just before trainees need to use it for the first time. If you are training the questionnaire on CAPI, introduce the CAPI software just before the questionnaire reading starts. If you are training the questionnaire on paper, train CAPI just before trainees practice for the first time on the tablets. This chapter is written keeping Survey Solutions in mind, but the same principles apply to other packages. CAPI, as any other software, is best learned by using it. Covering lots of functionality in an extensive theoretical session tends to be very ineffective as functionality that is not immediately put to practice is forgotten and needs to be retrained. It is more productive and a better use of time to initially only train the functionality that is necessary to proceed with the training, and to introduce additional functionality as required throughout the training. For example, trainees only need to learn how rejected interviews work once they need to deal with it in the latter part of the training. Limit the functionality you train to what is relevant from the perspective of an interviewer and what will be used in the actual survey. Trainees do not need to learn details about the supervisor interface or central server. The fewer slides you have in the CAPI training and the more demo and practice you do the better. Train CAPI functionality by demonstrating it on the big screen and asking trainees to follow on their own device. Make sure the class can follow and wait for those falling behind to catch up. Those lost should check with their neighbors or raise their hand, so one of the trainers can come to assist. If training the questionnaire directly in Survey Solutions, train on the following in the initial introduction to CAPI: A general overview, interviewer server, assignments, files, marking as complete, synchronizing to the server. Keep this part short. How to log-in to the Interviewer app. Make sure tablets and accounts have been set up prior to the training. How to create an interview file from their assignment. The general overview of the application and how to navigate through the questionnaire. The outline of a question, the question number, the question text (what to read and what not), the instructions, the answer options (different by type). Question enablement. It is sufficient to work on the first section of the questionnaire. The complete screen, marking an interview as complete. The started and completed screen on the dashboard, reopening an interview file, discarding an interview file. Having completed the above steps, trainees should be set-up to follow the questionnaire during the questionnaire reading sessions. The remaining functionality is best trained throughout the training on an as needed basis. Train: Different question types and roster forms during the questionnaire reading sessions, e.g. list questions in the household member module, yes/no questions in assets, etc. Trainees need to recognize and understand the difference between single select, multi-select, numeric, list and date questions, and understand the rosters. How to synchronize to receive assignments, create new interview files, review interviews on the complete screen, marking them as complete and synchronize to send a file. If you are conducting the tests LINK on Survey Solutions, a good moment to train this is just prior to the first test, as interviewers will have to synchronize to get the assignment and submit their test. How to add comments in the CAPI should be reviewed prior to group practice or practicing in the field, so that trainees can leave questions and comments directly in the questionnaire when practicing. How to receive rejected interviews, respond to comments, etc after the field test or the dress rehearsal, so trainees can practice it directly in interviews they have created. See details LINK TO SECTION. 14.4 Questionnaire content During the training, all questionnaires must be read and discussed in full, including all related definitions and concepts. Questionnaire reading sessions can be extremely dry and a waste of time if implemented badly. Follow below points to ensure the sessions are fruitful: Allow for enough time. Do not underestimate the required time. Some survey firms, especially when coming from opinion surveys, only allocate a few hours for a single quick read of the questionnaire. This is not sufficient for complex socio-economic surveys where interviewers need to understand a wide range of concepts, definitions, and scenarios to correctly administer the questionnaire. Avoid long blocks of questionnaire reading. They are an information overload and make it impossible for trainees to internalize the content. Spread the questionnaire reading over several days and let trainees regularly practice the newly learned material in between. Apply the theory. Definitions for survey concepts such as household, wage employed, self-employed, parcel, etc. are very theoretical and hard for trainees to translate into the real world. Illustrate them with examples and scenarios to make them tangible for trainees. Keep the content together. Explain concepts and key definitions as they come up in the questionnaire, so trainees immediately learn how to apply them to the questionnaire and can easily remember them. Covering all definitions and concepts up front is abstract and out of context. Avoid chalk and talk, i.e. trainers speaking or presenting in front of the class without much interaction. Involve the trainees as much as possible for example by making the questionnaire reading a conversation. Select a trainee to read aloud the question, answer options and the manual entry. Ask them to explain in their own words what they have understood. Correct them if needed. Ask the rest of the class for input. Probe the trainee with an example or scenario. Train using the Interviewer Manual (LINK). You must get trainees used to consult the manual to get additional information or to clarify doubts, or the manual will never be used. Ask trainees to keep the manual open on paper or as softcopy on their device. After reading each question, ask trainees to check if there is a corresponding manual and read it. Assign one of the trainers to update the manual directly during the training if the entry is unclear or insufficient. Ask for input from the class if needed and confirm with them if the update is clear. Distribute updated soft copies of the manual to the team every morning/evening. Use your time effectively. There will always be a few rare cases in the population that the questionnaire does not cover well. Trainees love pointing those out, e.g. But what if .? Get a feeling if these are frequent enough that they require you to update the questionnaire. If not, tell trainees to leave comments in the questionnaire if the situation arises in the field, and move on with the training, so that you have enough time to focus on the important concepts. If longer discussions arise on a certain point, clarify and stop the discussion. If you are not sure how to answer, get back to them the next day after having read up or consulted with the design team. Train the questionnaire directly in CAPI without training on paper first, if you use Survey Solutions or another CAPI software that provides a good overview of the questionnaire, makes routing behavior clearly understandable for interviewers and allows for free navigation (Do NOT do this with software that displays one question per screen). Training first on paper requires trainees to learn the instrument in two formats and often delays trainees becoming confident with the CAPI version. By training directly in CAPI, trainees only learn the questionnaire in the interface used in the field and learn from the beginning how to use and navigate it. It also reduces the material that needs to be covered. If the questionnaire is administered in other languages than the training language, display the questionnaire in the training language and ask interviewers to look at it in the translated local languages. For each question, ask it to be read in the training and local languages. This serves to verify the translation and to familiarize trainees with the questionnaire in the local language. Update the translations if anything needs correction. Ask the class for inputs or a dedicated person to help if there are many issues slowing down the training. As trainees read questions loud, as a mini role play to mimic the interviewer-respondent interaction. Check for their interviewer intonation way of asking tone of voice etc. Give feedback and ask interviewers to repeat until correct. MICS: Training sessions for each module should start with an overall short introduction to the module topic conveying why the topic is of importance. This introduction can be done by the topical expert who needs to be instructed to be concise. The session should then proceed by a detailed description of each question with instructions on how to administer the question and follow the skip patterns. This session is done by MICS trainer, who will use examples to illustrate flow of interview. When appropriate, sessions must address established ethical protocols, ranging from how to handle private information or specifically how to deal with respondents volunteering information on unlawful behaviour either by themselves or others, or protocol when observation or measurement reveal critical conditions. Every session should end with time allocated for questions and answers.  Unfortunately, due to budget and time constraints, often changes have been implmented to the questionnaire or processes between pre-test and training that have not been field tested when there are still changes to the instruments or were introduced pre-testing was rushed or no full-scale pilot conducted prior. The training makes . In those circumstances, interviewer training also effectively serves as a field test of methodology. Crucial to reduce of specification error. something on translation verification correct mistakes if you spot any. 14.5 Practicing Trainees should frequently practice learnt material in the form of group practicing, with respondents on-site or in the field. Different practice forms are not mutually exclusive. They fulfill different purposes and should occur at different stages of the training. mini role plays, 14.5.1 Group practice Frequent group practice sessions are important for trainees to familiarize themselves with the questionnaire, practice the use of CAPI, learn from peers, and to break-up the monotony of questionnaire training. Hold group exercise every day after new theoretical content has been introduced, so trainees can put it to practice and internalize it better. On days that are theory heavy, such as the questionnaire reading days, do not wait until the end of the day or until the questionnaire has been covered fully. Instead hold 2 or 3 practice sessions per day at natural breaks (e.g. between sections) to practice the material/sections covered just prior. The change of training modality helps to maintain attention levels. Immediately repeating material helps trainees to understand and internalize it and to uncover any questions. Practicing more frequently becomes infeasible as each practice requires time to setup. Group exercise can be effective for pairs of trainees or small groups of 3-8 trainees. Change group composition between exercises. When allocating trainees to groups, mix trainees and break up clusters that formed in the class, so that low/high performers and those paying more/less attention are distributed. Try to be efficient and not to waste time when creating groups. For pairs, ask trainees to practice with their peers in the row in front/behind. For groups, work out the number of groups n needed, and repeatedly count loud from 1 to n, pointing at a trainee for each number. Each number corresponds to a group, and each number/group can meet in a different point of the training venue, e.g. All number 4s, meet at the blackboard. Ask groups to practice the questionnaire sections that have been covered since the last practicing session. If there is time, groups can also practice all sections learned to date. They should not work through parts of the questionnaire that have not been covered yet. Give scenarios for trainees to ensure they practice scenarios that are likely to occur in the local context and cover all parts of the questionnaire. For example, when practicing the labor section, ask to complete it for an employed individual, self-employed, supporting family worker, etc. Within each group, one trainee should read and ask the questions while another one answers them. After a few questions/sections the roles should be rotated, so each trainee had the opportunity to act as an interviewer or respondent. All trainees in a group should follow on their own tablet and record the answers, which at the end should be identical for all trainees within a group in theory. Ask groups to submit their interviews after the feedback session to motivate them to get it correct. Feedback after practicing sessions tends to be thin. Asking the class if there were any doubts, questions or comments often results in general silence. In order to receive better feedback, ask groups to actively come up with feedback points, note them on a sheet of paper including the trainees names, and to submit those to the trainers at the end of the exercise. Feedback points can be any question or doubt they have about questionnaire, definitions, scenarios, etc or any mistakes they have found. You can set a minimum number of feedback points, e.g. 3 during the beginning of the training when things are not clear yet, and 1 towards the end of the training. Trainers should observe groups that have been allocated to them, correct mistakes and help struggling trainees on the spot, but also take note of the issues they observed for the feedback session. They should consider any issues, including the way questions were read or answered, the intonation, misunderstandings, questionnaire navigation, CAPI use etc. While observing, trainers should also record trainees skill levels (see Chapter SKILLS) and give individual feedback where necessary. Debrief with the entire class immediately after the exercise. Ask groups for their written feedback and discuss and answer their questions in class involving input from other teams/trainees. Ask trainers to provide the feedback from their observations, explaining what was done wrongly and also how to do it correctly. Ask trainees who have been observed to do something well to demonstrate to the class (e.g. how to explain a concept/question), so that all can learn from good examples. 14.5.2 On-site respondents While group practicing is a good way to get trainees familiarized with the instrument, the scenarios and concepts remain hypothetical and theoretical until trainees learn to apply them with actual respondents. The first exposure to the real-world often is during field testing, which can have some logistical downsides: It is time consuming to move trainees to the field and allocate them to respondents willing to participate. Also it is impossible for trainers to observe more than a handful of trainees who are scattered across many homes. Practicing with real respondents on-site is a great way to give trainees more and early exposure to field conditions, and for trainers to observe all trainees conducting interviews. The best timing for an on-site practice is after the questionnaire has been covered completely and be practiced in group practicing, and prior to the first field test. Hire a group of respondents to visit the training venue or some other convenient location to be interviewed there. Depending on the number of trainees, some 5-10 respondents are usually enough. They should be similar to respondents in the surveyed population, e.g. cultivate land and keep livestock in agricultural surveys, or have jobs or small businesses in labor force surveys. Make sure to compensate respondents adequately, pay for the transport and refreshments or to invite them to the training lunch should it be provided. For the practicing, group trainees into as many groups as you have respondents, ideally not more then 4-6 interviewers per respondent. Each group should conduct a full interview with the respondent. Each trainee should follow on their own device and record answers, and they should take turns in asking the questions, e.g. a few questions or a section at a time. Groups should support each other if individuals fall behind, have questions or make mistakes. Ask groups to submit their interviews after the feedback session to motivate them to get it correct. If there is sufficient time, groups can switch respondents and conduct a second or third interview. Collect feedback, observe and debrief as described above in Chapter GROUP PRACTICE. 14.5.3 Field practice The purpose of field practice is for trainees to get first experience in implementing the questionnaire with real respondents. There is no need to completely mimic field conditions as one does in dress rehearsals. Rather, aim to maximize the number of interviews each trainee can conduct and the number of respondents and scenarios they are exposed to. Prepare them well, observe trainees and collect good feedback to make them a success. Field practice sessions should be held after the questionnaire has been trained and practiced in class, and before the final dress rehearsal at the end of the training. Often it also makes sense to conduct it prior to sessions on household tracking, receiving rejecting interviews, logistics etc, so that content is broken down more and the field test can focus on the understanding and implementation of the questionnaire only. If the questionnaire is really long, e.g. a LSMS-ISA style questionnaire with long household and agricultural questionnaires, you probably want to conduct more frequent field practices for different parts. Field practice often is implemented inefficiently with a lot of time being spent on logistics that could have been prepared in advance: Teams wait for transport, for community introduction, look for respondents, etc. It is not uncommon that a whole day is spent for trainees to conduct one 1 hour interview in the field or that trainees could not practice as they did not find respondents. Prepare well, so trainees can practice interviewing, not waiting. Visit communities that are not too far from the training venue and large enough to have enough respondents for all trainees. Communities should have similar characteristics to the surveyed communities, but be outside of the sample. Contact or visit the communities the day prior to the field practice, do the community introduction and arrange for respondents at the time of your expected arrival, e.g. with the help of a community leader. Compensating respondents for their participation often helps with their availability and willingness to participate and is totally fine during field practicing. Again, the aim is for trainees to practice the questionnaire as much as possible with respondents. Try to minimize other logistical issues that could cause delays. Make sure all tablets are fully charged, synchronized and have battery packs available if needed. Arrange for food and drinks for trainees in the field or ask them to bring it. Make sure your transport arrives on time, knows the way and does not need to stop for petrol. Try to arrive early in communities, as respondents tend to become unavailable during lunch hours. Group trainees in pairs of 2, trying to take skill levels into account and pairing high with low performers. Allocate each pair with a respondent. Agree on a meeting place for trainees to return to and a time by when they need to be back latest. Ideally, there is time for more than one interview per pair. Similar to the on-site group practices, trainees should take turns interviewing the respondent and both record all answers and submit the interview file at the end of the day. Ask them to write down any issues they experience while they conduct the interview. Working in pairs, one of them can take notes while the other one is interviewing. For each interview, each pair should list at least one question or doubt they had, and something about their interview or respondent that was noteworthy and is useful to share with the class. Try to observe as many of the pairs as possible, help and correct where necessary, mark their skills (LINK), and note down any issues you have observed as general feedback for the class. Focus on trainees that are borderline of being selected or that have not been marked yet. Collect as much feedback as possible from trainees before you leave the community. It becomes much harder later and the quality of the feedback decreases rapidly. As pairs of trainees arrive back at the meeting place, ask for their written notes, debrief quickly with each pair, clarify any issues they had, and record general feedback for the class. Often trainers assume that the questionnaire is working well and that interviewers are able to administer it correctly because they did not report any issues during feedback sessions. This assumption is wrong. Many issues cannot be identified by interviewers, are forgotten or not reported. Those reported are biased towards trainees concerns, e.g. they are quick to report something that caused extra effort from their side. Trainers must observe interviews being conducted and compile their own feedback. To receive meaningful feedback from trainees, trainees must note down issues or questions as they experience them and must be debriefed shortly after conducting interviews. After each field test, hold a general debrief session with the entire class in which you respond to the written feedback from the team, answer any questions, and provide feedback from your observations. This might involve short retraining of concepts or questions that have been ill understood, practicing or role plays. Clarify all issues prior to the next field test. It is normal for quite a few things to go wrong in the first field test and to improve quickly. Sometimes, you can debrief to some extent directly in the field in an empty classroom, a community center, the shadow of a big tree, etc. This way interviews are still fresh in everyones head and there is no need to go back to the classroom after the transport back from the field. If you are required to debrief the same day in the classroom, make sure to leave the field in time and to allow for a long break, as everyone tends to be really tired after field practicing, especially after the transport back. Design your training schedule such that field practicing days are followed by classroom days, so you can have a detailed debrief session in the classroom with the big screen and other all required facilities. 14.6 Interviewing techniques In addition to understanding the questionnaire content, interviewers must master a range of interviewing techniques that are crucial to collect good quality data. Some examples are: Introduction and interview request: Being able to convince respondents to participate in the survey is key to keep unit non-response error low, particularly in urban areas or other settings with high refusal rates. Interview continuation: Being able to keep respondents engaged in lengthy interviews and stop them from prematurely ending the interview is important to minimize item non-response and measurement error. Inquiring: Respondents often do not understand or misinterpret a question, give incomplete answers, respond I dont know or do not want to disclose some information. Interviewers need to be able to get respondents to answer all questions in a neutral and non-leading manner. Controlling interviews: Respondents sometimes digress, give lengthy explanations, or talk about other things. Interviewers need to be able to lead the conversation in a pleasant and courteous manner to maintain the respondents cooperation. Unfortunately, these interviewing techniques are rarely trained. Instead it is assumed that interviewers automatically know them or will learn them over the course of the survey. In reality, while some interviewers are experienced or naturally good at it, many others struggle or resort to undesirable practices such as making wrong promises to obtain consent or leading responses. This can exacerbate interviewer effects and lead to high levels of measurement error and unit and item non-response. Teaching trainees that are not good a task the skills of those that are mitigates these effects, e.g. see Groves and McGonagle (2001). Teaching interviewing techniques goes beyond theory. All features of communication are relevant for the interaction between interviewers and respondents, including the choice of words, tone of voice, volume, as well as attitude, facial expressions or gestures. To learn a technique, trainees have to see and hear what they should and shouldnt do, and learn from good examples. To be able to put techniques into use, trainees need to repeatedly practice and receive feedback until they are able to (re)act correctly and quickly.. Which interviewing techniques are best trained depend on the survey. Interviewers can face different challenges in the field. For example, refusals may be rare in a household survey, but very frequent in a enterprise survey, requiring dedicated training on how to address them. In addition, best practices often are sensitive to the culture and context, e.g. how to inquire if respondents are hesitant to disclose their income. Therefore, the training agenda on interviewing techniques usually needs to be tailored to each survey. 14.6.1 Identify challenges and responses To establish which interviewing techniques are most relevant for the survey and should be trained, first compile a list of the key challenges that interviewers are expected to face in the field and need to be prepared for. Focus on those that are most common and have the biggest effect on survey error. Consider any response, concern or action by the respondent or any other situation that can negatively affect data quality if interviewers do not apply the right technique. Examples are: respondents refusing an interview stating reasons related to time, etc. other members trying to listen to sections to be administered in privacy respondents unable to quantify consumption amounts in units long and damaging breaks if tablet needs to be restarted error messages flagging gross mismatch in harvest quantities, land size and sale prices Record different ways in which a challenge may manifest itself or how the interviewer can recognize them in the field, using colloquial terms or used by respondents where it applies. As an example, respondents may say I am really busy, I need to leave or I need to cook/wash/ when refusing and stating reasons related to time. For each challenge, identify good interviewer responses. These can include verbal responses actions or procedures. Often, there is no single response to a challenge that works equally well with all respondents and in all circumstances. Good interviewers often have a repertoire of responses and tailor them to observable features of the respondent or the context. For example, if a respondent refused based on time and appears to be rushed or pre-occupied about something (e.g. fighting children) the best response tends to be to ask for a different moment, e.g. by saying I see that now is difficult. When would be a good moment for me to come back?. However, if the respondent refuses because they have to do a chore but seems generally collaborative, it is often good to offer to do the interview while they do their activity. For each challenge, also record bad practices or responses that interviewers should avoid, if there are any. As an example, if respondents do not know the quantity consumed of an item, a bad probing practice would be say Was it maybe 1 kg?, as this is leading the respondent to answer 1 KG. Allocate challenges and responses into separate modules by topic, importance and anticipated time required to train. It is usually good to have at least one module on Introduction and interview request, one on Inquiring and controlling interview and one on Challenges observed during field testing. You might need additional modules on key questions or sections, such as Estimating land size or Conducting consumption module. Ideally, challenges and responses should be compiled prior to the training to facilitate planning and make training sessions more productive. Use a combination of any of the following sources to identify challenges and responses: experience from previous surveys with similar context, populations and subject focus groups with experienced interviewers observations and feedback from pre-test or pilots If you are unable to prepare prior to the training, use challenges and responses observed during field practice sessions or hold short focus groups with the class at the beginning of each module. See HERE be a core set of challenges and responses that are common to many surveys. &amp;&amp; NOTE: WOULD IT MAKE SENSE TO COMPILE ONE? 14.6.2 Train &amp; Practice Schedule modules on interviewing techniques just prior to field practice and final field tests so trainees can apply the techniques soon after. Additionally, you might need to schedule some modules after field practice and field test to practice new situations that came up during field practice or retrain those that do not yet work well. Module between 45 min and 1.5 hrs tend to be sufficient to cover one core topic. For each module, first train interviewers on what challenges are likely to occur and what the best response are. Project the list of challenges, categories and good responses on the big screen. Work through each category, identifying together with the class the ways in which challenges may manifest themselves and any tips and tricks for a good response. Also cover bad practices that interviewers should avoid. Ask trainees that are good at a technique (experienced or good inter-personal skills) to demonstrate the best practices in front of class in a role play. Give feedback if necessary and ask the class for input to get consensus and make sure responses are appropriate for culture and context. Try to keep this part short. After having covered the theory, drill trainees until they are able to respond to a challenge quickly, correctly and in a natural manner. This is best done by asking trainees to simulate the interviewer-respondent interaction in front of class in a role play, providing immediate feedback and letting them repeat if necessary. Depending on class size and skill level, it might be best to divide the class into large groups, each supervised by a trainer. To practice, select one trainee to act as interviewer and if needed another one to act as respondent. Usually, it is more useful for the trainer to act as respondent as it allows them to better drive the conversation. Ask the selected trainee to come to the front or just stand up and speak towards the class, which is often faster. Explain the scenario and challenge and ask the trainee to re-enact the interviewer response as if it was a real field situation, speaking loudly so the entire class can hear. Give feedback on what they have done well and what not. Ask the class for inputs. If the response could be improved, ask the trainee to repeat the response. Collect feedback, correct them and ask them to repeat until the trainee does it well. Practice with as many trainees as possible or until the response generally works well. This can be quite a lot of fun and is a good way of breaking longer theoretical sessions. Two examples to illustrate: To practice introduction to the survey, select one trainee, ask them to stand up, tell them that they just knocked at a door, a certain type of person opened, that they should introduce themselves and ask for participation. Check that they are giving all necessary information, say nothing wrong, are easy to understand, use the right tone of voice, have good facial expressions and gestures, etc. Collect feedback from the class. Ask the trainee to repeat their introduction until it is correct in content and well presented. To practice asking for participation, ask a selected trainee to make an introduction and ask for participation as above. Act as respondent who is unwilling to participate, stating one of the challenges discussed in the training, such as such as I am really busy now or What do I get out of this?. Ask the trainee to respond and convince you to participate. Again, together with the class, check if the trainees response was quick, correct and natural. Give feedback on possible improvements and let the trainee repeat their response until satisfactory. 14.7 Special tasks For some surveys interviewers need to learn a special task that requires repeated and structured practice until they can perform it correctly and on auto pilot. As a trainer, you need to identify trainees who are struggling with the task and be aware of scenarios that cause issues. Examples for special tasks are: Plot size measurement Anthropometry Collecting samples, e.g. to measure water quality Extracting information from registries, records, etc. Behavioral games Typically, these tasks require interviewers to strictly implement a protocol, use additional tools or aids or understand differently structured documents. Explaining the theory and going through one example is not enough. You need to practice in a structured way and evaluate trainees, exposing them to different scenarios if necessary. The best way of doing this depends on the task at hand. One option is to run through scenarios together so all trainees practice the same thing at the same time. For example, if interviewers need to record details from school time tables, display a picture of an actual timetable from a school on the big screen, and ask trainees to record the required information on their tablets. Run through examples of different time table formats, so interviewers get used to different scenarios. Walk through the class and identify any trainees who are struggling. Provide feedback and discuss in the wider class if there is any confusion. If interviewers need to perform a task on automatic pilot, for example, operate testing equipment, ask the class to repeat it over and over until everyone can perform it correctly all the time. Another option is to set up workstations that interviewers need to go through. Set up different scenarios in or around the venue and staff some with trainers if necessary. Trainees go from station to station and complete the respective task. At the end, each trainee should have undergone the same scenarios and a general debrief can be held. Trainers can observe each trainee at their respective station, evaluate and provide personal feedback. Set up stations such that interviewers spend their time practicing, not queuing. Examples for workstations can be plots with different shapes and cultivation, trainers playing common scenarios in behavioral games, or different documents that need to be evaluated. 14.7.1 Specialized teams If a task is very complicated, requires special background knowledge or if there is not enough time to train it well to all interviewers during the training, field work model permitting, it is often best to specialize some fieldworkers to exclusively conduct the task and distribute them across the field teams. Examples where this tends to be useful are: Anthropometrics, especially when taking all measures, i.e. weight, height/length and middle upper arm circumference (MUAC) Learning outcome assessments used in school surveys that include reading, writing and math tests with strict administration protocols. Medical tests, such as taking blood pressure or measuring hemoglobin values. This often requires the use of specialized equipment and trained nurses. Training any of those tasks can take several days. For example, learning outcome assessments require trainees to handle test booklets, CAPI and a timer all at once while keeping a young child engaged. It usually takes days to practice until all trainees can conduct the assessment tests in a homogenos way, and not everyone is good at it. Train selected trainees in separate sessions in parallel to the interviewer training. Practice all components of the task, including the equipment use, measurement reading, and interaction with the respondent. Invite practicing respondents to the training venue to practice the human component and create closer to field conditions. For more details, see section Inhouse respondents. For anthropometrics, the components that need to be practiced include the correct setup of scales and height/length boards, taking the measurements, how to treat the children/babies, reading the measurements (careful, some are dislexic), and recording them. At the beginning, the measurements taken for the same person differ quite a bit between trainees. Practice until they converge after a few days. parallel sessions 14.8 Responding to inquiries In some surveys, interviewers receive feedback or inquiries about interviews they have submitted that require them to take some action or respond. In Survey Solutions, this is done using rejected interview files. This often goes wrong in surveys and needs to be covered in the training if implemented in the field. There is a big risk that interviewers fix interviews by simply changing answers to make issues go away, without actually addressing the underlying problem. Interviewers need to understand what they must and must not do. Train and practice in a dedicated session towards the end of the training. Ideally, trainees can practice with actual examples from interview files they have produced themselves. This makes it less theoretical and shows trainees that they made mistakes and what they were. You can use interview files from the field test (LINK) or even better, the final dress rehearsal (LINK), as interviews will generally be of better quality. This implies that the session needs to be held towards the very end of the survey. Prior to the session, carefully review submitted interviews and identify any issues. Normally, at the beginning there tends to be at least one per interview, make up some if there are none. Write comments and feedback as would be written during field work and reject the interviews to the interviewer. Try to do this with at least one file per trainee/pair. During the session, first give a quick general introduction to explain the rejected files tab on the interviewer dashboard, how to open a rejected file, how to read the rejection comment, how to see all comments by the supervisor, how to react to rejected files, respond to comments and resubmit them by marking them as complete and synchronizing. Then, project a rejected interview to the main screen and ask the respective trainee/pair to come to the front. They should explain what led to their mistake and what they should do if this occurred in the field. Ask them to respond to the issues and resubmit the file on screen. Ask the class to help and comment. Repeat this with as many files/trainees as possible and useful. Ask all trainees who did not solve their file on the big screen to solve it on their tablet and resubmit them. This exercise is also a great way to identify and streamline the rules around responding to rejected files, revisiting respondents, etc. For more information see chapter REJECTING FILES. 14.9 Final field test (often called pilot) In a dress rehearsal all survey questionnaires and protocols are tested under real-field conditions prior to field work. It is an important exercise to: Expose interviewers to real conditions and give them the opportunity to practice without affecting the sample Determine if the field team is ready for field work Identify and address any outstanding issues with questionnaire or field procedures Some surveys do not conduct a separate pilot prior to the training (link chapter PILOT) and refer to the dress rehearsal as pilot. In those cases, the dress rehearsal importantly serves as a final reality check for questionnaires and protocols, since they have not been tested in full in field conditions before. The dress rehearsal should be conducted at the end of the training and prior to the field work. Usually, 2-3 days of dress rehearsal are sufficient. If short debriefing/feedback sessions can be held in the late afternoon of each day, they can be on sequential days. If not, schedule debriefing/feedback days between field days. Allow for at least one class room day after the dress rehearsal for an extensive feedback session and to teach the class on how to deal with rejected files / inquiries from HQ. Additionally, you might need to allow for additional time for admin, rest days or travel to the field location. To create as close conditions to the field as possible, the dress rehearsal must be conducted with units that are similar to those in the survey sample, but crucially that are not part of the sample. For household surveys, this usually means conducting the dress rehearsal in un-selected communities within the survey areas and often requires the team to travel from the training location closer to the field. For surveys part of RCTs, this means practicing within the treatment arms. Ideally, locate the dress rehearsal such that the entire field team can be supervised and debriefed together in a central location. This ensures that all field workers have received the same inputs and reduces team effect. Split the dress rehearsal into different locations if there is a large variability in the sample that affects questionnaires or protocols. If doing so, try to streamline as much as possible the supervision and feedback given to all groups, e.g. by trainers exchanging notes before providing feedback to their respective group. Often it is easiest to select a set of units (e.g. communities) for the dress rehearsal during the sample design and have them undergo the same preparation steps as the sampled units, e.g. assignment of IDs, procurement of reference data, add them to the CAPI questionnaire, do community sensitisation, etc. For surveys with a separate listing exercise or panel surveys, conduct the dress rehearsal for all stages using the same units and keep the data, so you can properly prepare and test each stage of the survey. It can be very laborious and cumbersome to prepare separate data. One important aspect of dress rehearsal is for field workers to practice the selection, identification and tracking of survey units such as households, convincing respondents to participate and to learn how to make appointments and to manage time. Do not pre-arrange respondents as one would do during field practice. Equip interviewers with the same assignments or tracking sheet format they would receive during field work. If you do not have lists or names for the rehearsal units (e.g. you have the household names for sampled communities, but not for practicing communities), send someone to the community before the dress rehearsal to compile lists that you can use to produce the assignments/tracking lists. Allocate interviewers to their expected field teams and let the supervisors assume their management role, including the assignment of cases to interviewers. For the dress rehearsal, often it is beneficial for interviewers to conduct interviews in pairs. This reduces the number of cases required in any of the visited communities, increases the probability of finding available respondents and allows interviewers to help one another. Each pair can conduct multiple interviews per day and take turns in administering the questionnaire. As in the field tests, both interviewers should record all answers in their own tablet and submit the interview in the end. For the dress rehearsal you might need to relax replacement protocols and add additional replacement units, so interviewers can practice conducting some interviews even if nonresponse is high. Make sure field teams arrive early in the communities, so that interviewers have enough time to locate cases and conduct interviews before respondents start cooking/having lunch and become unavailable. As for the field practice, make all necessary logistical arrangements to prevent delays or hungry/thirsty field workers in the field. Collect feedback and debrief as described in chapter FIELD PRACTICE. Ask all interviewers to submit their interviews at the end of the day. Use the data to finalize your data system (link DATA SYSTEM) and quality control system, e.g. by updating the data checks. Review submitted interviews using the standard processes of the quality control system. In addition, visually review as many cases as possible and compile feedback for interviewers and the quality assurance team. The data collected during dress rehearsal is a great way to learn and practice receiving feedback with the team, see chapter feedback. 14.10 Supervisor Training check DHS, has some material 14.11 Data Monitor Training 14.12 Trainer meeting review feedback, review test results progress include predetermined supervisors References "],["assessment-selection.html", "Chapter 15 Assessment &amp; Selection 15.1 Written Tests 15.2 Evaluating soft skills 15.3 Specialised tasks 15.4 Field worker selection", " Chapter 15 Assessment &amp; Selection Often field worker selection is done by the implementing organisation that might follow their own procedures. Be cautious as the selection criteria and processes might not result in optimal outcomes. We recommend the following procedures. However, the processes described here are highly recommended. Try to exert some control over the implementing agency and make it conditional. ultimate means to get to knwo interviewers so you can select them. last stage of interviewer recruitment process. Past survey experience, CVs and interview often not enough during recruitment process to identify those that are good at being interviewers form those that are not. Motivation, etc can only be observed over a sustained amount of time. Selection rpoecess should be at the end. ?? Especially when working with unknown teams of trainees, it is beneficial to not determine roles up-front, but to select trainees into field worker roles (interviewer, supervisor, etc) based on their skills and knowledge. Long term supervisors sometimes develop a sense of entitlement or superiority, dont pay attention as much and as a result dont know the material as much as interviewers. Sometimes they also dont hold their position due to other factors. See more on sections FIELD WORKERS and SELECTION. if pre-determined supervisors data mmonitors or other roles, they have to attend, contribute to and demonstrate understanding of the material in order to take on supervisory roles. here make overall process clear again: train excess evaluate on ongoing basis, content and skills select at end of training rationale: - allow selection of best (only capable ones), discard - general motivator during the training, as trainees understand ti is serious and merit based selection, same attitudes transmit into field work - better performance: people do better if they know it matters and and cared about - selection into roles based on skills, capacity, best fits, supervisors, monitors etc. - transparent and objective selection, fair, if applied over repetaed surveys overall improvement of interviewer stock. - creates pool of trained fieldworkers that can be brought up to speed relatively quickly if there is field worker attriction during field work. 15.1 Written Tests measns to test caapcity also make sure trainers and designers thinking is aligned with that of the team very powerfu to show trianees what they do not know and can be agreta motivator to pay attention, participate in the training and learn. Conduct written tests on a regular basis throughout the training, i.e. daily or every other day during class room days. On days with in-field practicing it is often not feasible to conduct tests, and there tends to be less new theoretical material that can be tested. 15.1.1 Designing tests Tests should probe the understanding of the questionnaire content and manual covered in the training so far, including protocols, definitions, answer options, discussed etc. but also for general skills like calculating the percentage or average if needed for the survey. If possible, tailored tests to the discussion of the (previous) day, so that they also probe for trainees attention. Repeat questions or topics from previous tests that a large share of trainees answered wrongly. Write questions clearly and that they can be clearly answered unambiguously. Good ways to ask questions in the test include: Describe a difficult scenario, followed by a question of the questionnaire, asking trainees to record or select the correct answer. Write statements and ask if they are true or false. List things and ask trainees to select all that apply, e.g. select all those people who would be household members according to a definition, all the jobs that are considered a self-employment, etc Avoid putting many questions with a simple yes/no or true/false answer. They can be answered 50% correctly by just randomly selecting an answer. Include numeric questions and multi-select questions that require a combination of answers to be selected to be correct. Dont make it too obvious. Sprinkle irrelevant or misleading information into the question text or scenarios. Add Dont have enough information as an answer option. For multi-select answer questions, make sure the wrong options are plausible, and occasionally use only correct or wrong options. Avoid open-ended questions, unless you have enough resources to review and score them for all trainees. See here for examples of tests that have been used in some training sessions. IFAD PACKAGE TEST, ANY OTHER? Aim for tests that take 15-30 minutes to answer so you get enough data points while not taking too long. That translates to around 10-15 difficult questions, some with multi-response answers. 15.1.2 Conducting the test Written tests have been conducted in the following ways in interviewer trainings: Test printed on paper. Do not print all tests up front, as they probably need to be updated each day. Print in the training venue or bring a printer. Questions on screen and note answers on a sheet of blank paper. Very flexible and useful for spontaneous tests as it allows last minute updates. Trainees note their name, question numbers and answers on a blank sheet of paper. Questions are read out or displayed on the screen. CAPI. Only works on CAPI software that is quick and easy to code. Write the test as a questionnaire in CAPI, assign it to trainees who complete one on their tablet and send it back. Often you can copy questions from the questionnaire and modify them. The answers are immediately available as data and can be corrected using code. Trainees learn to handle the CAPI tool and submission of interviews. Make sure to have an internet connection for all trainees to sync. Google forms or similar. There are several tools online that can be used to quickly write tests, deploy them on the trainees devices and make the results available. Most of them are online based and require good internet connectivity. To conduct the quiz, disperse trainees throughout the training venue, so that they cannot copy from one another nor talk. For electronic tests, trainees only need their tablet, so can do without a desk. Walk around and invigilate the test to make sure nobody. Help those that have technical issues and provide clarifications (to the entire class) if there is any general doubt about the quiz. Usually, you can allow trainees to refer to the manual or their notes during the test. This reinforces the manual as a reference and probes if trainees can find out the correct answer with the tools they have available during the field. Give sufficient time to trainees to complete the test, even if they have to refer to the manual a few times. Those that take a bit more time are not necessarily worse interviewers. You can set an exact time limit, or just stop the test once most trainees have handed in their test and once there is no more progress among the remaining ones. There are different advantages for different hours of the day to hold the test. If holding it in the morning, trainees are freshest and have had a chance to revise in the evening if they were falling behind or absent, but it is probably best to do the feedback session right afterwards. Holding the test at the end of the day makes good use of the end of the day that often is unproductive, and also stops trainees from disappearing during the day as it effectively is an attendance call. Trainers have the opportunity to mark in the evening and give feedback the following morning, which is a good way to repeat the material of the previous day. Also, you can ask trainees who have completed the test to leave the venue , so they do not have to wait for everyone to finish. 15.1.3 Marking tests There are several options to mark tests quickly. For tests conducted on paper, collect completed test papers and redistribute them to other trainees during the feedback session. Give clear guidance on how to mark each question as you go through the test with the class, e.g. ask to put a tick or x on the left of the question number, and at the end, count ticks and write the number in the top left corner. During the feedback sessions, ask those looking at a particularly wrong answer for a question to read it out together with the name. The trainee who answered wrong can then explain to the class their thinking that led to the answer and be corrected by the class. To get a feeling on how individual questions went, ask for a show of hands of those who marked it as correct or wrong. At the end, collect the test papers, cross check the marking and and record the score for each trainee. For tests conducted on CAPI or any other electronic tool, you can export the data and write a short script to mark the test. The script should for each trainee calculate the score and for each question, give the percentage of correct answers, tabulate wrong answers and list the names of trainees who answer it wrong. Have (a draft of) the script ready, so you can have the feedback session shortly afterwards. In some tools you can specify the correct answers and do the marking directly in the tool. If using Survey Solutions, this can be done by creating variables and displaying the score in supervisor level questions or questions conditional on supervisor level questions. 15.1.4 Providing feedback For each test, hold a feedback session, ideally shortly after the test, but no later than the following morning. Do better than just going through the correct answers. Actively involve trainees who got questions wrong to learn where misunderstandings are coming from and to ensure that the corrections are understood at the end. Involve the rest of the class to correct and explain in their own words. One way to achieve this is to project the test on the screen and work through it question by question. For each question, ask one of the trainees who answered it incorrectly to come to the front, read the question, and answer it in front of class, explaining their reasoning and cognitive steps. The class should not help the trainee. Once answered, ask the class if the answer was correct or wrong, the reasons why and to provide feedback. Step in if the class response is incorrect. Make sure the trainee in front understands why they answer the question incorrectly. Sometimes discussions arise if test questions were ambiguous or if trainees feel like they have been marked incorrectly. Do not let discussion get carried away. Intervene, repeat what is correct and why and move on. If there were any issues with the test question or the marking, you can not count the question towards the overall score. Announcing this usually helps to appease discussions. Make sure to correct any issues in the manual or questionnaire if they were the reasons for the misunderstanding, as they would likely cause confusion again if unaddressed. Keep an Excel sheet or other systematic record that for each test records the total number of points possible, and the points achieved by each trainee. Calculate a total score and ranking. With tests probably not being equally long or difficult between the days, you probably want to calculate the total score as the sum of points achieved over the sum of total points possible. In the sheet, also record marks and comments from observations, see the following chapter. 15.2 Evaluating soft skills Understanding the questionnaire alone does not make a good field worker. Other soft skills are equally important and must be observed during the training to inform a good field worker selection. Examples are the ability to use a tablet and CAPI, friendliness, seriousness, thoroughness, motivation, intonation of voice, etc. Observing those skills with all trainees and evaluating their qualification as fieldworkers during the training requires you to be systematic, produce comparable scores and make good use of all sessions. Doing it just like that without a systematic marking and non comprehensively is not fair and becomes quickly unfeasible to get a comprehensive picture if there are more than a few trainees. There is benefit in identifying as early as possible if trainees do not possess any of the key skills, so you can still address it in the training, e.g. by giving additional extra tablet/CAPI practice sessions for those not comfortable enough using the tablet or software. Want to do skill matrix, i.e. know which trainee has what skills can be used together with written exams for field worker selection Benefits: find right people Identify gaps early, so can do something about it How to create a skill matrix: 15.2.1 Identify key skills To produce a comparable and comprehensive ranking, identify a few key skills that matter most for the project and cannot easily be probed for in the written test. Examples are: CAPI and tablet use Sound of voice, intonation (especially for phone surveys) Reading questions, reading speed Ability to convince respondents to participate in survey Capacity to self-organize Inquiring Friendliness and likability Determinism (especially if high unit non-response rate is expected) Trustworthiness and reliability (especially if working alone without supervision) Thoroughness Team leading and organizing (if selecting supervisors from trainees) Interaction with children and mothers (for anthropomorphic or student tests) Aim to get a rating for each of the selected key skills for each trainee during the training. Be realistic, prioritize and select only the skills that matter most, max 3 - 5, so you actually manage to score all trainees against them. If there is limited capacity and time to score against several criteria, give trainees scores for overall skills. When scoring, evaluate against the key skills, and record any diimensions in which they are outliers, i.e. perform very well or bad. 15.2.2 Design a scoring system You need to use a common scoring system to evaluate skills across trainees, especially if more than one trainer is evaluating trainees. Prepare a detailed scoring system with descriptions of each score. Make sure trainers know it and refer to it during scoring. Table 15.1 shows an example scoring system ranging from 1 to 5: Table 15.1: A skill scoring system. Score Rating Description 5 Excellent, exceptionally mastery Can be expected to perform extremely well. Can act as an exemplary role during training and field work. 4 Very good, above average More than adequate for effective performance. Possess high skill level. No counterproductive behavior or deficiencies. No major deficiencies. 3 Good, acceptable, average Should be adequate for performance requirements. Posses acceptable skill level. No major counterproductive behavior or deficiencies. 2 Weak, less than acceptable Insufficient for performance requirements. Does not possess sufficient skill level. Some counterproductive behavior or deficiencies. 1 Poor, unacceptable Significantly below performance requirements. Does not possess the skill. Counterproductive behavior. Many deficiencies. 15.2.3 Create a skill matrix Create a table with trainees in rows and skills as columns. Each cell should contain how the respective trainee scores in the relevant cell. Color coding helps to quickly generate an overview and identify generally weak skills or under-performing trainees. A quick and easy way of doing this is in a Spreadsheet. See an example here. There are different ways of filling the matrix: Trainers can record scores in their notes and copy them into the final matrix. This is easiest to set up, but cumbersome, difficult to keep track of and reconcile if trainees have been scored more than once for a skill. Print the empty skill matrix for trainers to record their scores. Copy the scores into the final matrix. Slightly better than above, but still cumbersome and difficult to reconcile. Record scores in a Google Form that contains a question to select the trainee name, one optional question for each skill, and a text field for comments. Trainers fill the form on an ongoing basis, rating only skills they have observed and recording comments. Answers are automatically stored in the Google Sheet, including the name of the scoring person and date time. Answers in the skill matrix can be calculated to be the average or last rating. Ideally, the skill matrix also contains the results for every test and the overall score, so you can get a quick overview of trainees performance. If needed, you can calculate an average score across all skills for each trainee and also combine it with the test result to obtain a single score only, but be aware that the individual components might be quite different and best not be aggregated, or might have to be weighted differently. Always also rely on the individual components if referring to the aggregate. 15.2.4 Rate A fieldworker training only provides you with a short amount of time to do a comprehensive rating of trainees skills, so start as early as possible and rate in all phases of the training. During the theory part of the training, engage as much as possible with trainees and score them against skills that you can rate based on the interaction, e.g. you can score their ability to read out questions or intonation of their voice easily during the questionnaire reading sessions. Some skills can also easily be observed during the classroom-based training by trainers not leading the class, e.g. the ability to use the CAPI software or tablets. During practicing sessions, in class or in the field, assign to each trainer a set of trainees to be observed and scored. Trainers should observe a trainee long enough to get a good enough impression to be able to score them against one or more skills, and should then move on to another trainee. Regularly check the skill matrix for completion, and target trainees that you have not evaluated yet. Rate If you can afford it, try that each trainee has been rated by more than one trainer to create a consensus decision. Give feedback to trainees. If you scored them low on a skill, explain why and that they should try to improve it throughout the training. Be aware of changes over time as trainees have (hopefully) acquired skills during the training. Scores from the beginning of the training for one trainee are not necessarily comparable to those for another at the end. Try that your scores are reflective as much as possible of the skill levels at the end of the training, as you are ultimately interested in trainees potential to perform in the field, not their learning curve throughout the training. Reconfirm extreme scores, especially bad ones towards the end of the training, and scores for trainees that are not clearly in or outside of the selection that is merging towards the end. 15.3 Specialised tasks have them do the tasks, and test outcomes. Anthro testing, compare against expert measurer. 15.4 Field worker selection Field worker selection should happen at the end of the training and be skill-based, objective and transparent. anthropomorphic standardization tests, compare individual testa against the group average and those of experts. stations, see MICS page 13/14. 15.4.1 Timing Select field workers at the end of the training and prior to the start of field work. This could be before or after the final dress rehearsal and depends on the survey circumstances. Selecting after the address allows for more time to observe trainees and for trainees to show their strength in the field. Trainees that will not be selected and act as a replacement reserve will have had some field experience. On the other hand, selecting prior to the dress rehearsal allows you to form the actual field teams already, so they can practice in their field composition and so that you can make adjustments if necessary. AK NOTES: Some trainees might deselect themselves during the training by dropping out. Prior to the selection, only dismiss trainees where it is already clear that they have unacceptable skill levels or personality traits (e.g. unrealibility). be careful to not drop trainees that might improve. Many need more time, or start to improve once they realise their relative performance levels are low. 15.4.2 Criteria The fieldworker selection should only be based on trainees overall test results, skill ratings and other objective characteristics relevant for the project. Do not select based on subjective criteria such as personal liking or criteria that are irrelevant such as having or not having a degree. How you weigh individual criteria highly depends on the project and training outcome and has to be decided on a case by cases basis. Do not exclusively rely on the overall test scores by simply selecting the trainees with the best scores. Do not overvalue or favor experience, as experienced interviewers are not necessarily better. If you select survey teams based on capacity, over time the best interviewers will be the most experienced ones. See Chapter RECRUITMENT for more information. The selection should be comprehensible and justifiable using the applied criteria. For example, if one trainee has been selected over another one who performed better in the tests, there should be objective reasons, such as higher scores in relevant key skills. You can decide to make the selection and criteria public to the trainees after the selection. 15.4.3 Selection process The following selection algorithm tends to work well. You might have to adapt it a little to meet the requirements of your survey: First, discarding trainees with unacceptable performance or skill levels. They should not be used as field workers unless they receive special training after the training to bring them to acceptable levels. Select all special roles among those who score highest in the required special skills. For example, select data quality control officers among those who have well understood the material, are very reliable and thorough. Select supervisors among those who have good leadership and organizational skills, but do not necessarily need to be top of the class in the test results if they are not involved in the data monitoring (see chapter FIELD). Select anthropometrics specialists who did best in anthropometrics training. Rank the remaining trainees based on test scores, skill ratings, and other criteria you might have and select from top. Often you might find that you have too few or too many trainees with adequate skill levels. Ideally you can adjust your field work model accordingly. If there are too few, only select those capable and use fewer or smaller teams for longer, or extend the training to bring them up to speed. If more trainees qualify than required, you can think of using larger or more teams for a shorter amount of time, but be aware that larger field teams are harder to monitor. Allocate selected staff into teams, double check they balance well and adjust steps 2 and 3 if not. For team allocation, keep in mind any special requirements you might have, such as gender balance, languages, etc. Try to balance the skill levels within teams by mixing top and bottom of class, so stronger trainees can support and act as a reference for weaker trainees. Make sure teams know who should learn from whom, and that they should support one another. Also be aware of personal traits and group dynamics when allocating into teams. Keep trainees with adequate skill levels who have not been selected as reserve to replace fieldworkers who drop out during field work. Inform them that they are on a waiting list and might be called should others leave. "],["introduction-1.html", "Introduction", " Introduction work in progress problems : one their own, far away, monitored, can be demotivating, little feedback, increasing idiosyncratic behavior leading to interviewer effects, measurement error "],["listing.html", "Chapter 16 Listing", " Chapter 16 Listing work in progress "],["monitoring.html", "Chapter 17 Monitoring", " Chapter 17 Monitoring work in progress "],["effective-feedback.html", "Chapter 18 Effective Feedback", " Chapter 18 Effective Feedback work in progress "],["avoiding-nonresponse.html", "Chapter 19 Avoiding nonresponse", " Chapter 19 Avoiding nonresponse strong revisiting protocols make it hard for interviewers, remove any incentives, have to give phone numbers confirm, etc follow up on non-repsonse "],["notes-2.html", "Chapter 20 Notes", " Chapter 20 Notes MICS has cluster control sheets first few days, trainers shoudl observe field work and provide field work. "],["introduction-2.html", "Introduction", " Introduction work in progress "],["data-editing.html", "Chapter 21 Data Editing", " Chapter 21 Data Editing work in progress ISCO codes, best done in batch by dedicated persons. "],["weighting.html", "Chapter 22 Weighting", " Chapter 22 Weighting "],["archiving.html", "Chapter 23 Archiving", " Chapter 23 Archiving work in progress "],["documentation.html", "Chapter 24 Documentation", " Chapter 24 Documentation work in progress - report on Standard errors - ideally "],["abbreviations-and-acronyms.html", "Abbreviations and Acronyms", " Abbreviations and Acronyms CAPI Computer Assissted Personal Interviewing CATI Computer Assissted Telephone Interviewing DK Dont Know LSMS Living Standard Measurement Survey NSO National Statistical Office PAPI Pen-and-Paper Personal Interviews ToT Training of Trainers TSE Total Survey Error TSQ Total Survey Quality "],["references.html", "References", " References "]]
