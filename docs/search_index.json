[["index.html", "The LSMS Guide to Survey Quality Preface About this book How to use this book About The LSMS Guideline series About the authors callout box examples", " The LSMS Guide to Survey Quality Andreas Kutka, Josefine Durazo, Kevin McGee, James Arthur Shaw Preface Hello there. Thanks for dropping by. However, you have come a bit early. This book is still work in progress and in a pre-publish state. PLEASE DO NOT SHARE. We look forward to seeing you back here in a few months, when everything is ready. Version 0.1. Last updated on 23 May 2022. About this book What is survey quality and how can it be achieved? This book gives practical advice experiences and practices that have worked in. based upon our experience in designing and implementing large scale socio-economic surveys in a variety of contexts. recognises the multi-dimensionality of survey quality and follows the total survey error paradigm total survey quality optimization the practical tips and techniques that  The book is targeted towards a range of survey practitioner roles. Survey designers, sponsors will find guidance on design decisions that are relevant for quality in chapters X and Y. They might also benefit from reading the introduction chapters to the following chapters. Survey implementer will find tips &amp; tricks and best practices in chapters Z and ZZ to maximize quality for all main steps of survey implementation. Individual sections provide details for individual key roles, including trainers, translators, etc. The book provides detailed steps and best practices for all survey phases. Survey practitioners are not encouraged to implement everything, as this would be beyond most surveys budget and time constraints, and as there might be alternative methods with equally good outcomes. Survey practitioners can use the detailed information for those components that form part of their quality assurance plan, or use the detail to reflect on or improve own practices. The book is a companion and go-to reference for survey practitioners aiming to improve the quality of the surveys they design and implement. The book follows the survey life cycle, and explaining the best practices and providing tips and tricks on how to achieve high quality surveys. The book is very comprehensive on each of the survey phases. Readers are not expected to implement all how-to-guidelines listed in the book. Instead, the book is meant to be a collection best practices to achive high quality in surveys. Survey practitioners are encouraged to evaluate which improvements can be implemented within the context and constraints of their respective survey. Follows the total survey quality framework, and touches base on the survey design, implementation and analysis. The book is written as companion for survey managers to help them take the right decisions, organize their team and works streams and how to obtain the right types of inputs in every phase of the survey to achieve high quality outputs. While providing a lot of detail in some parts, the book does not aim to be a comprehensive how-to-survey guide. Further guidelines and the input of experts are needed to achieve high quality surveys. The recommendations described in the book should not be interpreted as being the one and only way of implementing surveys. For many parts, there may be alternative approaches that deliver results of similar quality. However, the methodology presented in this book has proven to be effective in delivering quality surveys while being efficient, huzzle-free and without bad surprises. The authors welcome comments that point them towards other approaches. The book is divided into 6 parts. Part I provides a short introduction of key concepts of survey quality. It is the only theoretical part of the book. Parts II - IV follow the different stages of a survey life cycle from inception, design, preparation, training, fieldwork to post-field. Each part is comprised of several chapters. The first chapter to each part is an introduction that outlines the content of the remaining chapters and provides a cheatsheet version of our key recommendations, called TL;DR. References for each chapter are added to the bottom. In the Appendix, you can find the list of Abbreviations and Acronyms, as well as a complete list of all references used throughout the book. The book is a living document that is being expanded and revised on an ongoing basis. Whenever we become aware of new best practices, we will try to integrate them into the book. The last update was made on 23 May 2022. Book is often written with reference to a LSMS-style household surveys implemented in cAPI (Survey Solutions) as most surveys are now. However, almost everything in the book applies generally to most survey types or other software packages. How to use this book The book provides a lot of information and details that are probably difficult to absorb entirely and retain for the duration of the survey. For those who are managing one of their first surveys we recommend skim reading the document focusing on the introductions to each chapter and skipping the details. They summarize the key points and provide you with a general idea of what matters to achieve survey quality. Once the survey is underway, reading the relevant chapters prior to thinking about a phase will give you all the details you need at this point. Experienced survey managers who are curious about or aiming to improve certain aspects can read the relevant chapters only. The book is relatively extensive and contains a lot of detail. It is not written to be read in one go (we applaud your stamina if you do), but rather as go-to reference that can be visited during any stage of the survey to get relevant practical advice and learn about best practices. Chapters are written to be as independent as possible of each other. This causes some points to be repeated in another chapter. Links to other chapters are added to where relevant information is References are provided if they built upon other parts of the book or if more detail is provided elsewhere. Use the search function to quickly find all occurrences of a word or phrase. Open the search function by clicking on the search icon on top or by typing f on your keyboard. Use the up and down arrow keys to navigate between different search matches. You can edit the book! Yes, you read this correctly. If you come across anything that you think could be improved, from correcting typos to adding content to chapters, click on the edit icon on top. This will open the underlying file on github where you can make the modifications and submit them as a pull request (you will need a GitHub account). The book is written using Rmarkdown in bookdown. For most parts of the text, the syntax is following simple markup language, of which you can learn the basics in under 2 minutes. (For more experienced github &amp; Bookdown user, you can fork the depository and make commits). If accepted, your suggestions will be added with the next version. If you have any suggestions but prefer to not edit the book, please list an issue on GitHub. Thank you! You can share the book or any page you like using the sharing icons on the top right. Each chapter and subchapter have specfic links. Copy the link, either by right clicking on the blue # icon next to the heading and copying the link address, or by clicking on the (sub)chapter in the table of content and copying the url from the browser. About The LSMS Guideline series The LSMS Guidebook series offers information on best practices related to survey design and implementation. While the Guidebooks differ in scope, length, and style, they share a common objective: to provide statistical agencies, researchers, and practitioners with rigorous yet practical guidance on a range of issues related to designing and fielding high-quality household surveys. The series draws on experience accumulated from decades of LSMS survey implementation, the expertise of LSMS staff and other survey experts, and new research using LSMS data and methodological validation studies. About the authors Andreas Kutka . Josefine Durazo  Kevin McGee  James Arthur Shaw  callout box examples Did you know that  this book is work in progress and in a pre-publish state. something long and longer ahhds oh yeahaaahjkl Did you know that  this book is work in progress and in a pre-publish state. something long and longer ahhds oh yeahaaahjkl Warning  this book is work in progress and in a pre-publish state. something longer Warning  this book is work in progress and in a pre-publish state. something longer and longer ajetztabera test DO NOT SHARE! The book is work in progress and in a pre-publish state. "],["introduction-to-survey-quality.html", "Chapter 1 Introduction to Survey Quality", " Chapter 1 Introduction to Survey Quality Sample surveys are a crucial data source for official statistics, impact evaluations and social economic research. Survey data informs public policy and other important decisions. Quality in surveys is paramount for good policies and sound decisions and should be the aspiration of all survey practitioners. In Part I of this book we summarize key frameworks used by survey methodologists and national statistical offices (NSOs) to conceptualize and maximize quality in surveys. Skip this part if you are not interested in the theoretical background of survey quality. We will link back to the concepts throughout the remainder of the book, so you can read only the parts that are relevant. Quality can be defined as fitness for use. For something to be fit to use it must be free from deficiencies and respond to the users needs (Juran and Gryna 1980). In a survey context, freedom from deficiencies translates to the requirement of survey data to be accurate to allow inferences about the target population. In other words, a survey statistics must accurately describe the population for it to have any value. Responsiveness to users needs translates into aspects such as timeliness, usability and completeness that are important to the user of survey data. Even the most accurate data can be unusable if it is outdated, poorly documented or not rich enough for the intended analysis. Which user attributes matter, depend on survey. As we can already see, survey quality is a complex, multidimensional concept. In Chapter 2 we look at Total Survey Error (TSE). It is the dominant framework used by survey methodologists to maximize the accuracy of survey estimates within survey constraints. We describe different sources of survey error and provide real-world examples for each error source. We explain how error affects survey statistics and how the TSE can be used as a planning criterion to reduce overall error in survey estimates. In Chapter 3 we introduce the notion of Total Survey Quality (TSQ) that combines survey accuracy with user-specific dimensions of quality. The concept is the basis of multi-dimensional survey quality frameworks that are now used by many national statistical offices as a planning and evaluation tool. We present a general strategy how TSQ can be used to design surveys that maximizes survey quality. The chapters are based on the articles by Biemer (2010) and Groves and Lyberg (2010), and the books by Biemer and Lyberg (2003) and Weisberg (2005). They are good starting points for readers who are interested in learning more about the theory of survey quality. References "],["tse.html", "Chapter 2 Total Survey Error 2.1 Error sources 2.2 Survey-related Effects 2.3 The Impact of Error 2.4 Minimizing TSE", " Chapter 2 Total Survey Error In the survey context error refers to the deviation of a survey response from its underlying true value. Probably the best-known error type is sampling error that occurs when collecting information from a sample of the population rather than the full population. Early in the history of probability sample surveys, it was recognized that there is a variety of non-sampling error sources that can equally cause a survey statistic, often referred to as survey estimator, to deviate from its underlying true value in the population, referred to as population parameter. Over time, this knowledge evolved into the concept of Total Survey Error (TSE), which has become the dominant paradigm in the field of survey methodology Groves and Lyberg (2010). TSE acknowledges that error can arise from many different sources. For example, error might be introduced by researchers formulating questions wrongly, interviewers not probing sufficiently or respondents unwilling to respond. Error may occur at any point in the survey process, including the design, sampling, implementation, processing or analysis, and may be caused by any actor including researchers, interviewers or respondents. TSE denotes the aggregate of all error sources in an estimate and is thus the difference between a survey statistic and the population parameter. A good presentation of how TSE components link to the process of generating a survey statistic has been made by Groves et al. (2004), which is shown in Figure 2.1. Figure 2.1: TSE Components Linked to Steps in the Measurement and Representational Inference Process 2.1 Error sources The TSE framework decomposes survey error into different error sources. Survey methodologists have produced a number of similar classifications. They divide TSE into sampling error and non-sampling error, that in turn can be further decomposed. We follow the classification developed by Biemer and Lyberg (2003). We briefly introduce each error source and provide real-world examples that we have observed in our work with NSOs, international organisations and survey firms. 2.1.1 Sampling error Sampling error describes the error that occurs if a sample of the population is surveyed, instead of surveying the entire population, i.e. conducting a census. It is determined by the sampling scheme (simple, multi-stage, stratified, ), the sample size and the choice of estimator. If probability sampling is being used, the sample error is randomly distributed and can be mathematically computated. It is probably for that reason that sampling error is receiving a disproportionate high level of attention by researchers and survey designers. By definition, sampling error occurs in every sample survey. Sampling error becomes problematic if it is unnecessarily large due to faulty or inefficient sampling design or is minimized at too high expenses of other error sources. If non-probability sampling methods are being used, sampling error may also introduce bias. Examples include: The so-called Random walk is an oxymoron. Interviewers can easily tweak the sequence in which they list units, so that respondents are selected that meet certain characteristics, such as being available for an interview or being friendly. Random walk is a not a probability sampling method and introduces bias. Unfortunately, some surveys still use it for a fast and cheap second-stage sampling. A HIES used convenience sampling to select individuals within collective living quarters as respondents were unable to list all individuals living there. Interviewers selected whoever was available during their visit, effectively removing individuals from the sample that were unavailable during working hours, thus introducing bias. A probability sample method could have been used instead, for example, by listing all lockers or beds or other items associated with individuals, randomly selecting items from the list and putting effort into interviewing the associated individuals. A survey part of an impact evaluation study selected a varying number of households per community, depending on the community size. In the largest communities, up to 40 households were interviewed. While this sample design does not increase sampling error nor introduces bias, it is inefficient. Beyond 20, the marginal decrease of sampling error per additional household per community is negligible. The resources spent to interview them could have been used to decrease error from other sources instead, for example by extending training or increase monitoring. 2.1.2 Specification error Specification error (referred to as Validity in Figure 2.1 occurs when there is a misalignment between the concept a survey intends to measure and the concept that is actually being measured. As a consequence, the wrong parameter could be estimated by the survey and invalid inferences be made. It is often caused by insufficient definition of concepts, poor questionnaire design, lack of cognitive testing, absence of good manuals, or concepts not being preserved during questionnaire translation or CAPI scripting. Examples of specification error are: Surveys that aim to capture income from any kind of employment sometimes ask opening questions like In the past , did NAME work as an employee for a wage, salary, commission, or any payment in kind?. Designers and data users assume that this question captures informal jobs. Often however, respondents understand this question to ask about formal employment only and answer with No, even though they have worked as day laborers for a few days. The question does not capture the concept of employment as planned by designers or interpreted by analysts. Researchers or questionnaire designers developing questions without taking into consideration local circumstances or testing that underlying concepts are understood the same way in the population. For example, questions around saving behavior tend to be delicate and highly dependent on economic circumstances. Researchers assuming that goods bought on credit are included in a loan roster, while the questionnaire did not explicitly probe for them and trainers did not train interviewers to include them. Mistakes that happen when updating translations or instruments. last-minute change is made to the questionnaire during the training, in the local language that never finds its way into the original version in the design language which is being used as data reference. Data users will interpret the question differently to how it was being asked. Household to be translated as family in the local language, referring to a different set of people. Household refers to the economic unit and does not necessarily imply family ties. Many languages lack a word for household. An expression like those living with you would often be more appropriate. Sometimes, CAPI scripting can alter the nature and functionality of questions or modules to such a degree that it affects the way they are being administered and ultimately changes results. Researchers who do not observe the CAPI questionnaire being fielded remain unaware of such differences. For example, time use modules are often designed as a grid of activities and hours of the days into which lines are drawn for the primary activity done for a certain period. We have seen this module being scripted in CAPI as series of 48 questions asking the respondent for the main activity for each 30-minute interval. The responses produced are not at all comparable. DHS and Is it OK to beat your wife question NOTE: ARTHUR TO WRITE Different household definitions, different results? 2.1.3 Coverage error Coverage error or sometimes frame error occurs if the sampling frames that are used to select the survey sample include units that are not part of the target population, exclude units that are part of the population or duplicate them an unknown number of times. It also refers to errors in auxiliary variables associated with the frame units, or if they are missing. Frame error can cause parts of the population to be under- or over-covered, distorting the sample or impact evaluation design. Since building perfect frames is often practically not possible, most surveys suffer from frame error to some degree. More severe cases of frame error often stem from a lack of quality assurance of listing exercises, the use of unverified lists compiled by third parties, or the use of outdated lists. Examples are: During listing exercises listers sometimes focus on the population-dense village centers, as it is quick and easy to list a large number of households, but omit households that live further away or are difficult to reach. As a result, remote households are under-covered in the frame. Community lists are usually being used as frame for first-stage sampling in household surveys or to inform impact evaluation design. In some contexts, these lists can be of very poor quality. We have experienced field teams searching for non-existent communities or that community characteristics grossly mistmatch the auxiliary variables used for sampling or matching. A survey for an impact evaluation in Per√∫ used household lists for the second stage sampling in treatment areas that had been compiled by the project to be evaluated. Several local dynamics played into the creation of the lists. As a result, in many villages, several members of one household were listed as separate households on the list, friends and family of project officials from outside the village were included, or beneficiaries from the village excluded. 2.1.4 Nonresponse error Nonresponse error can occur on the unit level, when a sampled survey unit (e.g. household, individual, firm, etc.) cannot be interviewed for any reason, or on the item level, when parts of the questionnaire remain unanswered because the respondent did not answer some questions for any reason. Nonresponse error can severely affect the validity of survey data. Since the reasons for nonresponse are hardly ever randomly distributed, the actual respondents may no longer be representative of the population, causing the survey estimates to be biased. Nonresponse error has been plaguing survey methodologists in the global North, where nonresponse levels generally have been very high over the past decades Surveys in the global South have so far largely been spoiled with very high response rates, but nonresponse error is increasingly becoming an issue, especially in urban areas, where availability and willingness to participate in surveys is decreasing. While various approaches have been developed that aim to correct for nonresponse error, the best way to deal with it is to put solid measures in place to avoid it as much as possible. QUOTESNEEDED. Examples of nonresponse error are: Survey teams visiting communities during working/market hours and replacing unavailable households without making sufficient revisiting attempts on different days and hours. In a survey in urban South Africa, many interviewers were scared of dogs and replaced households if they owned a dog. Dog ownership was (initially) not observed and highly correlated with other household characteristics. The below average probability of Donald Trump supporters to respond to interview requests is thought to be the main reason that surveys systematically underestimated the support for Donald Trump ahead of the 2020 US presidential elections. In a COVID response phone survey, interviewers were unable to call those panel respondents who had not paid their phone bills and had their line (temporarily) cut off. Long questionnaires that cause respondent fatigue and lead to high rates of uncomplete interviews, affecting the last sections of the questionnaire. High Dont Know (DK) rate for income related question if interviewers do not probe and explain the question sufficiently. Respondents refusing to participate in the survey. This is happening increasingly often in some context, such as urban areas. Interviewers can be trained in best practices to convince respondents to participate. 2.1.5 Measurement error Measurement error often is one of the most damaging error sources in a survey. It occurs if the recorded value is an inaccurate measure of what was to be measured. They can be due to the interviewer, the respondent, the questionnaire, protocols or their interaction. They might be intentional or unintentional. Respondents might misinterpret a question, struggle to recall some information or deliberately give a wrong response. Interviewers might administer a question incorrectly, misunderstand questions or answer option, record typos, falsify responses or cause measurement error in many other ways. Poorly designed questionnaires, ambiguous questions, underdefined concepts often are big contributing factors to measurement error, as can be the interviewing mode. Some interviewers in an LSMS survey confused KG and Gram and selected the wrong answer option in the consumption unit. As a result, the food quantities they recorded were off by a factor of 1000. Respondents who purposefully under-report household members, crops, livestock or other items, knowing that each item they report will entail a long set of follow-up questions. Interviewers recording responses against the wrong questions or for wrong items due to confusing CAPI design that fails to provide them with a good overview of the questionnaire. A school survey conducted teacher interviews and recorded teacher attendance in classroom. Reason for absence was not recorded and no protocol was put in place to ensure both exercise were conducted during different hours. In a follow-up survey that corrected for both problems, 8% of the teachers were absent from classroom for being interviewed at the time. Agricultural surveys trying to capture very detailed information on the parcel-crop-season level can be too detailed for respondents to recall, or for interviewer and respondents to get lost in the conversation. 2.1.6 Processing error Processing error refers to any error occurring post-interview including error in data entry, editing, formatting and labeling, construction of indicators, calculation of survey weights, or tabulation of results. It is often caused by unclear interview rejection mechanisms, data editors who are not qualified enough, improper keeping of records or change logs. Examples of processing error we have observed are: Interviewers fixing issues in rejected interview files without recontacting the respondent or obtaining the correct answer. Wrong or non-systematic outlier detection, such as manually summarizing variables in statistical software and looking at the 5 highest and lowest values only. A NSO replaced the outlier values in any variable with its median value prior to data publication. Not correcting for changes to the instrument when appending the data from different versions. For example, f there has been a change to answer options or item lists, this can cause value labels to be assigned wrongly for parts of the sample. Wrongly label value codes, such as household assets. 2.2 Survey-related Effects Conducting a survey in the way it is being conducted, in its context, has itself effects on survey statistics. While these effects are often implicitly included in TSE and referred to as error, Weisberg (2005) uses the term survey-related effects and explicitly includes them into the framework. Examples of survey-related effects are: Interviewer effect: Interviewer characteristics such as mannerism, tone of voice, gender, ethnicity, etc. have shown to influence the type of response they elicit. This effect on survey statistics is not due to any mistakes or wrongdoing by the interviewers, and will exist for any survey using interviewers. Mode effect: Administering a question in face-to-face interviews can produce very different response patterns compared to asking the same question in telephone interviews or self-administered interviews. This, for example, can be due to respondents attitudes being different if they receive an interviewer at home (guest) to being called by phone (solicitor/marketer). In multi-mode surveys or with modality changing between rounds, mode effect can cause significant comparability issues. Questionnaire-related effects: Answers patterns can differ with the exact question phrasing. As long as there is no specification error, there is no question phrasing that is more correct than others. Similarly, the order in which questions are asked can change outcomes. Yet, they have to be aske in some order. The same is true for the phrasing and order of answer options. A well established example is the recency effect in phone surveys, where respondents select the last answer option because they remember it best. While some of the survey-related effects can be minimized, e.g. through randomization of question or answer option order or careful field team planning, they cannot ever be eliminated completely as they are an inevitable product of a survey itself. For example, since a survey requires interviews to be conducted somehow, mode effect will always exist in a survey. In some cases, it is possible to study and quantify effects in experimental design and to make ex-post adjustments. This, however is costly and requires provisions in the survey design. In practice, usually the best way to deal with survey-related effects is to be aware of them and keep them in mind during survey design and analysis. 2.3 The Impact of Error Sample surveys aggregate individual responses to obtain statistics for the sample, referred to as survey estimator, often with the aim to infer corresponding values in the population, referred to as population parameters. Survey error causes the survey estimate to deviate from the population parameter, diminishing the accuracy of the inferences derived from the survey data. In other words, with high levels of error a survey does not accurately describe the reality of the population. Error can affect survey estimators by increasing the variance of a variable or by introducing bias. For a survey estimator to be accurate it has to have a small bias and variance, which only happens if the TSE is low for the estimate. The ways in which error affects an estimate depend on whether the error is random or systematic, and whether it is uncorrelated or correlated. Error is considered random if it does not follow any pattern. For example, if respondents have difficulty recalling an item, some respondents might give higher values while others provide lower values. Across all observations, the error would have a mean of zero and would not affect the mean value of the variable. However, random error increases the variance of a variable, which in turn reduces the reliability of a survey estimator. The higher the variance, the higher the probability that the estimator would be different if the survey were to be repeated under the exact same conditions. The results would be less reliable. Increased variance furthermore reduces the magnitude of correlations with other variables and the statistical power in hypothesis tests. If error contains a systematic tendency we speak of systematic error or bias. As an example, if interviewers tend to under-report household members to shorten interview duration, the survey estimate of household size will underestimate the actual household size in the population. In statistical terms, the sample mean would be a negatively biased estimator of the population mean. Since systematic error directly affects the mean of a variable, it reduces the validity of the estimator, in other words the estimator is not accurately measuring what it is supposed to. If the systematic error is not constant, it may furthermore increase the variance of the variable, also reducing the reliability of the estimator. Figure 2.2 illustrates how variance and bias relate to the reliability and validity of a survey estimator. Imagine the midpoint of each image to represent the true population value and the black points to be the recorded responses. The points in A are scattered around the midpoint with no particular pattern, the error is random. We have high variance but no bias. The estimator is valid but not reliable. In B, the answers are offset in the same direction and by the same distance. The error is systematic, so there is bias, but variance is small. The estimator is reliable, but not valid. C shows systematic error that is not constant. We have high variance and bias. The estimator is neither valid nor reliable. In D, there is little error, we have low variance as answers are relatively close to the actual value, and there is no bias as the deviations show no pattern. The estimate is valid and reliable. Figure 2.2: Validity and realiability in survey estimators The magnitude of the effect of error depends on whether the error is uncorrelated or correlated. Error is uncorrelated if the error for different units (e.g. households) is unrelated. An example would be occasionally typos by interviewer when recording answers, if typos do not occur more frequently with certain types of households or some interviewers. Above discussion on the effects of random and systematic error was based on uncorrelated error. As we saw, the increased variance and bias can have serious effects on statistics. If the error for different units is related, we speak of correlated error. For example, interviews conducted by one interviewer who has misunderstood a question and administered it wrongly will contain the same error, while those of another interviewer may not. Correlated error has much more damaging effects on estimators as it multiplies the variance of a variable. Biemer and Lyberg (2003) show that only a moderate intra-interviewer correlation of \\(\\rho_{int} = 0.03\\), can result in an interviewer design effect \\(\\mathit{deff_{int}}\\) as large as 2.47. In other words, the intra-interviewer correlated error alone can cause the variance of a variable to be increased by 1.5 times. Error is typically also correlated for other survey roles, such as supervisors, data monitors or editors. One data monitor, for example, might thoroughly review interviews and provide useful feedback , reducing mistakes and measurement in their field teams while another data monitor only glancing over interviews will miss many of the issues and fail to reduce measurement error. 2.4 Minimizing TSE The TSE framework implies that the TSE affecting a survey estimate should be minimized in order to maximize its accuracy. Two points are important to note here. First, the total survey error needs to be minimized, that is the aggregate of all error sources. Errors from different sources do not occur in isolation, but are interdependent as they are all part of the overall survey process. Minimizing error from one source may negatively affect error from other sources. As an example, while increasing the sample size will reduce sampling error, it may not be a good overall strategy to increase the precision of a survey estimator. A larger sample can require more interviewers to be trained and monitored, increasing other non-sampling error types that can outweigh the gains from reduced sampling error. Another example can be complex parts of a questionnaire, such as a time-use module, that have been optimized for PAPI, but may cause significant error if implemented in CAPI without being carefully adapted to the functionality of the CAPI package being used. Understanding how decisions in one phase affect error in another requires a comprehensive view of the entire survey process. In order to effectively minimize TSE, different survey phases and components have to be integrated to operate as as a coherent whole. The second point to note is that TSE is to be minimized, meaning to be reduced as much as possible within survey constraints. All surveys face constraints such as the available budget, time and human resources or ethical considerations. Attempting to eliminate all error would exceed the constraints of any survey. Even with unlimited resources, some error sources could never be eliminated fully, such as respondents unwillingness to disclose some information causing item non-response. Instead, surveys should strive to avoid the most damaging errors and control others to the extent that they are mostly inconsequential and tolerable. How can survey practitioners prioritize which errors to address? A key challenge is that it is very hard to quantify survey error or its non-sampling components. This is only possible if the underlying true population value is known or if methodological experiments can be built into the survey design, which is an option for most surveys. In practice, minimizing TSE requires a detailed understanding of the main causes of survey error, their relative impact on accuracy, means to control them and the required effort. This knowledge is often anecdotal and based on experience in survey implementing institutions or individuals who have honed their error awareness, identification and correction over time. While there are some scientific publications on this topic, they tend to be theoretical or isolate and a address a single error source. Survey practitioners involved in the day-to-day implementation tend to not publish their practices. References "],["tsq.html", "Chapter 3 Total Survey Quality", " Chapter 3 Total Survey Quality As we have seen in Chapter 2, low levels of TSE are necessary for a survey estimator to be accurate. While accuracy is necessary for quality, it is not sufficient. Survey data must also respond to user needs in order to yield useful results. Data users often take data accuracy as given, assuming that it has been controlled by data producers. Instead they prioritize properties such as data being rich in detail, easily accessible and well documented. Data that is not sufficiently detailed, difficult to access or hard to interpret may be unfit for use and be perceived to have low quality by data users, even if being accurate. The concept of total survey quality (TSQ) considers the fitness of use of a survey estimate. It combines the accuracy of an estimate with non-statistical dimensions oriented towards the data user. The definitions of survey quality used by most NSOs in Europe, North America and Oceania, as well as that of international organisations such as Eurostat, IMF, and OECD explicitly acknowledge the multi-dimensional nature of survey quality. These definitions are referred to as survey quality frameworks. There is some (often minor) variation between organisations, but most frameworks include a subset of the quality dimensions summarized in Table 3.1 (Biemer 2010). Table 3.1: Common Dimensions of a Survey Quality Framework. Dimension Description Accuracy Total survey error is minimized Credibility Data are considered trustworthy by the survey community Comparability Demographic, spatial, and temporal comparisons are valid Usability/Interpretability Documentation is clear and metadata are well-managed Relevance Data satisfy users needs Accessibility Access to the data is user friendly Timeliness/Punctuality Data deliveries adhere to schedules Completeness Data are rich enough to satisfy the analysis objectives without undue burden on respondents Coherence Estimates from different sources can be reliably combined These frameworks are commonly used as the basis of survey quality reporting. Some of the dimensions are qualitative, making it difficult to generate a single metric to summarize the overall quality of a survey. Survey methodologists have not (yet) put forward a standard quality measure. Instead, evaluations tend to identify the strength and weaknesses of a survey by dimension and assess how well it achieved the goals of each dimension. Another very useful application of survey quality frameworks is as a design principle to maximize the TSQ in a survey. References "],["introduction-to-inception.html", "Introduction to Inception", " Introduction to Inception make a good plan, and adjust throughout the project. work in progress optimal survey design, reduces error make a plan to survey quality maximization, e.g minimize TSE while keeping the other dimensions acceptable, Biemer 2010 less can be more. A reduced questionnaire doing the key variable sof interest well can e better than trying to do many and ending up doing them worse. Fewer things to train, understand and monitor. Likewise, a smaller sample with a higher response rate might be more valuable. if there are too few resources available to do the survey well, try to either obtain more or question the feasibility of the survey. plan holistically. Efficiently allocating resources to maximize survey quality requires to look at the big picture and treat each activity as part of the whole that they are as each activity affects others. Some activities might be cut that have little or no effect on survey quality, to save resources for others where it matters. E.g. to afford a longer interviewer training one might reduce field team size and extend the duration of field work, if there are no time constraints or seasonality issues. good plan: to achieve timeliness. Overlap activities that can be done together. For example, with CAPi survey and a fixed data output format you can start working on the data documentation, cleaning, or even to some extend the analysis while the survey is still in the field. Do not overlap activities that depend on each other. For example, translation of instruments should only start once the instrument and CAPI coding have been finalised. Keepign track of change in overlapping activities is cumbersome, and leads in most cases to mistakes and survey erros. make gantt of key activities and draft timeline list of depending activities, e.g. only do training after instrument has been finished. Draw upon the experiences from other surveys in the same context, subject or modality. While every survey tends to be a different, the effect can often be generalised. In other words, what worked in one survey, often works in another survey with similar parameters. Make sure to have enough survey expereince in your team! "],["schedule.html", "Chapter 4 Schedule", " Chapter 4 Schedule Make sure your schedule keeps into consideration and works around the following forseeable events: Public holidays and vacations that might limit field workers or respondents availability or affect results, e.g. increased household expenses in the week of Christmas or Eid. Seasonality effects, that need to be catered for, such as such as harvest periods or that might affect your results. Weather patters such as rainy seasons that might limit field work operations. Allow margin to cater for unforseeable events that might affect your survey, such as abnormal weather events, unexpected field worker attrition, strikes or demonstrations "],["formulation.html", "Chapter 5 Formulation", " Chapter 5 Formulation work in progress "],["personel.html", "Chapter 6 Personel", " Chapter 6 Personel work in progress remove field team effects. As shown in Chapter LINK INTERVIEWER, interviewers, supervisors, data monitors or any other level can have huge effects on the TSE. As an example, interviewers of one supervisor who thoroughly back-checks interviews with respondents will be careful to not under report household members, parcels etc, while those of a more lenient supervisor may notice that they can get away with under reporting. that is centraize processes, so they can e monitored and streamlined, to remove supervisor, or data editor effects. Examples are: - instead of reviewing interview files in field y each supervisor, let them do only the basic completion checks, and do in depth review and feedback of the interviewed by a central team of editors, that regularly debrief, receie rotate staff, so that not always the same person enters data , reviews interviews, provides feedback to one interviewer only. review the work of each data monitor, editor, superviisor, for data entry, rotate data entry staff "],["contracting-considerations.html", "Chapter 7 Contracting considerations", " Chapter 7 Contracting considerations work in progress "],["in-field-sampling-for-design.html", "Chapter 8 in field sampling (For design)", " Chapter 8 in field sampling (For design) if in-field sampling, has to be traceable, and recreateble, e.g. write numebrs in book, use kish tables, send data to HQ if there is internet remove all possibility of interviewers to ahve an influence over selection. "],["introduction-to-preparation.html", "Introduction to preparation", " Introduction to preparation work in progress "],["fieldworker-recruitment.html", "Chapter 9 Fieldworker recruitment", " Chapter 9 Fieldworker recruitment general rule, smaller teams better, easier to keep track off, higher interviewer effect, so need to streamline and monitor. special roles: for separate tasks, split roles. for example, dedicated anthropomterics staff, less content for all trainees, can specialize and get good at it, more time to train and learn properly requires parallel training. check DHS, quite comprehensive. for supervisor, data monitors, if new team, recommend to identify suitable candidates during the training when abilities and personality become clearer. Beware of experienced trainees coming from other surveys. Standards might have been lower in their previous training and they may have developed some undesirable habits that you need to untrain. Also, they might have a stronger sense of I already know that and engage less. Most surveys are significantly different for even experienced interviewers to learn the concepts and protocols. MICS:  Recruitment/Selection of pool of field staff It is key to identify individuals to invite to the training. The practise of recruitment or selection of staff to invite for training differ from country to country and from survey to survey. In some situations, the implementing agency can decide to recruit a brand-new set of staff and in others, select from an existing pool of staff, either from a previous survey or actual staff on payroll. Here follows some overall recommendations to select the adequate mixture of participants and screen individuals for the training:  Gender: The protocol demands that the individual questionnaires are administered by an Interviewer of the same sex as the respondent. Depending on the sample of individual men (all households, half, one third, etc.), one or more male Interviewers will be required on each team. If the survey does not include an Individual Questionnaire for Men, then all the Interviewers must be women. However, since it is also recommended, based on experience, that all teams include members of both sexes, it is advised that, the Supervisor or the Measurer is male (if no male interviewers). It is important that all interviews can be observed by the Supervisor, but Supervisors should be advised to leave an individual interview during the most sensitive subjects, such as sexual behaviour and victimisation or other questions where an observer of the opposite sex may make the respondent uncomfortable.  Education: Normally, secondary education of trainees is a good target to bear in mind when recruiting. There are mixed experiences with university graduates: Recent graduates are often highly motivated and can be excellent Interviewers, but in other cases graduates have proven problematic by developing their own protocols in the field (being too smart). In any case, monitor performance carefully for all field staff.  Experience: Having worked in the field on other surveys is certainly helpful for Supervisors and can be equally so for other staff. However, please be careful, as other surveys may not pay as much attention to quality as the MICS or may have instituted protocols that are not recommended for MICS and are difficult to unlearn. A typical example is for Interviewers that have worked on market research that is not always conducted according to the standards of a national statistical office or MICS.  Language: All field staff must be completely fluent in the language(s) used for the training, which is typically also the language of the survey documents, such as the instructions and manuals. When deciding on the number of Interviewers fluent in other languages, it is important to have the fieldwork plan in mind, as well as a complete understanding of languages necessary in the different parts of the country. For example, if the fieldwork plan requires that just one team is fluent in a particular language, it is important that more trainees are invited than needed with this particular language skill. The rule of 10% is a rule of thumb that applies to total number of trainees, whereas it may be appropriate to invite 1-2 extra with a special language skill.  Appearance: Fieldwork is demanding, not just on physical fitness, but also, for some, on the ability to dress appropriately.  Attitude: A good candidate will show a respectful attitude and maturity and take interest in the work.  Diversity: With the demands for languages above, there is a good chance that recruiting happens across the country. However, for various reasons, there may be a natural bias towards candidates from the capital city or other major urban areas. It is important to ensure that the opportunity to apply is given across the country, perhaps through advertising that covers all regions. Even if the language requirements can be met in the capital, some candidates tend to appear so sophisticated to a rural population that a good rapport can never be established.  Avoid: It is risky to use staff currently employed for example in the health sector, both because of the issues mentioned under education and experience above and because a large part of the survey is measuring performance of the health sector and thus there is a potential conflict of interest. An objective set of requirements, based on the above, should always be developed and be transparent applied, so that the pressure the survey managers may feel to hire certain individuals can be eliminated. It is equally important that all applicants that meet the requirements are interviewed and tested as part of the selection process. This takes a lot of work and therefore planning far in advance is necessary. Very simple testing can be applied, i.e. if the candidate reads well, writes correct answers to simple questions, and can communicate in whatever languages are necessary and indicated by the applicant. There is a further need to at least check if candidates can operate simple functions in a tablet computer. Advanced computer literacy is not necessary, but a certain comfort with computers or smart phones is valuable. Additionally, measures must be able to see well (with glasses if used) as they will be reading out measurements that may be unclear in certain lighting. "],["fieldworker-terms.html", "Chapter 10 Fieldworker terms", " Chapter 10 Fieldworker terms make it fair: pay fair. Mix of daily and piece rate. Only use peice rate if control meachnisms in place to counter act effects, only use daily rate if sufficent control that do sufficnet work. due diligence: insurance for health and accidents give incentives to do well, e.g. bonus at the end if still there (to combat attrition) and based on performance put in deterrents to bad behaviour, make sure you can fire if gross mis conduct, not hire again write up short terms and conditions, including pay, DSA allowances, contract, duration, selection process, possibility of not getting selected, required tasks, duration fo field work and places of work. Make every candidate read the terms and conditions. If they agree, they should sign a copy. This is important that expectations of all trainees are correct at the beginning of the training, so trainee attricition is reduced, and to avoid collective walkouts by trainees. "],["introduction.html", "Introduction", " Introduction work in progress "],["instruments.html", "Chapter 11 Instruments 11.1 Questionnaire 11.2 CAPI/CATI 11.3 Translation", " Chapter 11 Instruments 11.1 Questionnaire discourage Dont Know responses. Dont make them very obvious by adding them to all questions. They are a tempting solution for iinterviewers. There are different types of Dont know, often they are related to respondents not makeing an effort to recall something. Intervieers should be encouraged to get respondents to answer. Only add to questions where you accept them. Instead use special codes that are displayed once at eginning of the questionanire and trained to intervieiwers. 11.2 CAPI/CATI 11.3 Translation Many questionnaires are designed in one language but fielded in one or more other languages. The way questionnaires are translated affects results, see e.g. Seo, Chung, and Shumway (2014). Interviewers speak the language and just translate on-the-go. is a big source of measurement error and interviewer effects, so make sure to translate your instruments! Using CAPI/CATI makes it easy to provide the questionnaire to interviewers in different languages. One can normally switch language within the questionnaire. If managed well, even translating to multiple languages is neither too much effort nor very costly, and a low-hanging fruit to increase survey quality. Translations done badly can quickly spiral out of control, causing tremendous amounts of work and potential mistakes. 11.3.1 What should be translated? Into which languages should I translate? Translate into all languages in which a significant proportion of the sample will be interviewed. The size of the proportion depends on the context and available resources, but can be as low as 5%. Only translate into languages that can be read fluently by interviewers. Some local languages are spoken only or have no spelling convention, making them very hard to read. Do I have to translate the whole questionnaire? Always translate question text, answer options and instructions to the respondents, as they are being read out to the respondent, or help the interviewers record their response. Translate interviewing-facing parts such as interviewer questions, instructions or warning messages if the field teams are more comfortable using a language over the design language. If they are fluent in the design language, there are only marginal benefits. Keep questionnaire structure such as names for screens, sections, rosters or the outcome variable in the design language or translate to a common language. Having a common reference makes training, management and feedback easier as everyone is literally on the same page (e.g. Parcel Listing). Add interviewer variables at the end of the instrument to record the main language in which the interview was conducted, if an interpreter was used and what the level of understanding was. 11.3.2 Who should translate? There are a few professional firms and translators that specialize in survey translations and cover a range of common languages. For many local languages these are unfortunately not an option. A sound translation of survey instruments requires translators to: be fluent in the design/source language be native speaker in the target language have survey field experience (to know survey expressions and ways to phrase questions) have good understanding of the subject (e.g. WASH, health, education) local contextual knowledge (to know how things are referred to locally) This set of skills and experience is rarely combined in one person. Translations from professional translators can sound too formal or bulky, while translations from field staff might miss the essence of a question or a construct. In the absence of professional survey translation services, the best options are often local consultants or experienced field workers who have worked with similar types of surveys before. Ideally, translation is done by a group that combines the above listed experiences and receives input where they lack, e.g. to correctly translate main toilet types or drinking water sources. It is useful for questionnaire designers to work through the instruments with the translators to make sure complex or nuanced parts and constructs are understood. A common communication channel (e.g. Email to all, WhatsApp group,) for translators to ask questions and for designers to send clarifications to all translators is important to ensure consistent translations into multiple target languages. 11.3.3 Some translation guidelines Follow below guidelines when translating survey questionnaires. Share this list with the translation team. Preserve meaning. Literal or close translations are often inadequate, especially if the target and source language and culture are distant. Find the best way to express the same meaning in the target language. Be precise. Use expressions to describe a construct if there is no word for it in the target language. E.g. use those who live with you instead of family if there is no word for household Dont omit words that provide any type of reference, such as on average,in total, main. Keep it simple. Use language and expressions that are easy to understand for all respondents in the sample. Make sure the same parts of questions are stressed. E.g. in English, the reference period is put at the beginning of a sentence to highlight it. In other languages this might be elsewhere. Adjust to different customs and culture. Direct translations might sound overly positive, negative, polite, impolite, etc. E.g. you might have to drop or add the word please. Be consistent within and across instruments. Use the same words or expressions to translate one concept in the source language. Check consistency by searching for key expressions in the translation sheet (e.g. Ctrl+f household) and checking they have been translated the same. Use established (good) surveys as references. They have already been tested and fielded in your context and often are a useful source of nomenclature. Preserve formatting, it carries meaning. Whatever is bold, underlined or UPPERCASE in the source language should be the same in the target languages. Preserve code. CAPI/CATI text sometimes contains dynamic parts that are written in code and must not be translated, e.g. piped text in Survey Solutions %MEMBER% or &lt;html&gt; tags. Use the custom Stata command sursol transcheck to make sure all Survey Solution formatting and code is correct in the target languages. Stay local. More widely spoken languages such as Spanish or Arabic can differ significantly between countries in how they call or express certain things. Use local translators, and review and adapt if using (parts of) questionnaires from other countries. Use the CAPI/CATI or a paper questionnaire in the source language on the side as guidance. Translations will depend on the context that is often not given when working in translation sheets. Give feedback if you think the source language needs updating and get clarifications if you are not sure how to interpret something. For more practical translation guidelines, see Chapter 12 of the ESS Translation Guidelines (European Social Survey 2018). 11.3.4 Translation verification Verify translations of your instrument. Despite being much-cited and persistently being added to ToRs, back-translations are not a great means to verify translations. Most big organizations such as the European Social Survey (2018) or the US Census Bureau (Pan and Puente 2005) that run multilingual surveys and put a lot of effort into comparative survey design use and recommend the TRAPD method (Translation, Review, Adjudication, Pretesting and Documentation). It involves reviewing and comparing (at least) two independent draft translations and agreeing on one translation that is cognitively pre-tested. Insights from the pre-test feed back to the translation, and the whole process is documented. Translation, review and adjudication are normally done together by a team that combines translators, survey field experts and content subject experts. A team-based approach deals better with mistakes, idiosyncratic interpretations and translator blind spots, compared to individual translators working on their own. Surveys with tight budgets and time constraints can use a reduced version of TRAPD to verify translation. Save yourself the back translation. Instead, use independent translator(s) to systematically review all rows in your translation sheet. The reviewer(s) either approve a translation or suggest an alternative translation in a new column. Translators and reviewers discuss all discrepancies and choose a final translation. Using a translation management system as described further below, translation and review processes can overlap and do not necessarily add much time to survey preparations. Make sure to desk test your translation in the CAPI/CATI before the pre-test. Often, the context, the way questions are displayed in the interface or the sequence require translations to be updated. As a last defense, verify the translation during the training. During the questionnaire review part, project the questionnaire and read out loud each question in the design/common language. Trainees follow on their devices in the local languages they are fluent in and are likely to use in the field. For each question stress the meaning of the question and check with the trainees if the translation is accurate. Assign one person to make updates to the translation sheet on the go based on feedback from the trainees. Keep note of contentious items and address them with selected trainees on the side or after the session in order to not delay the training. 11.3.5 When to translate Translate a questionnaire too early, and juggling the inevitable many updates can quickly overtake your work and introduce mistakes. Translate too late and the instruments are not verified, poorly translated or simply not ready in the local languages at the time of the pre-test, destroying one of its main purposes. In the preparation phase, carve out enough time prior to the pre-test for translation and verification. Try to complete the questionnaire before that, so that it only undergoes minimal change once translations have started. Translating a normal socio-economic questionnaire can take several days, and so does the verification process. If there is little time, you can use a staggered approach to provide enough time for translation. Translators can already translate and verify completed sections while the remaining sections are being finalized or put into CAPI/CATI. Use a translation management system as described below to stay on top of the updates and avoid mistakes. If you are using a paper version to develop the questionnaire, do not directly translate the paper version, but wait for the electronic questionnaire to be developed. When building the CAPI/CATI tool, questions often have to be modified to function in the interface, especially around roster and dynamic question text. If already translated, these changes have to be made in many versions, and often in languages the questionnaire designer does not speak, making updates difficult and error prone (e.g. when trying to find the word NAME in local languages to replace it with dynamic text). It is a lot easier, faster and less error prone to translate and manage updates once the tool has been converted to CAPI/CATI. If you require translated paper versions e.g. for training or archiving purposes, create them using your translated CAPI instrument. Often this can happen during or after field work, once the questionnaire does not change any longer, and during less work-intense parts of the project. 11.3.6 Stay on top of updates Lets face it. Most questionnaires will still undergo some change after the translation process has started. Done wrongly, late updates to the questionnaire can easily turn into a managing nightmare and cause significant undetected differences between the source and target languages. Done correctly, one can relatively easily stay on top of questionnaire modifications, even if there are a considerable amount of change and several target languages. Do not: Do not translate in the CAPI/CATI tool directly. They tend to hold only one additional text field for each language and have no means to let you manage or document the translation and verification process. Do not translate or store translations directly in translation sheets exported from CAPI/CATI, as they will be outdated with the next update to the questionnaires. You will inevitably end up trying to juggle different parts of the questionnaires on different translations spreadsheets for different languages. Do not work with offline versions of translation sheets sent back and forth by email. As above, this quickly becomes unmanageable. Instead: Work with an online spreadsheet such as Excel or Google sheets that holds all the translations and allows questionnaire designers and translators to simultaneously collaborate on the same live document. Define clear processes, how questionnaire designers mark rows that need to be translated, reviewed or updated, and how translators record the translation status of each row. Make sure designers and translators follow the processes to make the system work. Examples are: Add columns status (fixed set of answers, e.g. to translate, to review, to update, translated, reviewed, dont translate) and comments. Designers change the column status for rows they added or updated in the source language and provide details in comments if necessary. The first translator modifies the target language and sets status to translated, the reviewer to reviewed. At the end of the process, all rows should be in the final status reviewed. Designers add a special symbol that does not occur in the questionnaire or code (e.g. @) to the beginning of the target language column for rows that need to be translated, updated, or reviewed. Translators remove the symbol after updating the target text. Use conditional colour coding to highlight rows of certain status, and filtered views to generate custom views, e.g. a view for translators containing all rows they need to work on. Updating translations If you are working with Survey Solutions, use the translate_questionnaires STATA tool to keep your translation sheet in sync with the questionnaires and to produce the translation templates to be uploaded to the Survey Solutions Designer. The tool can be adapted relatively easily to work with other CAPI/CATI packages that export and import tabular translation sheets. For CAPI/CATI software such as ODK or Kobo that use spreadsheets to build the instruments, one can integrate the translation process into the same spreadsheet used to build the instrument by adding additional columns. Use an online spreadsheet to allow simultaneous access to designers and translators, and define processes as described above. 11.3.7 On-the-fly translation As described above, translate into as many languages as possible and try to avoid on-the-fly translations as much as possible. Sometimes however, on-the-fly translations are inevitable, e.g. if local languages cover small parts of the sample population or are non-scripted. If this is the case, make sure that interviewers have practiced translating the questionnaire in front of others prior to the field, and that there has been a group consensus on how to translate the questionnaire into the local languages. Both significantly improve the quality of the translations and can be implemented using group practice during the training. For group practices, group together trainees according to the local languages they are likely to use in the field. One trainee playing interviewer administers a question in the local language, those playing respondents follow on their tablets in the source language and make corrections or suggestions on how to improve the translation. Trainees should rotate roles such that every trainee has practiced translating the entire questionnaire to others speaking the same language. If you will have to work with local interpreters to translate interviews on-the-fly, make sure that interviewers explain well the key concepts such as household or parcel, and that they are understood by the translator. Make sure that you have budgeted for local interpreters if they are likely to be needed. References "],["sample.html", "Chapter 12 Sample", " Chapter 12 Sample work in progress "],["manual.html", "Chapter 13 Manual", " Chapter 13 Manual work in progress "],["pilot.html", "Chapter 14 Pilot", " Chapter 14 Pilot "],["introduction-to-training.html", "Introduction to Training TL;DR 14.1 notes", " Introduction to Training Fieldwork training is arguably one of the most important activities of any survey and has one of the biggest impacts on data quality. It serves several purposes: Build fieldworker capacity. In-depth content training, extensive practicing and capacity-based selection ensure that fieldworkers are capable of implementing the survey as designed. This is key to limit measurement and nonresponse error, as well as coverage and sampling error if the survey includes listing and in-field sampling. Standardize fieldworker behavior. Repeated practicing under supervision and continuous feedback streamlines how fieldworkers implement the survey. Reducing idiosyncratic behavior is important to mitigate the very damaging fieldworker effects. Scrutinize methodology. Particularly in surveys with limited pre-testing and piloting, fieldwork training often effectively is the most detailed review of the questionnaires, translations, CAPI and survey processes. Identifying and correcting mistakes is key to reduce specification error. Unfortunately, in many surveys fieldwork training is not effective in producing fieldworkers that are fully capable of implementing the survey, or is inefficient in being too demanding on trainers and survey budget and timeline. This part of the book gives recommendations on how to implement fieldwork training that effectively prepares field workers while not while not overburdening trainers or overstretching survey resources. It assumes fieldwork training to be in-person, as is custom for most socio-economic surveys with personal interviews. For remote training, check XYZ ??QUESTION Do we have any resources? Readers who are interested in a cheatsheet version can read the key recommendations for fieldwork training in the TL;DR below. Those looking for more detail will benefit from reading the respective chapters. Chapter 15 Planning and Preparation is targeted at survey designers, managers, fieldwork managers, trainers or any other role that is involved in the planning, budgeting and preparation of the fieldwork training. It provides recommendations on the training location, timing, size, the trainers, training content and schedules, and includes checklists of the key items to prepare prior to the training. Chapter 16 Conducting the Training gives useful recommendations on how an effective and efficient fieldwork training can be conducted. It provides extensive details and step-by-step guides for different training modules. It is mainly targeted towards trainers and is best during training preparation and the training itself. LSMS recommends to continuously evaluate trainees throughout the training and to select fieldworkers based on capacity at the end. Chapter 17 Assessment &amp; Selection is mainly targeted towards trainers and gives step-by-step guide on how to effectively implement written tests, systematically evaluate skills and select fieldworkers. TL;DR Follow below key recommendation to ensure a successful interviewer training. Train an excess of 40% of fieldworkers to allow for trainee attrition, competency-based fieldworker selection and to have a fieldworker reserve. This can be reduced to 20% if you are working with experienced teams with low attrition. Anything less does not allow for a meaningful selection process. Select based on skills. Continuously evaluate trainee performance and capacity in written tests, role plays, questions in class and observing them while practicing. Select fieldworkers at the end of training in a transparent process based on their capacity and skills. Select roles based on skills__ who wil be supervisor, data monitors, etc Train an excess of 20-40% of field workers and select them based on skills at the end of the training, even if working with experienced teams. Trainees take the training more seriously, pay attention, and learn and the overall skill level of field workers is higher. Dont cut on duration! Allow for at least one full week of training per 1.5 hours questionnaire length. Anything less is normally not enough for the complex content of a socio-economic questionnaire to be learned and practiced. Keep it small. Try not to train more than 50 persons at a time, so you get to know the strengths and weaknesses of the trainees. Adjust field work to take longer to allow a smaller field work team. Make it interactive. Avoid one-directional presentations that overwhelm trainees with too much information. Involve trainees and frequently change the training modality. Practice. Frequent practicing is crucial for trainees to fully internalize the content and be able to apply in the field. Practice on-site and in the field. Be prepared. Make sure questionnaires, manuals and tests are truly ready prior to the start of the training. Have the venue fully set up and all admin and logistics sorted. Focus. Only teach what interviewers need to know to do their work well. Spend more time on things that are crucial to the survey. Instead of giving a 2hr presentation on the analysis of the data, give a short introduction and practice with interviewers how to introduce the study in the field. Enough trainers. Delivering good training is more than a full time job. Many things need to happen in parallel to the training being delivered. More trainers are usually better, and best if stakeholders, designers and analysts are among them. Engage. As a survey designer, do not do your emails on the side table and leave it to the survey firm, fieldwork manager or trainers alone to deliver the training. Actively participate, check that the content is correctly understood, explain what matters, etc. You have put a lot of effort into the design. Make sure it is implemented correctly. Trainers need expertise. Sending a junior colleague without much experience to be in charge of the training is not a good idea. You need solid survey, context and subject experience among the trainers to deliver a solid training. Assess readiness for field. At the end of the training, a final field test that mimics field conditions. where field workers can be observed and feedback given without using the actual sample. If team is not ready, e.g. there are common outstanding issues, extend the training by a few days to revisit and strengthen problematic areas. Interviewing skills are essential. Not all trainees possess them. Especially new recruits need to learn and practice how to introduce the survey, respondents to participate, maintain the repsondent responsive, elicit responses for difficult questions. Only select capable interviewers. Only select interviewers that have demonstrated full understanding and capability of satisfactory and reliable completion of required tasks. If there are more than needed and you have flexibility, you can have slightly more teams operate over shorter time as long as overall size is kept at bay. If not enough trainees are available, extend training. 14.1 notes debriefing fter interviews retraining, when quality assurance mechanisms keep notes to write training report, for documentation and to learn for future rounds. design training prrotocols "],["planning-and-preparation.html", "Chapter 15 Planning and Preparation 15.1 Training size 15.2 Trainers 15.3 Content 15.4 Timing &amp; Duration 15.5 Schedule 15.6 Location 15.7 Preparation", " Chapter 15 Planning and Preparation The chapter presents recommendations for survey designers, survey managers and trainers on how to plan and prepare an effective and efficient fieldwork training. Fieldwork training should be planned well in advance and must be well prepared. They usually consist of a main interviewer training and separate training for supervisors, data monitors or other fieldworker roles. Survey designers, take note of the chapters duration, training size and trainees, as they are budget and timeline relevant. For survey managers and trainers all sub-chapters are relevant to plan and prepare solid fieldworker training. 15.1 Training size LSMS strongly recommends to train more trainees than are ultimately needed as fieldworkers, i.e. as interviewers, supervisors, data monitors or other roles. This allows for a competency-based selection of fieldworkers at the end of the training. Training more people than needed also compensates any attrition of trainees during the training and builds a reserve of already trained persons that can be brought up to speed relatively quickly during fieldwork, should fieldworkers need to be replaced. Train at least 40% more fieldworkers than are required if you are working with unknown trainees, a new implementing partner or in a new context. For example, train at least 56 people if you are aiming for 40 fieldworkers. Even if some drop out before the end of the training, this still allows you to not select some of the trainees that are not up to the task. You can reduce the excess to 20% if you are working with experienced teams who you have worked with before and if you are expecting few trainees to drop out during the training. Training less than 20% additional people is not recommended as it prohibits a meaningful fieldworker selection process. Often, some trainees will drop out during the training, particularly after day 1 or 2 once there is a clearer understanding of the required effort. Mitigate early trainee attrition by keeping on standby some of the applicants that have not been invited to the training, so they can quickly replace any trainees that have not come back. You might have to bring them up to speed in extra hours and ask them to independently learn already covered material to catch up. It is not advised to bring in additional trainees after day 3, as they will have fallen behind too far. In some surveys, there have been walk-outs of trainees, often towards the end of the training, in which trainees collectively refuse to continue the training or fieldwork unless their demands are met. Usually, there is no time or budget to repeat the training with other trainees, leaving survey management in a very weak bargaining position and putting the survey itself at risk. Avoid walk-outs by setting fair fieldworker terms and conditions, making them clear to trainees prior to the training, and asking them to agree by signing a copy of the terms and conditions. In many circumstances it is beneficial to select supervisors, data monitors and any other fieldwork roles from the pool of trainees based on their performance and capabilities observed during the training (see Assessment &amp; Selection). If any of the fieldwork roles are predetermined, the designated persons should participate in the fieldwork training without being counted towards the trainees excess (and demonstrate full understanding of the trained material). Keep the number of trainees as small as possible, ideally below 50 trainees, which corresponds to around 36 fieldworkers. Learning outcomes quickly decrease with increasing class size, as each trainee receives less supervision and personal feedback and as it is more difficult for trainers to keep track of trainees performance. If possible, adjust the fieldwork plan, so that fewer fieldworkers work over a longer period of time and fewer fieldworkers need to be trained. See Chapter Fieldworker Recruitment for other benefits of having a smaller field team. If it is unavoidable to train more than 50 trainees (e.g. surveys in large country or with large sample) or a centralized training is not possible (e.g. different languages or geographically dispersed sample), it is best or can be imperative to split fieldworker training into separate rooms or even locations. In those instances it is paramount to standardize the training between different rooms/locations as much as possible in order to minimize the training effect. This requires the training to be largely guided by standardized and extremely well-prepared training materials, and effective communication between the training rooms/locations, so that all trainees benefit from relevant feedback, comments, questions, etc. raised in one room/location. on top of that, there must be sufficient capable trainers who have received a comprehensive Training of Trainers (ToT). the ToT can be a dedicated exercise or integrated as an explicit outcome of the pilot training. Several scenarios are possible if a large number of fieldworkers need to be trained: Break-out sessions. This is the first best option to train class sizes between 50-100 trainees if they can be trained centrally. Split the class into groups of manageable size around 15-20 trainees and assign dedicated trainers to each group. Train all modules that are sensitive to class size in the groups, including the questionnaire reading and practicing. Trainers must keep notes of any additional clarifications they provided, questions they received or feedback they gave. Hold daily plenary sessions with the entire class in which feedback is given based on the notes from individual groups. Modules that are less size-sensitive, such as the opening module or fieldwork logistics can be trained in the full class. Parallel training. If joint plenary session become unfeasible due to larger class size, the group of trainees must be split and trained either in parallel or consecutive training. If possible, try to avoid this by adjusting the fieldwork plan. In parallel training, groups are independently trained at the same location. Each group should have a resident trainer, who is responsible for the training in this group. Other trainers can rotate between groups, for example those responsible for training a certain module or task, bringing some consistency to the training. Due to groups and trainers not overlapping, it is harder to harmonize training between groups. It is therefore important that trainers debrief during the breaks and at the end of the day. Consecutive training. Different groups of interviewers are trained one after the other. A core set of trainers repeats the same training for each group, providing consistency between the groups. Since each group should start fieldwork immediately after their training, this approach only works if a staggered field start is possible (e.g. if field work is sufficiently long). Additional trainers should attend the training for each group, so that they can supervise the field start of one group while the core trainers move on to train the next one. Parallel locations. It is not recommended to simultaneously train in multiple locations, as it is difficult to harmonize fieldwork training between different locations. If unavoidable, it is essential for trainers to keep good notes, frequently communicate and debrief with trainers from other locations. 15.2 Trainers The success of a fieldwork training hinges upon having enough and competent trainers. Conducting a fieldwork training is a substantial amount of work! Doing it well even more so. Prepare the training well to avoid significant reductions in quality or trainers getting overwhelmed. Make provisions for enough trainers so they can conduct the training well. For every room, there needs to be a minimum of two active trainers at any point in time, one to lead the training and another one to support the class or do any of the supporting activities that need to happen in parallel, such as updating manuals, compiling feedback, creating practicing scenarios, etc. Larger classes require more trainers as supervision and supporting activities increase. Ideally, there should be one active trainer for every 15-20 trainees. Trainers must have availability to actively engage in the training most of the time. Avoid the situation where trainers are physically present, but either are in meetings, stare at their laptops or do other tasks unrelated to the survey. Cater for more trainers if trainers have ongoing other responsibilities outside of the training (which is often the case). All trainers must have: A good understanding about the survey in general A detailed understanding of the questionnaires, definitions, protocols, manual, CAPI, etc. Ideally, they have been involved in the survey design process, participated in the pre-test or pilot and will be involved in managing and monitoring fieldwork. A general understanding of the survey subject, e.g. the regional agricultural systems for an agricultural survey. Good facilitation skills and the ability to lead and manage all components of the training. Collectively, the trainers of one room should also have: A good understanding of the indicators construction, the intended data use or analysis. Subject and context expertise, e.g. know in detail the local circumstances of child education, nutrition and health for a survey on early child development. Experience in implementing the survey type and implementing surveys in the context Experience in conducting fieldworker training Speak the trainees language. Do not train in a language the trainees are not fluent in. If some trainers (usually the survey designers or analysts) do not speak the training language, use an interpreter or ask other trainers to translate for them. Particularly in surveys with limited pre-testing and piloting, often mismatches between the survey design and field reality are uncovered during the training and adjustments need to be made quickly. For example, a locally practiced exchange labor system might not be picked up accurately in the questionnaire. Trainers need the above expertise and experience to be able to identify those issues and take decisions that are analytically correct, in-line with the survey design and make sense in the local context. There is a tendency to send junior analysts or PhD students without sufficient survey or context experience to conduct the interviewer training. This can work, as long as other experienced trainers are present, but quickly leads to quality issues if they are the main training lead. Similarly, handing over a sample and questionnaire to a survey firm and letting them implement the training without monitoring them closely is usually a bad idea. Firms commonly lack the analytical background, do not know about the details of the survey design, are differently motivated, and might not have the capacity to conduct the training to the required standard. If working with a new firm, it is recommended to send your own trainers to be present at the training, support if necessary and ensure the training meets the quality standards described here. Some surveys advocate for subject experts to facilitate some of the modules. Unfortunately, they often have their own agenda and priorities and come with a varying capacity to conduct effective fieldworker training, making it sometimes a waste of time (e.g. long, theoretical presentations on details that are largely irrelevant for interviewers). If external subject experts will lead parts of your training, make sure they fully understand the relevant parts of the questionnaire and what knowledge they need to transmit to trainees. Prior to the training, discuss the scope of their engagement and review their contributions, such as presentations. Subject experts can also be given a backstopping role to answer any questions on the subject and contribute to relevvant disucssions. 15.3 Content To equip trainees with all the knowledge and skills necessary to do a successful job as fieldworker, training must go beyond a simple discussion of the questionnaire. It should cover the following components, see details on how to implement them in the next chapter. CAPI use. Interviewers must understand and be able to use all functionality of the CAPI software they need to use in the field, including handling the CAPI questionnaire, creating and sending interview files, etc. it is best to keep theory to the limit and introduce functionality throughout the training as needed, so trainees can practice it immediately. Questionnaire content. All questionnaires must be covered in detail, including question text, answer types and options, how to administer questions and sections, the routing/skipping behavior, and the underlying definitions and concepts. Practicing. Repeated practicing, in groups, with respondents on-site and in the field is crucial for trainees to fully internalize the trained material, learn how to put it into practice and to get exposure to real respondents and scenarios during the training. Interviewing techniques. Trainees must learn and practice a range of skills that are critical to collect good data, such as how to introduce yourself, the survey and convince respondents to participate, how to probe, inquire, and get clarifications, how to control the interview. Special tasks. Many surveys include exercises such as capacity tests, anthropometrics, plot measurement or behavioral games that require strict adherence to protocols for comparable measurements. Trainees must practice until they implement them correctly and until behavior across the entire field team has been homogenized. Pre-interview skills. Interviewers need to learn the format in which they will receive their assigned units (e.g. households), any selection protocols they might have to follow, how to identify and locate units, when to revisit, replacement protocols, how to manage their workload, etc. Post-interview skills. It is important for interviewers to know correctly perform a range of activities after an interview, including checking for completion, addressing inconsistencies, leaving comments, submitting files, receiving feedback, respond to inquiries on submitted interviews, making corrections, asking questions, etc. Final field test. At the end of the training, a final field test should be conducted to identify and address any outstanding issues with questionnaires or field procedures, expose interviewers to real field conditions and determine if they are ready for field work. Supervisor training: Supervisors must learn how to manage field teams, manage the relationship to the communities and respondents, and the tasks they are responsible for in the survey, such as handling money, making logistical arrangements, checking completions, communication to survey management or data monitors, etc. Data monitor training: Data monitors need to learn all the tools and background information necessary to be effective in their roles, including how to review interviews, conduct re-interviews review audio, how to interpret summarized information, how to provide effective feedback to fieldworkers, how to compile feedback for survey management and archiving. 15.4 Timing &amp; Duration Fieldwork training should be held just prior to the start of fieldwork, with a maximum of 1-3 days between training and fieldwork to allow for administrative matters, a rest day travel to the field. An immediate fieldstart with strong supervision and feedback in the first few days is the best way to ensure the survey is being implemented as trained and that fieldworker behavior is streamlined. The longer the break, the higher is the risk of interviewers forgetting details, making mistakes and developing idiosyncratic behavior. If fieldwork is delayed by more than a few days, conduct a short refresher training just prior to the field start to revisit and practice key points of the trained material. For longer delays, a longer refresher training will be necessary. For breaks beyond a few weeks, often a complete retraining is the only solution, also because there tend to be (some) new trainees. Schedule enough time for fieldwork training. The optimal duration depends on a range of factors, including the nature of the questionnaire, tasks required by fieldworkers, level of trainees, etc. As a general rule of thumb, for a LSMS-style survey with a 2-2.5 hours questionnaire allow around 2.5 weeks (15 working days) for the main interviewer training, including practicing and final field tests. Training duration can vary to some degree with survey factors, but keep in mind that many components such as opening session, CAPI use, interviewing skills, final field tests need be trained independently, so even for a survey with a relatively short 45-minutes questionnaire, at least 1 week (6 working days) of interviewing training should be scheduled. To some readers, this might sound excessively long. Many survey firms, especially with a background in opinion surveys will suggest much shorter training duration. However, in socio economic surveys, due to the substantial amount of content that fieldworkers need to learn and be able to apply, it is usually impossible to train the field teams to an acceptable level in a shorter amount of time. Do not save on training duration, unless you are very confident that all trainees will have fully internalized all material and will be able to perform well as field workers! Be prepared for possible extensions. In many surveys, trainees are not ready to go to the field at the end of the training and the training needs to be extended for a few days. If extensions have not been budgeted for, this can cause quality issues elsewhere, e.g. the survey firm having to cut corners elsewhere to compensate for increased costs. Avoid this by setting out with a realistic training schedule and putting contingencies for potential extensions into the budget and timelines. Avoid fieldworker training that last longer than weeks (18 working days). Trainees ability to absorb and remember material and general morale decreases with training length. Also, since every day of fieldworker training is quite expensive, paying for days that are increasingly ineffective, longer trainings become increasingly inefficient and can reduce the funds available for other important survey activities. Conducting an efficient and yet effective training as described in Chapter 16 Conducting the training should allow most surveys to be trained well in less than 3 weeks. For surveys with a large number of questionnaires (e.g. school surveys) or special tasks such as anthropometrics or plot measurements that take a substantial amount of time to train (e.g. schedule 4-6 days for a solid anthropometrics training), often it is unrealistic for interviewers to learn all questionnaires and tasks sufficiently well in the time available. Instead, divide questionnaires and tasks between specialized fieldworkers roles, e.g. anthropometrics to be taken by dedicated measurers and interviews to be conducted by interviewers. This way, each fieldworker role can focus in their questionnaires/tasks and learn how to implement them well. Different fieldworker roles can be trained in parallel training sessions, which cuts overall training length and costs. Supervisor training should be held outside of main interviewer training hours, so that supervisor (candidates) can attend the interviewer training in full. If supervisors are known prior to training, supervisor training can be held just prior to the main training for a couple of days. If supervisors are selected during the training, it needs to be held towards the end of the training in the late afternoon/evenings or during the day in modules that are not relevant for supervisors (e.g. administrative days, revision of material they demonstrated to know, etc). The duration depends on the scope of the tasks, but generally 2-3 days are sufficient. Data monitor training is best held after the final field test, so its data can be used as training and practicing material. Data monitors should attend interviewer training and field testing to get a good understanding of the activities they will monitor later. Availability of trainers permitting, it can be held in parallel to the supervisor training, between training and field work, and during the first few days of field work. Generally, 2-3 days are sufficient for initial training and practicing and an additional 2-3 days of supervised work during the first few days of field work. 15.5 Schedule Prepare a realistic training schedule and track training progress on an on-going basis. Keep in mind the following general considerations: Stay flexible. Create a schedule that identifies all modules and allocates the required time. Use the schedule to track progress, ensure you are not falling behind and to plan your logistics, e.g. on what days you will need the training venue or transport. Expect timings to shift a little during the training, as some things tend to take longer or shorter than anticipated. Do not force through the original schedule, but adjust the training schedule to the updated requirements of the training. Stay flexible to allow sufficient time to address issues, retrain topics, or dive into more detail as needed. Allocate time generously. It is better to finish training early one day rather than keeping trainees until late, rushing content or trainees not being ready for the field at the end. If you are falling behind schedule during the training, extend the training as early as possible before logistical arrangements make this impossible. If training progresses faster than anticipated, use the time for additional practicing. Only shorten the training if you are certain that the trainees are fully ready for the field. Do not exceed 6-7 hours of training time per day and give frequent breaks, without the breaks taking over the day. For example, 7 hours per day can translate to a morning session from 8.30 AM - 12 PM, and an afternoon session from 1 PM - 5 PM, each with a flexible 15 minutes break. Close the main training early enough to allow for supervisor training, daily trainer debriefs and preparing for the following days. Frequently change training modality, to keep trainees engaged and able to absorb and solidify content. For example, regularily break long blocks of content training with practicing or interviewing skill role games. Schedule around low energy periods. Schedule theory and important content in the mornings when trainees tend to be freshest. Trainee attention tends to decrease in the afternoon, and is at its lowest after coming back form the field or directly after having a heavy lunch. During those periods, schedule hands-on and interactive modules that require greater engagement from trainees, such as practicing, interviewing techniques or written tests. Avoid training after coming back from the field. Train what is relevant. Only train content that interviewers need to know. Allocate time on topics proportionate to the direct importance for the interviewers. For example, spent less time on the general background of the survey and more on how to introduce it properly. Train when needed. Train material just before it is needed and can be practiced. Otherwise, trainees often forget and the content needs to be retrained. For example, do not train the household IDs, household identification, and replacement protocols at the beginning of the training, but just prior to the final field test, once trainees already are confident with the questionnaire, have the capacity to focus on the new content and are about to use it in the field for the first time. Train supervisors and data monitors outside of main training hours. They need to attend the main fieldworker training in full to understand field procedures. Typical time slots are prior to the main training (if roles are predetermined) or towards the end of the training once roles have been selected, during the late afternoon or on administrative, travel or rest days. Schedule time to debrief after field days. Avoid multiple testing or practicing days back to back. Trainees can get used to their idiosyncratic, potentially undesired behavior. After a field practice or field test, allow for enough time to debrief, provide feedback and retrain topics that have not gone well yet, so trainees can try to do better on the next day. Typically, one day of in-calss training is needed between field days. Rest days. For training lasting over one week make sure to schedule one or two rest days per week. Trainees and trainers need regular breaks for good learning outcomes. The training schedule in Table 15.1 illustrates how an effective and efficient fieldwork training can be organised for a survey with 2.5-3 hr questionnaire. The schedule splits the questionnaire into two parts (e.g. household and agricultural questionnaires). First, one part is covered and extensively practiced on-site and in the field, allowing trainees to solidify the material. The same is repeated for the second part of the questionnaire, before field practices and final field test at the end bring it all together. The schedule follows the above general principles and includes daily written tests and reviews. The number in brackets reflect the indicative hours allocated for each module. Hover over each day to find out more details on the reasoning behind the sample schedule. You can download the sample schedule as an Excel version from here. Table 15.1: An Illustrative Training Schedule. Day Main Parallel 1 Opening module (2) CAPI use (1) Questionnaire content (3) Written test (1) 2 Review written test (1) Questionnaire content (3) Group practicing (2) Written test (0.5) 3 Review written test (1) Questionnaire content (3) Front-of-class interview (2) Written test (0.5) 4 Review written test (1) Questionnaire content (2) Practice respondents on-site (2.5) Debrief (1) 5 Written test (0.5) Interviewing skills (2) Field practice (4) 6 Debrief field practice (2) Questionnaire content (2) Group practicing (2) Written test (0.5) 7 Rest day 8 Review written test (1) Questionnaire content (3) Front-of-class interview (2) Written test (0.5) 9 Review written test (1) Questionnaire content (3) Group practicing (2) Written test (0.5) 10 Review written test (1) Questionnaire content (2) Practice respondents on-site (3.5) 11 Debrief (1) Interviewing skills (1.5) Field practice (4) 12 Debrief field practice (2) Pre-interview tasks (2) Post-interview tasks (2) Fieldworker selection (1) 13 Final field test (all day) Supervisor Training (after, 2) 14 Rest day 15 Debrief final field test (3.5) Post-interview tasks (3) Supervisor Training (after, 2) 16 Final field test (all day) Supervisor Training (after, 2) 17 Debrief final field test (3) Administrative matters (4) Supervisor Training (4) Data Monitor Training (4) 18 Travel to the field Data Monitor Training (7) 19 Field start Data Monitor Training OLD FORMAT, WHICH ONE DO WE PREFER? (IGNORE CONTENT) Module Hrs Location Day 1 Opening module 2 on-site CAPI use 1 on-site Questionnaire content 3 on-site Test A 1 on-site Day 2 Review Test A 0.5 on-site Questionnaire content 2 on-site Group practicing 2 on-site Test B 0.5 on-site Day 3 Review Test B 1 on-site Questionnaire content 3 on-site Front-of-class interview 2 on-site Test C 0.5 on-site Day 4 Review Test C 1 on-site Questionnaire content 3 on-site Group practicing 2 on-site Test D 0.5 on-site Day 5 Review Test D 0.5 on-site Questionnaire content 3 on-site Practice respondents on-site 2 on-site Day 6 Debrief practice respondent on-site Interviewing skills: Introduction, asking for participation 3 Field practice (afternoon, 1 interview per team of 2) Day 7 Rest day Day 8 Debrief field practice Questionnaire content Group practicing Test E Day 9 Review Test E Questionnaire content Front-of-class interview Test F 0.5 on-site Day 10 NA NA 1 on-site Day 11 NA 1 on-site Field practice (afternoon) 4 on-site Day 12 Debrief field practice 2 on-site Interviewing skills: Inquiring, difficult situations Pre-interview tasks: tracking sheets, identifying households, replacement protocols, Post-interview tasks: interview review, synchronization, completeness check Fieldworker selection Day 13 Final field test day 1 7 field Afterwards: Supervisor Training 2 on-site Day 14 Rest day Day 15 Debrief final field test day 1 3 on-site Post-interview tasks: respond to rejected interviews 3 on-site Afterwards: Supervisor Training 2 on-site Day 16 Final field test day 2 7 field Afterwards: Supervisor Training 2 on-site Day 17 Debrief final field test day 2 3 on-site Administrative matters 4 on-site Supervisor training 4 on-site 15.6 Location Fieldwork training is best held in a location that: Has sufficient logistically capacity: Is easy to reach for trainers and trainees, has a suitable training venue, sufficient accommodation and food options, good network coverage, print shops, etc. facilitates field testing: Is close to communities that are similar to but outside of the sample. If the survey is limited to a specific region or area, it is best to conduct the fieldwork training there. facilitates field start: Allows field teams to start field work shortly after the training and trainers to supervise the first few days of field work. Find a training venue with a big hall large enough to comfortably fit the entire team (or multiple halls if parallel sessions are held) and space to breakout into smaller teams for group exercises, such as a garden, hallways, the cantine or additional rooms. It is crucial that there is good internet connectivity at the venue, either through on-site Wifi, or general network connectivity. The venue should be either able to accommodate trainees and cater for food, or be easily accessible and close to restaurants or eateries. It should be quiet and free from distractions. Good venues tend to be hotels or conference centers with garden or open space. Avoid universities, schools, towns halls or other places that are busy and become easily unavailable. Book the venue early. Check for availability, in case the training needs to be extended on short notice. 15.7 Preparation Avoid wasting precious training time by preparing everything that can be prepared prior to fieldwork training. Prior to the training, the following should be in place: Inform trainees of survey details and terms. To minimize attrition of trainees, make sure that trainees have been informed of and agreed to the overall survey parameters and contract terms, including the selection process, their remuneration package, insurance coverage, areas to be visited, the length of field work, the transport and lodging arrangements during field work, expected effort, etc. Often, trainees drop out from the training if their expectations were wrong. Have trainees sign a copy of the details prior to the trianing to indicate agreement. Contractual matters. All contracts, payment arrangements, insurance policies and other administrative matters should be prepared such that they do not delay training or fieldwork and that trainees are paid without delays and are covered by insurance. Avoid interrupting the training with contractual matters, e.g. by pulling out individual trainees to sign contracts. Instead, schedule dedicated admin slots prior to the training, after class or between training and fieldwork. For fieldworker selection at the end of the training, make sure that all contractual matters including insurance and payments can be addressed in the short time window between training and field start. Logistical/administrative support. Get a dedicated person to support with all the logistical and administrative work that has to be done during the training, e.g. setting up the venue, organizing transport, contacting local leaders, printing, purchasing equipment, signing fieldworker contracts, etc. If these activities have to be done by the trainers, they become a substantial distraction and can significantly affect training quality and pace. Projector with a BIG screen. The projected content must be visible and legible to all trainees. You should be able to connect a PC and tablet/phone to the projector. Ideally, it should be possible to quickly link trainees devices to the projector, to facilitate exercises and practicing. Test the set-up prior to the start of the training. Stable internet connection for all devices. Trainees have to sync their devices several times per day. Usually, the WiFi connection of the training venue is not sufficient to connect all devices at the same time. Often, additional mobile network routers or WIFI hot-spots are required. One option is to already provide the internet connection that is planned for field work, e.g. the dongles, mobile phone routers or sim cards. Food and refreshments for the breaks. If food is provided by the venue, make sure it is ready in time for the break. If trainees eat out for lunch, set reasonable break times that allow trainees to finish lunch and to be back in time. Incentivize punctuality, e.g. those coming back late from the break have to sing a song. One tablet/phone per trainee. It is crucial for all trainees to learn how to navigate the questionnaire CAPI and use the device. If you do not have enough devices (since there are more trainees than interviewers) borrow devices or ask some trainees to use their private devices. Devices must meet the minimum specifications for the CAPI software (see here for Survey Solutions) and have the correct CAPI version installed. Consider using an app blocker to allow using specific apps only. Add shortcut/icons on the main screen for CAPI, the softcopy of the manual, the calculator or any other relevant app. Fully set-up CAPI software. For Survey Solutions this includes the installation of Survey Solutions on a server, interviewer accounts for each trainee, questionnaires imported to the server and assignments made. For training, often open assignments with infinite quantity (-1) are useful. Create several assignments with different scenarios if the questionnaire includes substantial pre-loading (e.g. household rosters in a panel survey). Electricity. Avoid delays due to power issues. Provide (lots of) extension cords to charge tablets during the training and/or in the evenings. Provide power banks for field days if the device charge does not last a full day. Samples of documents. If fieldworkers need to interpret, verify or extract information from any documents or items such as immunization cards, school timetables, or anti-malaria medications, get enough copies/samples to allow for hands-on practicing. If there is variation, provide different examples, e.g. different antimalarial medication brands. Measurement equipment. If fieldworkers need to use any special measurement equipment, such as height/length boards, GPS units, water testing kits, student testing booklets, etc., provide enough units, so that trainees can practice in a meaningful way. Field procedure documents. Sufficient copies of any documents other than instrument and manuals that fieldworkers need to use in the field, such as introduction letters, household tracking lists, maps, cluster completion sheets, referral forms, etc. Finalized questionnaires. The questionnaires should be reviewed, extensively pre-tested or piloted, translated and the translations verified. The CAPI version of the questionnaire should be extensively desk-tested and error free. Mistakes can cause significant issues or delays during the training. Provide printed copies of the paper questionnaire if needed. Fieldworker manuals. All manuals for interviewer, supervisor or other fieldworker roles have to be made available to trainees from the beginning of the training, so trainees can use them and get used to using them as a reference. Modifications and additions should be made to the manuals during the training wherever it was unclear or incorrect. Either hand out a printed version of the manual at the beginning and a revised version at the end, or provide regularly updated soft copies in pdf to the trainees which they can look at on their devices (which tends to work very well). Organize in-field practicing. Select practice areas/communities well in advance. They must not be included in the sample. Make all necessary administrative arrangements such as getting permissions, introduction to village officials, etc. If necessary, arrange respondents for field practices, so trainees can spend more time practicing than looking for respondents to participate (dont do this in areas/communities for the final field test). If you require certain types of respondents (e.g. children for athropomoterics) check with institutions where they are concentrated, e.g. child clinics, schools, etc. Organize reliable transport for the trainees to travel to the field practice. Make sure the transport waits for the team, not the other way round. Organize on-site practicing. Arrange respondents for on-site practicing to visit the training venue. Arrange for their transport and remuneration and dont forget to also provide food/refreshment for them. Facilitation material. Microphones and speakers including batteries and fully tested, flip charts with non-permanent markers in multiple colors for drawings, name tags/stickers for all participants, printed attendance lists and feedback forms, for trainees notebooks, pens in 2 colors to allow marking and clipboards, etc. It tends to e useful to have a printer or printing facilities in the venue to allow ad-hoc printing of e.g. exercises, tests, etc. Field work logistics and material. All necessary logistics need to be in place to allow fieldwork to commence immediately after training, including fieldworker contracts to be signed after selection, insurance policies, payment arrangements, transport, accommodation, etc. Similarly, have all fieldwork equipment ready, such as bag packs, rain coats, ID cards, battery packs, t-shirts, respondent incentives, etc. "],["conducting-the-training.html", "Chapter 16 Conducting the Training 16.1 General considerations 16.2 Opening module 16.3 CAPI use 16.4 Questionnaire content 16.5 Practicing 16.6 Interviewing techniques 16.7 Special tasks 16.8 Pre-interview tasks 16.9 Post-interview tasks 16.10 Final field test (often called pilot) 16.11 Supervisor Training 16.12 Data Monitor Training", " Chapter 16 Conducting the Training This chapter is targeting trainers and provides details on how to deliver an effective and efficient interviewer training that facilitates learning without overstretching survey resources. The chapter assumes in-person training as is the case for most surveys conducted with in person interviews. The first sub-chapter covers general considerations for the training. The remainder of the chapters is organised by training module. Each sub-chapter provides details on the rational, optimal timing and recommendations for the implementation of the respective module. See the training schedule in Chapter 15 on how individual modules relate and are best combined. Prior to the start of fieldworker training, make sure everything is fully set-up. 16.1 General considerations Throughout the fieldworker training, consider the following points: Essentials only. Only train fieldworkers on things that are relevant for them and that their respective role is required to do. For example, there is no need for interviewers to learn details about the survey analysis or Survey Solutions supervisor CAPI role. Confirm with designers/analysts. The interpretation of a question or concept must be aligned all the way from how it has been designed to how it is fielded. This is not always the case. Keep note of anything you are uncertain about or anything that is under-defined and run it by designers/analysts on a daily basis. Ask them to be on standby so you receive timely responses. Get back to the trainees once you have received the response. Eliminate language barriers. Do not train in a language the trainees are not fluent in. If some trainers (usually the survey designers or analysts) do not speak the training language, use an interpreter or ask other trainers (e.g. fieldwork manager) to translate or reiterate their points in the training language. Support trainees. Especially during the first few days, trainees often get lost in the CAPI or questionnaire and are unable to follow the sessions. Trainers who are not actively leading a session should walk around the class, check that trainees are in the right place and support those falling behind. Probe for understanding. It is wrong to assume things have been understood if you just ask Any questions? and there are none. There hardly ever are any. Probe using common examples or scenarios. Ask variations of your test questions. For example, think of working profiles/jobs that are typical in the surveyed population and probe trainees if these are wage employed or self-employed. Encourage questions. From the beginning, make it clear to trainees that they should ask if they have ANY question, that it is better to ask than to remain with a doubt, and that there are no bad questions. Take questions seriously and address them. If trainees prefer not to ask in front of the class, they can raise their hand and be visited by a trainer. Involve trainees. Asking frequent questions. Let trainees read aloud questions and manuals. Ask them to correct one another. Dont always pick the most engaged trainees but try to involve everyone. Select a trainee using a random name picker, so it can be anybodys turn to answer your questions at any moment. They make it fun if displayed in front of class. Alternatively, select the person that is closest to where a paper ball lands (that you or anyone throws over their back). Select one trainee for a couple of questions to speed up things. Get to know trainees. Ask trainees to wear name tags during the first few days. Learn their names as quick ass possible. This makes the training more personal and helps trainers to better connect trainees in-class performance with written tests or test interviews. Collect and react to feedback. At the end of each day, use feedback forms to collect anonymous feedback from trainees on how clear different sessions or trainers were, topics they did not understand or questions they have. Review feedback forms prior to the next day and adjust your training schedule or methods if needed. ??KEVIN: provide feedback form example. Break up and mix groups. If you start noticing groups of trainees sitting together who are not paying attention or are in the bottom end of performance, distribute them across the classroom. Ask low performers to sit next to high performers, so they can help when falling behind. Make it fun! Maintain a general positive and fun attitude. Play fun and exciting group games. The last person entering the venue late and unexcused can sing a song in front of the class (Youtube is full of Karaoke videos). Hold a end of training party. Use video &amp; audio. It can be a good way of demonstrating certain things like interviewing tequniques or special tasks to trainees, so they can see/hear what they should and should not do and learn from examples. Use existing material if good, or collect video/audio material during pre-test, pilots and field practices to demonstrate good and bad practices to the class. Stress importance. Make it repeatedly clear to the trainees that it is important that they do a good job, that the survey result and ultimately policy and decisions depend on it. Give them a sense of purpose. Like any other human being, interviewers perform better at work if they care about what they do. Trainees are responsible for their learning. Make clear to trainees that it is up to them to ensure they have understood the covered material. they can read the manual, practice outside the class, learn from peers, ask questions, etc. to make sure they understand everything. There is a transparent and objective selection process at the end and they must pass it. Set and enforce rules. Trainees must attend all days and sign attendance every day. Ask trainees to be on time and respect training hours yourself by not running late or running over time. No side conversations, no (excessive) use of cell phones. Enforce in a fun but serious way, e.g. anyone breaking rules can donate to the cookie char, has to hand in their cell phone, sing a song, etc. Zero tolerance for cheating. Make it clear from the beginning that there will be serious consequences for anybody being caught intentionally making up data or purposefully breaking protocols (ideally being immediately dismissed). Also make clear that fieldworkers have nothing to fear as long as they follow what has been taught during training and check with supervisors or fieldwork managers if there are any situations they are unsure about. Daily trainer meeting. Trainers should meet daily at the end of the day to compare notes, evaluate progress and trainee performance, review tests and class feedback and plan the following day(s). Include supervisors if they are pre-determined. 16.2 Opening module The opening module serves to introduce trainees to the survey and training. In many surveys, opening modules are unnecessarily long and involve too many people. Keep it as short as possible (1-2 hrs are enough) to be able to cover some content on the morning of the first day. Cover: Opening speeches. Try to keep speeches by officials or leaders as short as politically possible and avoid, if possible, the opening of the class to depend on officials, as they might be late. Introduction to survey/organization. Give a brief introduction of the survey, its objectives, the organization, the project/study, etc. Do not go into too much detail here and limit it to what is essential for fieldworkers to know. If needed, provide more details in the manual and refer trainees to it. Introduction of trainers and trainees. All trainers and core survey team should briefly introduce themselves and their roles. Avoid introduction rounds for trainees if the class size is large, they are dull an ineffective. Play a fun ice breaker instead. Ask all trainees and trainers to write their name on a name tag and carry it during the training. Training plan. Run through the rough training schedule, daily working hours, explain need for punctuality, that there will be frequent test and evaluation of skills, that trainees are responsible for their learning, that all questions are encouraged, that there is zero tolerance for cheating, that trainees have to follow what is lerant here (and forget other surveys they have done if done differently). Survey details and terms. Run through the overall survey parameters and contract terms, including the selection process, their remuneration package, insurance coverage, areas to be visited, the length of field work, the transport and lodging arrangements during field work, expected effort, etc. Importantly, this should be a refresher, NOT be the first time trainees learn details about their pay, other terms or the field work. Make sure all trainees have understood and signed a copy of the general terms prior to the training. Administrative/contractual matters. If you need to address administrative or contractual matters with trainees, such as contracts signing, registering for insurance, signing out tablets, etc, try to do this as mcuh as possibel before the training start (e.g. from 8-10 AM), or in a dedicated slot during which trainers do not need to be present. At the beginning, introduce the survey, sketch out the training and field work plan and set the training rules. Keep it short so you can focus on content during this day. 16.3 CAPI use This chapter assumes Survey Solutions to be used, but the same principles apply to other CAPI packages. CAPI, as any other software, is best learned by using it. Covering all functionality in an long theoretical session tends to be very ineffective as any functionality that is not immediately put to practice by trainees is forgotten and needs to be retrained later in the training. It is more productive and a better use of time to initially only cover the asic functionality that is necessary to proceed with the training, and to gradually introduce additional functionality as required throughout the training. For example, trainees dont need to learn how rejected interviews work until they learn about post-interview activities in the latter part of the training. Limit the functionality you train to what is relevant from the perspective of an interviewer and what will be used in the actual survey. For example, trainees do not need to learn how the Supervisor interface works, what exactly happens at HQ, or how assignments work or what a Yes/No question type is if there is none in the questionnaire. The fewer slides you have in the CAPI training and the more demo and practice you do the better. Train functionality by demonstrating it on the big screen and asking trainees to follow on their own device. Make sure the class can follow and wait for those falling behind to catch up. Those lost should check with their neighbors or raise their hand, so one of the trainers can come to assist. For example, show on the big screen how to login to the Interviewer app. Wait until all trainees are logged in on their tablets. Show on the big screen where to find an assignment card, explain what it is and show how to create a file. Wait for everyone to have opened an interview file on their tablet, and so on. Clarify any questions or doubts trainees might have on the way. You can train the questionnaire directly in CAPI, without training it on paper first, if you are using Survey Solutions or another CAPI software that provides a good overview over the questionnaire, makes routing behavior clearly understandable and allows for free navigation between different parts of the questionnaire. Note, this is usually not the case for software showing one question per screen. Training the questionnaire directly in CAPI has the advantages that trainees only need to learn the questionnaire in the format they will ultimately use in the field (e.g. no need to learn how skips work on paper). Trainees will also practice CAPI along the way, making them much earlier confident in using the CAPI version of the questionnaire and the CAPI software in general. New trainees do not notice any difference, and experienced trainees who are used to paper questionnaires usually stop missing them after a day or two. Train the basic CAPI functionality just before trainees need to use it for the first time. usually, around 1 hour is enough. If you are training the questionnaire on paper, train the CAPI basics just before trainees practice for the first time on the tablets, usually on the first or second day. If you are training the questionnaire directly on CAPI, introduce the CAPI basics just before starting with the questionnaire content, usually on the morning of the first day. For Survey Solutions, cover the following topics in the initial introduction: A general overview, including how the Interviewer App and HQ server interact, what assignments and interview files are, what synchronizing does. Keep this part short. How to log-in to the Interviewer app. Make sure tablets and accounts have been set-up prior to the training. How to create an interview file from their assignment. A general overview of the Interviewer App and how to navigate through the questionnaire. The outline of a question, the question number, the question text (what to read and what not), the instructions, the answer options (different by type). Question and section enablement. It is sufficient to work on the first section of the questionnaire. The complete screen, marking an interview as complete. The started and completed screen on the dashboard, reopening an interview file, discarding an interview file. Having completed the above steps, trainees should be able to follow in the questionnaire during the questionnaire content modules. The remaining CAPI functionality is best trained throughout the training when trainees need to use the functionality for the first time. During the questionnaire content modules, train the different question types, roster forms, static texts, sections and sub-sections as they come up, e.g. list questions in the household member module, yes/no questions in assets, etc. Trainees need to recognize and understand the difference between single select, multi-select, numeric, list and date questions, and understand the rosters. Prior to practicing or the first written tests (if done in Survey Solutions), train how to synchronize to receive assignments, create new interview files, review interviews on the complete screen, marking them as complete and synchronize to send a file. Conducting the test on Survey Solutions tends to be useful, as trainees need to correclty synchronize to get the assignment and submit their test (send the completed interview file). Prior to group or field practicing, train how to add comments to questions and at synchronization, so that trainees can leave questions and comments directly in the interview files when practicing. After field practicing or the final field test, train how to receive rejected interviews, respond to comments, and resubmit rejected interviews, so trainees can practice post-interview activities using interview files they have created themselves. 16.4 Questionnaire content A single, joint reading of a questionnaires is not sufficient for trainees to learn and understand the various concepts, definitions, and scenarios that are necessary to correctly administer a socio-economic questionnaire. Some implementing agencies, such as survey firms used to implementing opinion surveys, sometimes do not recognize the required effort and vastly underallocate training time for questionnaire content. Trainees must understand in detail the entire questionnaire(s), including the underlying definitions and concepts. Usually, this makes for a substantial amount of material to be covered, which if not trained well, can be quite dull and an ineffective use of time. Consider below points to make the questionnaire content modules interesting and effective: Avoid long content sessions. Covering the content of an entire questionnaire usually takes many training days. Avoid training content for more than 2 hours at a time. Long content sessions tend to be an information overload for trainees and make it impossible for them to internalize the content. Instead, spread the content training over more days and let trainees practice the newly trained material at least twice per day, e.g. after every long questionnaire section (or a few short ones). Less instructional, more interactive. Avoid one-directional training methods where trainers give long presentations or read questions without really interacting with trainees. This tends to be a very ineffective way of getting across questionnaire content. Instead, involve the trainees as much as possible. For each question (or a few at a time), select one trainee to read aloud the question and any instructions, answer options or corresponding manual entry. Ask the trainee to explain in their own words what they have understood and how the question is to be administered. Make sure the rest of the class can hear them. Correct them if needed. Ask the rest of the class for input. Check if the trainee has understood by probing with an example. Give feedback on tone of voice and way of asking the question. Ask the trainee to read out the question again if needed. Explain the big picture. At the beginning of each section (or part of the questionnaire if you have small sections), provide a brief general overview to the class, including the overall structure and routing behavior, things to be aware off, key concepts and definitions, the targeted respondent(s), etc. It is better to do this section by section (rather than once at the beginning of the training), so trainees can immediately see and later practice what has been trained, and can thus remember it more easily. Go into detail. Many questions have to be covered in more depth for interviewer to be able to administer them correctly. This can include explanations on the underlying concepts and definitions, how to categorize responses into the given answer options, common scenarios that require further probing, what error messages mean, what conditions open the question, how the question relates to other questions, how to sense check responses, etc. Make it tangible. Definitions for survey concepts such as household, wage employed, parcel, etc. can be very theoretical and hard for trainees to translate into the real world. When training anything theoretical or abstract, always try to illustrate it with examples and scenarios that are common in the surveyed population to make it easier to understand for trainees. For example, to explain what is wage-employed and self-employed, list jobs or work profiles that are common and show which category they fall in. Use the manual. The interviewert manual should hold all conventions on how to correctly administer the questionnaire. It must be always up-to-date to be correct. Also, trainees must develop the habit of consulting the manual to get additional information or to clarify doubts, or it will never be used. Ask trainees to keep the manual open on paper or as soft copy on their device. After reading each question, ask trainees to check if there is a corresponding manual entry and to read it. One of the trainers should update the manual directly during the training if the entry is unclear or insufficient. Ask for input from the class if needed and confirm with them if the updated entry is clear. Distribute updated soft copies of the manual to the team every morning/evening. Use your time effectively. There will always be a few rare cases in the population that the questionnaire does not cover well. Trainees love pointing those out, e.g. But what if .? If these scenarios are frequent they might require you to update the questionnaire or manual. If not, tell trainees to leave comments in the questionnaire if the situation arises in the field, and move on with the training, so that you have enough time to focus on the important parts. If longer discussions arise in class, clarify and stop the discussion. If you are not sure how to answer, get back to them the next day after having read up or consulted with the design team. Multi-language questionnaires. If the questionnaire has been translated into other languages, display the questionnaire in the training language on the big screen and ask each trainee to follow the questionnaire on their tablet in the local language they are most likely to use. For each question, ask it to be read in the training and local languages. This serves to verify the translation and to familiarize trainees with the questionnaire in the local language. CAPI. If training the questionnaires directly in CAPI (recommended), show and explain any new relevant CAPI functionality as you work your way through the questionnaire. On the big screen, demonstrate how the questionnaire should be filled in, how different question types function (e.g. text questions with patterns, search in combo box, etc.), highlight instructions or other parts that are not meant to be read out, what responses trigger what enablement or validation conditions, the overall organisation of a section and how to navigate it, etc. Ask trainees to follow on their tabets. Demonstrate. Show how questions or (parts of) sections should be administered by demonstrating the part of the interview in front of class, either as trainer yourself, or by asking experienced trainees to act as respondent and interviewer. This is particularly useful for complex modules or parts that require inquiring, e.g. probing for the age of household members, a child nutrition module, parcel sketch, etc. Trainees can follow and record the responses in their tablet. Give immediate feedback, highlight any mistakes or parts done well, and provide additional information if necessary. Update the questionnaire &amp; CAPI. Depending on the level of pre-testing, piloting and subsequent change, the questionnaire, CAPI and translations may still contain issues that are uncovered during the questionnaire content training. Ideally, trainers not actively leading the training can make immediate corrections, as it tends to be less time consuming and allows corrected versions to be used sooner during the training. Be consistent and follow updating protocols when making changes, e.g. update the live version of the paper questionnaire, the CAPI and the translation sheet. Do NOT simply update the questionnaire on CAPI or local language only, as this will lead to inconsistencies, mistakes and survey error. Ask the class for inputs if useful, e.g. how to phrase a question, or what answer categories exists. If there are many corrections, ask a dedicated person in order to not slow down the training. 16.5 Practicing Trainees should frequently practice learnt material. Daily group practice and front-of-class interviews towards the beginning of the fieldworker training are effective ways for trainees to learn and solidify newly covered parts of the questionnaires and to break-up the monotony of questionnaire content training. Practicing with respondents on-site or in the field towards the latter half of the training exposes trainees to real-world scenarios and respondents, and prepares them for fieldwork. Each practicing modality serves a different purpose. It is not recommended to use them interchangeably, but to combine them throughout the course of the training. 16.5.1 Group practice Towards the beginning of the training, while new questionnaire content is being learnt, frequent practice is important for trainees to familiarize themselves with the questionnaire, learn how to adminster the questions and practice the use of CAPI. Good ways of practicing during this phase are group practice or front-of-class interviews. Hold practice sessions on every day that new theoretical content has been introduced, so trainees can put it to practice and internalize it better. Do not wait to practice until the questionnaire has been covered fully or until the end of the day if a lot of material has been covered. Instead hold 2-4 practice sessions per day at natural breaks (e.g. between sections) to practice the material/sections covered just prior. The change of training modality helps to maintain attention levels. Immediately repeating material helps trainees to understand and internalize it and helps to uncover remaining doubts. Practicing more frequently becomes unfeasible as each practice requires time to setup. Group exercise can be effective for small groups of 2-8 trainees. Change group composition between exercises. When allocating trainees to groups, mix trainees and break up clusters that formed in the class, so that low/high performers and those paying more/less attention are distributed. Try to be efficient and not to waste time when creating groups. For pairs, ask trainees to practice with their peers in the row in front/behind. For groups, work out the number of groups n needed, and repeatedly count loud from 1 to n, pointing at a trainee for each number. Each number corresponds to a group, and each number/group can meet in a different point of the training venue, e.g. All number 4s, meet at the blackboard. All number 3, at the door. Ask groups to practice the questionnaire sections that have been covered since the last practicing session. If there is time, groups can also practice all sections learned to date. They should not work through parts of the questionnaire that have not been covered yet. Give scenarios for trainees to ensure they practice scenarios that are likely to occur in the local context and cover all parts of the questionnaire. For example, when practicing the labor section, ask to complete it for an employed individual, self-employed, supporting family worker, etc. ??NOTE, add example scenario box? Within each group, one trainee should read and ask the questions while another one answers them. After a few questions/sections the roles should be rotated, so each trainee had the opportunity to act as an interviewer or respondent. All trainees in a group should follow on their own tablet and record the answers, which at the end should be identical for all trainees within a group in theory. Ask groups to submit their interviews after the feedback session to motivate them to get it correct. Feedback after practicing sessions tends to be thin. Asking the class if there were any doubts, questions or comments often results in general silence. In order to receive better feedback, ask groups to actively come up with feedback points, note them on a sheet of paper including the trainees names, and to submit those to the trainers at the end of the exercise. Feedback points can be any question or doubt they have about questionnaire, definitions, scenarios, etc or any mistakes they have found. You can set a minimum number of feedback points, e.g. 3 during the beginning of the training when things are not clear yet, and 1 towards the end of the training. Trainers should observe groups that have been allocated to them, correct mistakes and help struggling trainees on the spot, but also take note of the issues they observed for the feedback session. They should consider any issues, including the way questions were read or answered, the intonation, misunderstandings, questionnaire navigation, CAPI use, etc. While observing, trainers should also rate and record trainees skill levels and give individual feedback where necessary. Debrief with the entire class immediately after the exercise. Ask groups for their written feedback and discuss and answer their questions in class involving input from other teams/trainees. Ask trainers to provide the feedback from their observations, explaining what was done wrongly and also how to do it correctly. Ask trainees who have been observed to do something well to demonstrate to the class (e.g. how to explain a concept/question), so that all can learn from good examples. 16.5.2 Front-of-class interview Asking trainees to conduct (parts of) an interview in front of the class is a good way to demonstrate how interviews should (not) be conducted and evaluate trainee skills. It also shows if the class is able to spot any mistakes done and allows trainers to provide feedback on interviewing to the entire class. It is a good means of practicing after a good part of the questionnaire has been covered, and before practicing with respondents on-site. This exercise tends to take much longer than a normal interview, so either practice only parts of the questionnaire, or schedule several hours. Project one tablet to the big screen on which the responses for the interview will be recorded. It is best to not chnage the tablet during the exercise. For front-of-class interviews, select one trainee to come to the front and act as interviewer in a mini-role play. Select one at random that has not yet been to the front. It usually makes sense to rotate trainees after a few questions (e.g. 5-10), so more trainees get the opportunity to practice in the front and make the exercise less monotonous. Ask the next trainee to already prepare and wait in front in order, so the rotation of trainees does not take up too much time. Select someone to act as respondent throughout the whole exercise. This can be another trainee, or preferably one of the trainers or a real respondent. Using a trainer has the benefit that they can direct the interview with their answer and probe if some scenarios are understood. Using real respondents exposes the trainees to scenarios and respondents that are closer to what they expect to find in the field. For real respondents, hire a person that is similar to the surveyed population to visit the training venue for the afternoon/morning. Ask the interviewing trainee to administer the questions as if they were in a real interview and to record the responses on the tablet that is projected onto the big screen. The respondent should give normal responses with some difficult scenarios. Ask the rest of the class to follow the interview and raise their hand if they have a question or comment about the interview in front. As a trainer, pause the interview and give immediate feedback if the interviewer could have done better. Consider any aspects of the interview process, including tone of voice, reading speed, way of inquiring, etc. Ask the class for input, e.g. if they spotted the mistake or have any ideas how to do it better. Ask the trainee to repeat the question taking the input on-board. Also highlight things they have done well, so it is clear to the class that this is the way the questionnaire should be administered. All trainees should record all answers in their own tablet. At the end, their interview files should look the same. Asking the class to submit their interview files after the exercise is a good way to encourage them to follow closely and make sure they record everything. 16.5.3 On-site respondents Practicing with real respondents on-site is a great way to give trainees more and early exposure to real-world scenarios, while keeping the logistical effort and required training time low. Trainees learn how to apply the theory they learnt and how to handle real respondents. Another advantage of this modality is that trainers can observe all or most trainees conducting interviews, which is nearly impossible during field practicing when they are scattered across many homes. The best timing for an on-site practice is after the questionnaire has been covered completely and be practiced in group practicing, and prior to the first field test. Hire a group of respondents to visit the training venue or some other convenient location to be interviewed there. Depending on the number of trainees, some 5-10 respondents are usually enough. They should be similar to respondents in the surveyed population, e.g. cultivate land and keep livestock in agricultural surveys, or have jobs or small businesses in labor force surveys. Make sure to compensate respondents adequately, pay for the transport and refreshments or to invite them to the training lunch should it be provided. For the practicing, group trainees into as many groups as you have respondents, ideally not more then 4-6 interviewers per respondent. Each group should conduct a full interview with the respondent. Each trainee should follow on their own device and record answers, and they should take turns in asking the questions, e.g. a few questions or a section at a time. Groups should support each other if individuals fall behind, have questions or make mistakes. Ask groups to submit their interviews after the feedback session to motivate them to get it correct. If there is sufficient time, groups can switch respondents and conduct a second or third interview. Collect feedback, observe and debrief as described above for group practice. 16.5.4 Field practice The purpose of field practice is for trainees to get first experience in implementing the questionnaire with real respondents. There is no need to completely mimic field conditions as one does in dress rehearsals. Rather, aim to maximize the number of interviews each trainee can conduct and the number of respondents and scenarios they are exposed to. Prepare field practice well, observe trainees and collect good feedback to make it a success. Field practice often is implemented inefficiently with a lot of time being wasted on logistics that could have been prepared in advance: Teams waiting for transport, community introduction, looking for respondents, etc. It is not uncommon that during a whole day of field practicing, trainees only get to practice for 1 hour or could not practice at all as they did not find any respondent. Prepare well, so trainees can practice interviewing, not waiting. Field practice sessions should be held after the questionnaire has been trained and practiced in class, and before the final dress rehearsal at the end of the training. Often it also makes sense to conduct it prior to sessions on household tracking, receiving rejecting interviews, logistics etc, so that content is broken down more and the field test can focus on the understanding and implementation of the questionnaire only. If the questionnaire is really long, e.g. a LSMS-ISA style questionnaire with long household and agricultural questionnaires, it can be beneficial to conduct more frequent field practices for different parts of the questionnaire. try to be efficient when conducting field practices. If relatively short or only parts of a questionnaire are tested, often half-day field practices are be sufficient to give some exposure to trainees and identify sufficient issues to be addressed prior to the next field practice. Since it is usually difficult to conduct class room training after the team has come back from the field, it is often helpful to conduct the field practice in the afternoon, so you can use the morning. Visit communities that are not too far from the training venue and large enough to have enough respondents for all trainees. Communities should have similar characteristics to the surveyed communities, but be outside of the sample. Contact or visit the communities the day prior to the field practice, do the community introduction and arrange for respondents at the time of your expected arrival, e.g. with the help of a community leader. Compensating respondents for their participation often helps with their availability and willingness to participate and is totally fine during field practicing. Again, the aim is for trainees to practice the questionnaire as much as possible with respondents. Try to minimize other logistical issues that could cause delays. Make sure all tablets are fully charged, synchronized and have battery packs available if needed. Arrange for food and drinks for trainees in the field or ask them to bring it. Make sure your transport arrives on time, knows the way and does not need to stop for petrol. Try to arrive early in communities, as respondents tend to become unavailable during lunch hours. Group trainees in pairs of two, trying to take skill levels into account and pairing high performers with low performers. Allocate each pair with a respondent. Agree on a meeting place for trainees to return to and a time by when they need to be back latest. Ideally, there is time for more than one interview per pair. Similar to the on-site group practices, trainees should take turns interviewing the respondent and both record all answers and submit the interview file at the end of the day. Ask them to write down any issues they experience while they conduct the interview. Working in pairs, one of them can take notes while the other one is interviewing. For each interview, each pair should list at least one question or doubt they had, and something about their interview or respondent that was noteworthy and is useful to share with the class. Try to observe as many of the pairs as possible, help and correct where necessary, mark their skills (LINK), and note down any issues you have observed as general feedback for the class. Focus on trainees that are borderline of being selected or that have not been marked yet. Collect as much feedback as possible from trainees before you leave the community. It becomes much harder later and the quality of the feedback decreases rapidly. As pairs of trainees arrive back at the meeting place, ask for their written notes, debrief quickly with each pair, clarify any issues they had, and record general feedback for the class. Often trainers assume that the questionnaire is working well and that interviewers are able to administer it correctly because they did not report any issues during feedback sessions. This assumption is wrong. Many issues cannot be identified by interviewers, are forgotten or not reported. Those reported are biased towards trainees concerns, e.g. they are quick to report something that caused extra effort from their side. Trainers must observe interviews being conducted and compile their own feedback. To receive meaningful feedback from trainees, trainees must note down issues or questions as they experience them and must be debriefed shortly after conducting interviews. After each field test, hold a general debrief session with the entire class in which you respond to the written feedback from the team, answer any questions, and provide feedback from your observations. This might involve short retraining of concepts or questions that have been ill understood, practicing or role plays. Clarify all issues prior to the next field test. It is normal for quite a few things to go wrong in the first field test and to improve quickly. Sometimes, you can debrief to some extent directly in the field in an empty classroom, a community center, the shadow of a big tree, etc. This way interviews are still fresh in everyones head and there is no need to go back to the classroom after the transport back from the field. If you are required to debrief the same day in the classroom, make sure to leave the field in time and to allow for a long break, as everyone tends to be really tired after field practicing, especially after the transport back. Design your training schedule such that field practicing days are followed by classroom days, so you can have a detailed debrief session in the classroom with the big screen and other all required facilities. 16.6 Interviewing techniques In addition to understanding the questionnaire content, interviewers must master a range of interviewing techniques that are crucial to collect good quality data. Some examples are: Introduction and interview request: Being able to convince respondents to participate in the survey is key to keep unit non-response error low, particularly in urban areas or other settings with high refusal rates. Interview continuation: Being able to keep respondents engaged in lengthy interviews and stop them from prematurely ending the interview is important to minimize item non-response and measurement error. Inquiring: Respondents often do not understand or misinterpret a question, give incomplete answers, respond I dont know or do not want to disclose some information. Interviewers need to be able to get respondents to answer all questions in a neutral and non-leading manner. Controlling interviews: Respondents sometimes digress, give lengthy explanations, or talk about other things. Interviewers need to be able to lead the conversation in a pleasant and courteous manner to maintain the respondents cooperation. Unfortunately, these interviewing techniques are rarely trained. Instead it is assumed that interviewers automatically know them or will learn them over the course of the survey. In reality, while some interviewers are experienced or naturally good at it, many others struggle or resort to undesirable practices such as making wrong promises to obtain consent or leading responses. This can exacerbate interviewer effects and lead to high levels of measurement error and unit and item non-response. Teaching trainees that are not good a task the skills of those that are mitigates these effects, e.g. see Groves and McGonagle (2001). Teaching interviewing techniques goes beyond theory. All features of communication are relevant for the interaction between interviewers and respondents, including the choice of words, tone of voice, volume, as well as attitude, facial expressions or gestures. To learn a technique, trainees have to see and hear what they should and shouldnt do, and learn from good examples. To be able to put techniques into use, trainees need to repeatedly practice and receive feedback until they are able to (re)act correctly and quickly.. Which interviewing techniques are best trained depend on the survey. Interviewers can face different challenges in the field. For example, refusals may be rare in a household survey, but very frequent in a enterprise survey, requiring dedicated training on how to address them. In addition, best practices often are sensitive to the culture and context, e.g. how to inquire if respondents are hesitant to disclose their income. Therefore, the training agenda on interviewing techniques usually needs to be tailored to each survey. 16.6.1 Identify challenges and responses To establish which interviewing techniques are most relevant for the survey and should be trained, first compile a list of the key challenges that interviewers are expected to face in the field and need to be prepared for. Focus on those that are most common and have the biggest effect on survey error. Consider any response, concern or action by the respondent or any other situation that can negatively affect data quality if interviewers do not apply the right technique. Examples are: respondents refusing an interview stating reasons related to time, etc. other members trying to listen to sections to be administered in privacy respondents unable to quantify consumption amounts in units long and damaging breaks if tablet needs to be restarted error messages flagging gross mismatch in harvest quantities, land size and sale prices Record different ways in which a challenge may manifest itself or how the interviewer can recognize them in the field, using colloquial terms or used by respondents where it applies. As an example, respondents may say I am really busy, I need to leave or I need to cook/wash/ when refusing and stating reasons related to time. For each challenge, identify good interviewer responses. These can include verbal responses actions or procedures. Often, there is no single response to a challenge that works equally well with all respondents and in all circumstances. Good interviewers often have a repertoire of responses and tailor them to observable features of the respondent or the context. For example, if a respondent refused based on time and appears to be rushed or pre-occupied about something (e.g. fighting children) the best response tends to be to ask for a different moment, e.g. by saying I see that now is difficult. When would be a good moment for me to come back?. However, if the respondent refuses because they have to do a chore but seems generally collaborative, it is often good to offer to do the interview while they do their activity. For each challenge, also record bad practices or responses that interviewers should avoid, if there are any. As an example, if respondents do not know the quantity consumed of an item, a bad probing practice would be say Was it maybe 1 kg?, as this is leading the respondent to answer 1 KG. Allocate challenges and responses into separate modules by topic, importance and anticipated time required to train. It is usually good to have at least one module on Introduction and interview request, one on Inquiring and controlling interview and one on Challenges observed during field testing. You might need additional modules on key questions or sections, such as Estimating land size or Conducting consumption module. Ideally, challenges and responses should be compiled prior to the training to facilitate planning and make training sessions more productive. Use a combination of any of the following sources to identify challenges and responses: experience from previous surveys with similar context, populations and subject focus groups with experienced interviewers observations and feedback from pre-test or pilots If you are unable to prepare prior to the training, use challenges and responses observed during field practice sessions or hold short focus groups with the class at the beginning of each module. See HERE be a core set of challenges and responses that are common to many surveys. &amp;&amp; NOTE: WOULD IT MAKE SENSE TO COMPILE ONE? 16.6.2 Train &amp; Practice Schedule modules on interviewing techniques just prior to field practice and final field tests so trainees can apply the techniques soon after. Additionally, you might need to schedule some modules after field practice and field test to practice new situations that came up during field practice or retrain those that do not yet work well. Module between 45 min and 1.5 hrs tend to be sufficient to cover one core topic. For each module, first train interviewers on what challenges are likely to occur and what the best response are. Project the list of challenges, categories and good responses on the big screen. Work through each category, identifying together with the class the ways in which challenges may manifest themselves and any tips and tricks for a good response. Also cover bad practices that interviewers should avoid. Ask trainees that are good at a technique (experienced or good inter-personal skills) to demonstrate the best practices in front of class in a role play. Give feedback if necessary and ask the class for input to get consensus and make sure responses are appropriate for culture and context. Try to keep this part short. After having covered the theory, drill trainees until they are able to respond to a challenge quickly, correctly and in a natural manner. This is best done by asking trainees to simulate the interviewer-respondent interaction in front of class in a role play, providing immediate feedback and letting them repeat if necessary. Depending on class size and skill level, it might be best to divide the class into large groups, each supervised by a trainer. To practice, select one trainee to act as interviewer and if needed another one to act as respondent. Usually, it is more useful for the trainer to act as respondent as it allows them to better drive the conversation. Ask the selected trainee to come to the front or just stand up and speak towards the class, which is often faster. Explain the scenario and challenge and ask the trainee to re-enact the interviewer response as if it was a real field situation, speaking loudly so the entire class can hear. Give feedback on what they have done well and what not. Ask the class for inputs. If the response could be improved, ask the trainee to repeat the response. Collect feedback, correct them and ask them to repeat until the trainee does it well. Practice with as many trainees as possible or until the response generally works well. This can be quite a lot of fun and is a good way of breaking longer theoretical sessions. Two examples to illustrate: To practice introduction to the survey, select one trainee, ask them to stand up, tell them that they just knocked at a door, a certain type of person opened, that they should introduce themselves and ask for participation. Check that they are giving all necessary information, say nothing wrong, are easy to understand, use the right tone of voice, have good facial expressions and gestures, etc. Collect feedback from the class. Ask the trainee to repeat their introduction until it is correct in content and well presented. To practice asking for participation, ask a selected trainee to make an introduction and ask for participation as above. Act as respondent who is unwilling to participate, stating one of the challenges discussed in the training, such as such as I am really busy now or What do I get out of this?. Ask the trainee to respond and convince you to participate. Again, together with the class, check if the trainees response was quick, correct and natural. Give feedback on possible improvements and let the trainee repeat their response until satisfactory. 16.7 Special tasks For some surveys interviewers need to learn a special task that requires repeated and structured practice until they can perform it correctly and on auto pilot. This can take As a trainer, you need to identify trainees who are struggling with the task and be aware of scenarios that cause issues. Examples for special tasks are: Plot size measurement Anthropometry Collecting samples, e.g. to measure water quality Extracting information from registries, records, etc. Behavioral games Typically, these tasks require interviewers to strictly implement a protocol, use additional tools or aids or understand differently structured documents. Explaining the theory and going through one example is not enough. You need to practice in a structured way and evaluate trainees, exposing them to different scenarios if necessary. The best way of doing this depends on the task at hand. One option is to run through scenarios together so all trainees practice the same thing at the same time. For example, if interviewers need to record details from school time tables, display a picture of an actual timetable from a school on the big screen, and ask trainees to record the required information on their tablets. Run through examples of different time table formats, so interviewers get used to different scenarios. Walk through the class and identify any trainees who are struggling. Provide feedback and discuss in the wider class if there is any confusion. If interviewers need to perform a task on automatic pilot, for example, operate testing equipment, ask the class to repeat it over and over until everyone can perform it correctly all the time. Another option is to set up workstations that interviewers need to go through. Set up different scenarios in or around the venue and staff some with trainers if necessary. Trainees go from station to station and complete the respective task. At the end, each trainee should have undergone the same scenarios and a general debrief can be held. Trainers can observe each trainee at their respective station, evaluate and provide personal feedback. Set up stations such that interviewers spend their time practicing, not queuing. Examples for workstations can be plots with different shapes and cultivation, trainers playing common scenarios in behavioral games, or different documents that need to be evaluated. 16.7.1 Specialized teams If a task is very complicated, requires special background knowledge or if there is not enough time to train it well to all interviewers during the training, field work model permitting, it is often best to specialize some fieldworkers to exclusively conduct the task and distribute them across the field teams. Examples where this tends to be useful are: Anthropometrics, especially when taking all measures, i.e. weight, height/length and middle upper arm circumference (MUAC) Learning outcome assessments used in school surveys that include reading, writing and math tests with strict administration protocols. Medical tests, such as taking blood pressure or measuring hemoglobin values. This often requires the use of specialized equipment and trained nurses. Training any of those tasks can take several days. For example, learning outcome assessments require trainees to handle test booklets, CAPI and a timer all at once while keeping a young child engaged. It usually takes days to practice until all trainees can conduct the assessment tests in a homogenos way, and not everyone is good at it. Train selected trainees in separate sessions in parallel to the interviewer training. Practice all components of the task, including the equipment use, measurement reading, and interaction with the respondent. Invite practicing respondents to the training venue to practice the human component and create closer to field conditions. For more details, see section Inhouse respondents. For anthropometrics, the components that need to be practiced include the correct setup of scales and height/length boards, taking the measurements, how to treat the children/babies, reading the measurements (careful, some are dislexic), and recording them. At the beginning, the measurements taken for the same person differ quite a bit between trainees. Practice until they converge after a few days. parallel sessions 16.8 Pre-interview tasks better trained towards the end of training, prior to field practicing or final field tests, so interviewers can practice in the field immediately, no point in covering in detail at the beginning since most of it will not be used for a few days identify &amp; track respondents cover section in sursol, how to create interview file when and how to revisits replacement protocols how to identify households selection protocols, if e.g. in field sampling using kish tables. prep maps, tracking sheets, discuss explain protocols some parts might require exercising, e.g. how to use kish tables for in-field selection ?? TEAM: any comments on required content and modality? 16.9 Post-interview tasks ??Team any other post-interview tasks? In some surveys, interviewers receive feedback or inquiries about interviews they have submitted that require them to take some action or respond. In Survey Solutions, this is done using rejected interview files. This often goes wrong in surveys and needs to be covered in the training if implemented in the field. There is a big risk that interviewers fix interviews by simply changing answers to make issues go away, without actually addressing the underlying problem. Interviewers need to understand what they must and must not do. Train and practice in a dedicated session towards the end of the training. Ideally, trainees can practice with actual examples from interview files they have produced themselves. This makes it less theoretical and shows trainees that they made mistakes and what they were. You can use interview files from the field test (LINK) or even better, the final dress rehearsal (LINK), as interviews will generally be of better quality. This implies that the session needs to be held towards the very end of the survey. Prior to the session, carefully review submitted interviews and identify any issues. Normally, at the beginning there tends to be at least one per interview, make up some if there are none. Write comments and feedback as would be written during field work and reject the interviews to the interviewer. Try to do this with at least one file per trainee/pair. During the session, first give a quick general introduction to explain the rejected files tab on the interviewer dashboard, how to open a rejected file, how to read the rejection comment, how to see all comments by the supervisor, how to react to rejected files, respond to comments and resubmit them by marking them as complete and synchronizing. Then, project a rejected interview to the main screen and ask the respective trainee/pair to come to the front. They should explain what led to their mistake and what they should do if this occurred in the field. Ask them to respond to the issues and resubmit the file on screen. Ask the class to help and comment. Repeat this with as many files/trainees as possible and useful. Ask all trainees who did not solve their file on the big screen to solve it on their tablet and resubmit them. This exercise is also a great way to identify and streamline the rules around responding to rejected files, revisiting respondents, etc. For more information see chapter REJECTING FILES. 16.10 Final field test (often called pilot) At the end of the training, a final field test should be conducted in which all survey questionnaires and protocols are tested under real-field conditions prior to field work. It is an important exercise to: Expose interviewers to field-like conditions and give them the opportunity to practice without affecting the sample Determine if the field team is ready for field work Identify and address any outstanding issues with questionnaire or field procedures Some surveys do not conduct a separate pilot prior to the training and refer to the final field test as pilot. In those cases, the final field test importantly also serves as a reality check for questionnaires and protocols, since they have not been tested in full under field conditions before. The dfinal field test should be conducted at the end of the training and prior to the field work. Usually, 2-3 days of final field testing are necessary but also sufficient. If short debriefing/feedback sessions can be held in the late afternoon of each day, they can be on sequential days. If not, schedule debriefing/feedback days between field days. Allow for at least one class room day after the final field test for an extensive feedback session and to teach the class on how to deal with rejected files, inquiries from HQ or any other post-interview tasks. Additionally, you might need to allow for additional time for admin, rest days or travel to the field location. To create as close conditions to the field as possible, the final field test must be conducted with units that are similar to those in the survey sample, but crucially that are not part of the sample. For household surveys, this usually means conducting the final field test in un-selected communities within the survey areas and often requires the team to travel from the training location closer to the field. For surveys part of RCTs, this means practicing within the treatment areas. Ideally, locate the dfinal field test such that the entire field team can be supervised and debriefed together in a central location. This ensures that all field workers have received the same inputs and reduces team effect. Split the final field test into different locations if there is a large variability in the sample that affects questionnaires or protocols. If doing so, try to streamline as much as possible the supervision and feedback given to all groups, e.g. by trainers exchanging notes before providing feedback to their respective group. Often it is easiest to select a set of testing PSUs (e.g. communities) during the sample design and have them undergo the same preparation steps as the sampled units, e.g. assignment of IDs, procurement of reference data, add them to the CAPI questionnaire, do community sensitization, etc. For surveys with a separate listing exercise or panel surveys, conduct the dress rehearsal for all stages using the same units and keep the data, so you can properly prepare and test each stage of the survey. It can be very laborious and cumbersome to prepare separate data. One important aspect of th efinal field test is for field workers to practice the selection, identification and tracking of survey units such as households, convincing respondents to participate and to learn how to make appointments and to manage time. Do not pre-arrange respondents as one would do during field practice. Equip interviewers with the same assignments or tracking sheet format they would receive during field work. If you do not have lists or names for the rehearsal units (e.g. you have the household names for sampled communities, but not for practicing communities), send someone to the community prior to compile lists that you can use to produce the assignments/tracking lists. Allocate interviewers to their expected field teams and let the supervisors assume their management role, including the assignment of cases to interviewers. For the final field test, often it is beneficial for interviewers to conduct interviews in pairs. This reduces the number of cases required in any of the visited communities, increases the probability of finding available respondents and allows interviewers to help one another. Each pair can conduct multiple interviews per day and take turns in administering the questionnaire. As in the field tests, both interviewers should record all answers in their own tablet and submit the interview in the end. For the final field test you might need to relax replacement protocols and add additional replacement units, so interviewers can practice conducting some interviews even if nonresponse is high. Make sure field teams arrive early in the communities, so that interviewers have enough time to locate cases and conduct interviews before respondents start cooking/having lunch and become unavailable. As for the field practice, make all necessary logistical arrangements to prevent delays or hungry/thirsty field workers in the field. Collect feedback and debrief as described for field practice. Ask all interviewers to submit their interviews at the end of the day. Use the data to finalize your data system and quality control system, e.g. by updating the data checks. Review submitted interviews using the standard processes of the quality control system. In addition, visually review as many cases as possible and compile feedback for interviewers and the quality assurance team. The data collected during dress rehearsal is a great way to learn and practice receiving feedback with the team, see chapter feedback. 16.11 Supervisor Training ??Team: any inputs? DHS has some (limited) material 16.12 Data Monitor Training ??Team: any inputs? References "],["assessment-selection.html", "Chapter 17 Assessment &amp; Selection 17.1 Written Tests 17.2 Evaluating skills 17.3 Specialised tasks 17.4 Field worker selection", " Chapter 17 Assessment &amp; Selection Often field worker selection is done by the implementing organisation that might follow their own procedures. Be cautious as the selection criteria and processes might not result in optimal outcomes. We recommend the following procedures. However, the processes described here are highly recommended. Try to exert some control over the implementing agency and make it conditional. ultimate means to get to knwo interviewers so you can select them. last stage of interviewer recruitment process. Past survey experience, CVs and interview often not enough during recruitment process to identify those that are good at being interviewers form those that are not. Motivation, etc can only be observed over a sustained amount of time. Selection rpoecess should be at the end. ?? Especially when working with unknown teams of trainees, it is beneficial to not determine roles up-front, but to select trainees into field worker roles (interviewer, supervisor, etc) based on their skills and knowledge. Long term supervisors sometimes develop a sense of entitlement or superiority, dont pay attention as much and as a result dont know the material as much as interviewers. Sometimes they also dont hold their position due to other factors. See more on sections FIELD WORKERS and SELECTION. if pre-determined supervisors data mmonitors or other roles, they have to attend, contribute to and demonstrate understanding of the material in order to take on supervisory roles. here make overall process clear again: train excess evaluate on ongoing basis, content and skills select at end of training rationale: - allow selection of best (only capable ones), discard - general motivator during the training, as trainees understand ti is serious and merit based selection, same attitudes transmit into field work - better performance: people do better if they know it matters and and cared about - selection into roles based on skills, capacity, best fits, supervisors, monitors etc. - transparent and objective selection, fair, if applied over repetaed surveys overall improvement of interviewer stock. - creates pool of trained fieldworkers that can be brought up to speed relatively quickly if there is field worker attriction during field work. Different fieldworker roles can be trained in parallel training sessions. These can start from day 1 if there is no overlap in questionnaires/tasks and if different profiles of trainees have been recruited for different roles, e.g. trained nurses to conduct anemia tests. If not, it is usually useful to start parallel sessions after a few days of general training once there is a clearer picture of trainees capabilities and personalities. E.g. measurers need ot eb good with kids and practice, good number rading skills, but do not need to understand all concepts in a long hh questionnaire. Allows to trainees that perfroma ccordingly in the training. slower ones might become the best interviewers. 17.1 Written Tests measns to test caapcity also make sure trainers and designers thinking is aligned with that of the team very powerfu to show trianees what they do not know and can be agreta motivator to pay attention, participate in the training and learn. Conduct written tests on a regular basis throughout the training, i.e. daily or every other day during class room days. On days with in-field practicing it is often not feasible to conduct tests, and there tends to be less new theoretical material that can be tested. 17.1.1 Designing tests Tests should probe the understanding of the questionnaire content and manual covered in the training so far, including protocols, definitions, answer options, discussed etc. but also for general skills like calculating the percentage or average if needed for the survey. If possible, tailored tests to the discussion of the (previous) day, so that they also probe for trainees attention. Repeat questions or topics from previous tests that a large share of trainees answered wrongly. Write questions clearly and that they can be clearly answered unambiguously. Good ways to ask questions in the test include: Describe a difficult scenario, followed by a question of the questionnaire, asking trainees to record or select the correct answer. Write statements and ask if they are true or false. List things and ask trainees to select all that apply, e.g. select all those people who would be household members according to a definition, all the jobs that are considered a self-employment, etc Avoid putting many questions with a simple yes/no or true/false answer. They can be answered 50% correctly by just randomly selecting an answer and are no good indicator of level of understanding. Instead, include more numeric questions and multi-select questions that require a combination of answers to be selected or steps to e taken to be correct. Dont make it too obvious. Sprinkle irrelevant or misleading information into the question text or scenarios. Add Dont have enough information as an answer option. For multi-select answer questions, make sure the wrong options are plausible, and occasionally use only correct or wrong options. Avoid open-ended questions, unless you have enough resources to review and score them for all trainees. See here for examples of tests that have been used in some training sessions. IFAD PACKAGE TEST, ANY OTHER? Aim for tests that take 15-30 minutes to answer so you get enough data points while not taking too long. That translates to around 10-15 difficult questions, some with multi-response answers. 17.1.2 Conducting the test Written tests have been conducted in the following ways in interviewer trainings: Test printed on paper. Do not print all tests up front, as they probably need to be updated each day. Print in the training venue or bring a printer. Questions on screen and note answers on a sheet of blank paper. Very flexible and useful for spontaneous tests as it allows last minute updates. Trainees note their name, question numbers and answers on a blank sheet of paper. Questions are read out or displayed on the screen. CAPI. Only works on CAPI software that is quick and easy to code. Write the test as a questionnaire in CAPI, assign it to trainees who complete one on their tablet and send it back. Often you can copy questions from the questionnaire and modify them. The answers are immediately available as data and can be corrected using code. Trainees learn to handle the CAPI tool and submission of interviews. Make sure to have an internet connection for all trainees to sync. Google forms or similar. There are several tools online that can be used to quickly write tests, deploy them on the trainees devices and make the results available. Most of them are online based and require good internet connectivity. To conduct the quiz, disperse trainees throughout the training venue, so that they cannot copy from one another nor talk. For electronic tests, trainees only need their tablet, so can do without a desk. Walk around and invigilate the test to make sure nobody. Help those that have technical issues and provide clarifications (to the entire class) if there is any general doubt about the quiz. Usually, you can allow trainees to refer to the manual or their notes during the test. This reinforces the manual as a reference and probes if trainees can find out the correct answer with the tools they have available during the field. Give sufficient time to trainees to complete the test, even if they have to refer to the manual a few times. Those that take a bit more time are not necessarily worse interviewers. You can set an exact time limit, or just stop the test once most trainees have handed in their test and once there is no more progress among the remaining ones. There are different advantages for different hours of the day to hold the test. If holding it in the morning, trainees are freshest and have had a chance to revise in the evening if they were falling behind or absent, but it is probably best to do the feedback session right afterwards. Holding the test at the end of the day makes good use of the end of the day that often is unproductive, and also stops trainees from disappearing during the day as it effectively is an attendance call. Trainers have the opportunity to mark in the evening and give feedback the following morning, which is a good way to repeat the material of the previous day. Also, you can ask trainees who have completed the test to leave the venue , so they do not have to wait for everyone to finish. 17.1.3 Marking tests There are several options to mark tests quickly. For tests conducted on paper, collect completed test papers and redistribute them to other trainees during the feedback session. Give clear guidance on how to mark each question as you go through the test with the class, e.g. ask to put a tick or x on the left of the question number, and at the end, count ticks and write the number in the top left corner. During the feedback sessions, ask those looking at a particularly wrong answer for a question to read it out together with the name. The trainee who answered wrong can then explain to the class their thinking that led to the answer and be corrected by the class. To get a feeling on how individual questions went, ask for a show of hands of those who marked it as correct or wrong. At the end, collect the test papers, cross check the marking and and record the score for each trainee. For tests conducted on CAPI or any other electronic tool, you can export the data and write a short script to mark the test. The script should for each trainee calculate the score and for each question, give the percentage of correct answers, tabulate wrong answers and list the names of trainees who answer it wrong. Have (a draft of) the script ready, so you can have the feedback session shortly afterwards. In some tools you can specify the correct answers and do the marking directly in the tool. If using Survey Solutions, this can be done by creating variables and displaying the score in supervisor level questions or questions conditional on supervisor level questions. 17.1.4 Providing feedback For each test, hold a feedback session, ideally shortly after the test, but no later than the following morning. Do better than just going through the correct answers. Actively involve trainees who got questions wrong to learn where misunderstandings are coming from and to ensure that the corrections are understood at the end. Involve the rest of the class to correct and explain in their own words. One way to achieve this is to project the test on the screen and work through it question by question. For each question, ask one of the trainees who answered it incorrectly to come to the front, read the question, and answer it in front of class, explaining their reasoning and cognitive steps. The class should not help the trainee. Once answered, ask the class if the answer was correct or wrong, the reasons why and to provide feedback. Step in if the class response is incorrect. Make sure the trainee in front understands why they answer the question incorrectly. Sometimes discussions arise if test questions were ambiguous or if trainees feel like they have been marked incorrectly. Do not let discussion get carried away. Intervene, repeat what is correct and why and move on. If there were any issues with the test question or the marking, you can not count the question towards the overall score. Announcing this usually helps to appease discussions. Make sure to correct any issues in the manual or questionnaire if they were the reasons for the misunderstanding, as they would likely cause confusion again if unaddressed. Keep an Excel sheet or other systematic record that for each test records the total number of points possible, and the points achieved by each trainee. Calculate a total score and ranking. With tests probably not being equally long or difficult between the days, you probably want to calculate the total score as the sum of points achieved over the sum of total points possible. In the sheet, also record marks and comments from observations, see the following chapter. 17.2 Evaluating skills Understanding the questionnaire alone does not make a good field worker. Other skills are equally important and must be observed during the training to inform a good field worker selection. Examples are the ability to use a tablet and CAPI, friendliness, seriousness, thoroughness, motivation, intonation of voice, etc. Observing those skills with all trainees and evaluating their qualification as fieldworkers during the training requires you to be systematic, produce comparable scores and make good use of all sessions. Doing it just like that without a systematic marking and non comprehensively is not fair and becomes quickly unfeasible to get a comprehensive picture if there are more than a few trainees. There is benefit in identifying as early as possible if trainees do not possess any of the key skills, so you can still address it in the training, e.g. by giving additional extra tablet/CAPI practice sessions for those not comfortable enough using the tablet or software. Want to do skill matrix, i.e. know which trainee has what skills can be used together with written exams for field worker selection Benefits: find right people Identify gaps early, so can do something about it How to create a skill matrix: 17.2.1 Identify key skills To produce a comparable and comprehensive ranking, identify a few key skills that matter most for the project and cannot easily be probed for in the written test. Examples are: CAPI and tablet use Sound of voice, intonation (especially for phone surveys) Reading questions, reading speed Ability to convince respondents to participate in survey Capacity to self-organize Inquiring Friendliness and likability Determinism (especially if high unit non-response rate is expected) Trustworthiness and reliability (especially if working alone without supervision) Thoroughness Team leading and organizing (if selecting supervisors from trainees) Interaction with children and mothers (for anthropomorphic or student tests) Aim to get a rating for each of the selected key skills for each trainee during the training. Be realistic, prioritize and select only the skills that matter most, max 3 - 5, so you actually manage to score all trainees against them. If there is limited capacity and time to score against several criteria, give trainees scores for overall skills. When scoring, evaluate against the key skills, and record any diimensions in which they are outliers, i.e. perform very well or bad. 17.2.2 Design a scoring system You need to use a common scoring system to evaluate skills across trainees, especially if more than one trainer is evaluating trainees. Prepare a detailed scoring system with descriptions of each score. Make sure trainers know it and refer to it during scoring. Table 17.1 shows an example scoring system ranging from 1 to 5: Table 17.1: A skill scoring system. Score Rating Description 5 Excellent, exceptionally mastery Can be expected to perform extremely well. Can act as an exemplary role during training and field work. 4 Very good, above average More than adequate for effective performance. Possess high skill level. No counterproductive behavior or deficiencies. No major deficiencies. 3 Good, acceptable, average Should be adequate for performance requirements. Posses acceptable skill level. No major counterproductive behavior or deficiencies. 2 Weak, less than acceptable Insufficient for performance requirements. Does not possess sufficient skill level. Some counterproductive behavior or deficiencies. 1 Poor, unacceptable Significantly below performance requirements. Does not possess the skill. Counterproductive behavior. Many deficiencies. 17.2.3 Create a skill matrix Create a table with trainees in rows and skills as columns. Each cell should contain how the respective trainee scores in the relevant cell. Color coding helps to quickly generate an overview and identify generally weak skills or under-performing trainees. A quick and easy way of doing this is in a Spreadsheet. See an example here. There are different ways of filling the matrix: Trainers can record scores in their notes and copy them into the final matrix. This is easiest to set up, but cumbersome, difficult to keep track of and reconcile if trainees have been scored more than once for a skill. Print the empty skill matrix for trainers to record their scores. Copy the scores into the final matrix. Slightly better than above, but still cumbersome and difficult to reconcile. Record scores in a Google Form that contains a question to select the trainee name, one optional question for each skill, and a text field for comments. Trainers fill the form on an ongoing basis, rating only skills they have observed and recording comments. Answers are automatically stored in the Google Sheet, including the name of the scoring person and date time. Answers in the skill matrix can be calculated to be the average or last rating. Ideally, the skill matrix also contains the results for every test and the overall score, so you can get a quick overview of trainees performance. If needed, you can calculate an average score across all skills for each trainee and also combine it with the test result to obtain a single score only, but be aware that the individual components might be quite different and best not be aggregated, or might have to be weighted differently. Always also rely on the individual components if referring to the aggregate. 17.2.4 Rate skills A fieldworker training only provides you with a short amount of time to do a comprehensive rating of trainees skills, so start as early as possible and rate in all phases of the training. During the theory part of the training, engage as much as possible with trainees and score them against skills that you can rate based on the interaction, e.g. you can score their ability to read out questions or intonation of their voice easily during the questionnaire reading sessions. Some skills can also easily be observed during the classroom-based training by trainers not leading the class, e.g. the ability to use the CAPI software or tablets. During practicing sessions, in class or in the field, assign to each trainer a set of trainees to be observed and scored. Trainers should observe a trainee long enough to get a good enough impression to be able to score them against one or more skills, and should then move on to another trainee. Regularly check the skill matrix for completion, and target trainees that you have not evaluated yet. Rate If you can afford it, try that each trainee has been rated by more than one trainer to create a consensus decision. Give feedback to trainees. If you scored them low on a skill, explain why and that they should try to improve it throughout the training. Be aware of changes over time as trainees have (hopefully) acquired skills during the training. Scores from the beginning of the training for one trainee are not necessarily comparable to those for another at the end. Try that your scores are reflective as much as possible of the skill levels at the end of the training, as you are ultimately interested in trainees potential to perform in the field, not their learning curve throughout the training. Reconfirm extreme scores, especially bad ones towards the end of the training, and scores for trainees that are not clearly in or outside of the selection that is merging towards the end. 17.3 Specialised tasks have them do the tasks, and test outcomes. Anthro testing, compare against expert measurer. 17.4 Field worker selection Field worker selection should happen at the end of the training and be skill-based, objective and transparent. anthropomorphic standardization tests, compare individual testa against the group average and those of experts. stations, see MICS page 13/14. 17.4.1 Timing Select field workers at the end of the training and prior to the start of field work. This could be before or after the final dress rehearsal and depends on the survey circumstances. Selecting after the address allows for more time to observe trainees and for trainees to show their strength in the field. Trainees that will not be selected and act as a replacement reserve will have had some field experience. On the other hand, selecting prior to the dress rehearsal allows you to form the actual field teams already, so they can practice in their field composition and so that you can make adjustments if necessary. AK NOTES: Some trainees might deselect themselves during the training by dropping out. Prior to the selection, only dismiss trainees where it is already clear that they have unacceptable skill levels or personality traits (e.g. unrealibility). be careful to not drop trainees that might improve. Many need more time, or start to improve once they realise their relative performance levels are low. 17.4.2 Criteria The fieldworker selection should only be based on trainees overall test results, skill ratings and other objective characteristics relevant for the project. Do not select based on subjective criteria such as personal liking or criteria that are irrelevant such as having or not having a degree. How you weigh individual criteria highly depends on the project and training outcome and has to be decided on a case by cases basis. Do not exclusively rely on the overall test scores by simply selecting the trainees with the best scores. Do not overvalue or favor experience, as experienced interviewers are not necessarily better. If you select survey teams based on capacity, over time the best interviewers will be the most experienced ones. See Chapter RECRUITMENT for more information. The selection should be comprehensible and justifiable using the applied criteria. For example, if one trainee has been selected over another one who performed better in the tests, there should be objective reasons, such as higher scores in relevant key skills. You can decide to make the selection and criteria public to the trainees after the selection. 17.4.3 Selection process The following selection algorithm tends to work well. You might have to adapt it a little to meet the requirements of your survey: First, discarding trainees with unacceptable performance or skill levels. They should not be used as field workers unless they receive special training after the training to bring them to acceptable levels. Select all special roles among those who score highest in the required special skills. For example, select data quality control officers among those who have well understood the material, are very reliable and thorough. Select supervisors among those who have good leadership and organizational skills, but do not necessarily need to be top of the class in the test results if they are not involved in the data monitoring (see chapter FIELD). Select anthropometrics specialists who did best in anthropometrics training. Rank the remaining trainees based on test scores, skill ratings, and other criteria you might have and select from top. Often you might find that you have too few or too many trainees with adequate skill levels. Ideally you can adjust your field work model accordingly. If there are too few, only select those capable and use fewer or smaller teams for longer, or extend the training to bring them up to speed. If more trainees qualify than required, you can think of using larger or more teams for a shorter amount of time, but be aware that larger field teams are harder to monitor. Allocate selected staff into teams, double check they balance well and adjust steps 2 and 3 if not. For team allocation, keep in mind any special requirements you might have, such as gender balance, languages, etc. Try to balance the skill levels within teams by mixing top and bottom of class, so stronger trainees can support and act as a reference for weaker trainees. Make sure teams know who should learn from whom, and that they should support one another. Also be aware of personal traits and group dynamics when allocating into teams. Keep trainees with adequate skill levels who have not been selected as reserve to replace fieldworkers who drop out during field work. Inform them that they are on a waiting list and might be called should others leave. "],["introduction-1.html", "Introduction", " Introduction work in progress problems : one their own, far away, monitored, can be demotivating, little feedback, increasing idiosyncratic behavior leading to interviewer effects, measurement error "],["listing.html", "Chapter 18 Listing", " Chapter 18 Listing work in progress "],["monitoring.html", "Chapter 19 Monitoring", " Chapter 19 Monitoring work in progress "],["effective-feedback.html", "Chapter 20 Effective Feedback", " Chapter 20 Effective Feedback work in progress "],["avoiding-nonresponse.html", "Chapter 21 Avoiding nonresponse", " Chapter 21 Avoiding nonresponse strong revisiting protocols make it hard for interviewers, remove any incentives, have to give phone numbers confirm, etc follow up on non-repsonse "],["notes-1.html", "Chapter 22 Notes", " Chapter 22 Notes MICS has cluster control sheets first few days, trainers shoudl observe field work and provide field work. "],["introduction-2.html", "Introduction", " Introduction work in progress "],["data-editing.html", "Chapter 23 Data Editing", " Chapter 23 Data Editing work in progress ISCO codes, best done in batch by dedicated persons. "],["weighting.html", "Chapter 24 Weighting", " Chapter 24 Weighting "],["archiving.html", "Chapter 25 Archiving", " Chapter 25 Archiving work in progress "],["documentation.html", "Chapter 26 Documentation", " Chapter 26 Documentation work in progress - report on Standard errors - ideally "],["abbreviations-and-acronyms.html", "Abbreviations and Acronyms", " Abbreviations and Acronyms CAPI Computer Assissted Personal Interviewing CATI Computer Assissted Telephone Interviewing DK Dont Know LSMS Living Standard Measurement Survey NSO National Statistical Office PAPI Pen-and-Paper Personal Interviews ToT Training of Trainers TSE Total Survey Error TSQ Total Survey Quality "],["references.html", "References", " References "]]
