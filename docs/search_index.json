[["index.html", "The LSMS Guide to Survey Quality Preface About this book How to use this book About The LSMS Guideline series About the authors", " The LSMS Guide to Survey Quality Andreas Kutka, Josefine Durazo, Kevin McGee, James Arthur Shaw Preface Hello there. Thanks for dropping by. However, you have come a bit early. This book is still work in progress and in a pre-publish state. PLEASE DO NOT SHARE. We look forward to seeing you back here in a few months, when everything is ready. Version 0.1. Last updated on 19 July 2022. About this book What is survey quality and how can it be achieved? This book gives practical advice experiences and practices that have worked in. based upon our experience in designing and implementing large scale socio-economic surveys in a variety of contexts. Readers who are interested in a cheat sheet version can read the key recommendations for fieldwork training in the TL;DR below. Those looking for more detail will benefit from reading the respective chapters. recognises the multi-dimensionality of survey quality and follows the total survey error paradigm total survey quality optimization the practical tips and techniques that  The book is targeted towards a range of survey practitioner roles. Survey designers, sponsors will find guidance on design decisions that are relevant for quality in chapters X and Y. They might also benefit from reading the introduction chapters to the following chapters. Survey implementer will find tips &amp; tricks and best practices in chapters Z and ZZ to maximize quality for all main steps of survey implementation. Individual sections provide details for individual key roles, including trainers, translators, etc. The book provides detailed steps and best practices for all survey phases. Survey practitioners are not encouraged to implement everything, as this would be beyond most surveys budget and time constraints, and as there might be alternative methods with equally good outcomes. Survey practitioners can use the detailed information for those components that form part of their quality assurance plan, or use the detail to reflect on or improve own practices. The book is a companion and go-to reference for survey practitioners aiming to improve the quality of the surveys they design and implement. The book follows the survey life cycle, and explaining the best practices and providing tips and tricks on how to achieve high quality surveys. The book is very comprehensive on each of the survey phases. Readers are not expected to implement all how-to-guidelines listed in the book. Instead, the book is meant to be a collection best practices to achive high quality in surveys. Survey practitioners are encouraged to evaluate which improvements can be implemented within the context and constraints of their respective survey. Follows the total survey quality framework, and touches base on the survey design, implementation and analysis. The book is written as companion for survey managers to help them take the right decisions, organize their team and works streams and how to obtain the right types of inputs in every phase of the survey to achieve high quality outputs. While providing a lot of detail in some parts, the book does not aim to be a comprehensive how-to-survey guide. Further guidelines and the input of experts are needed to achieve high quality surveys. The recommendations described in the book should not be interpreted as being the one and only way of implementing surveys. For many parts, there may be alternative approaches that deliver results of similar quality. However, the methodology presented in this book has proven to be effective in delivering quality surveys while being efficient, huzzle-free and without bad surprises. The authors welcome comments that point them towards other approaches. The book is divided into 6 parts. Part I provides a short introduction of key concepts of survey quality. It is the only theoretical part of the book. Parts II - IV follow the different stages of a survey life cycle from inception, design, preparation, training, fieldwork to post-field. Each part is comprised of several chapters. The first chapter to each part is an introduction that outlines the content of the remaining chapters and provides a cheatsheet version of our key recommendations, called TL;DR. References for each chapter are added to the bottom. In the Appendix, you can find the list of Abbreviations and Acronyms, as well as a complete list of all references used throughout the book. The book is a living document that is being expanded and revised on an ongoing basis. Whenever we become aware of new best practices, we will try to integrate them into the book. The last update was made on 19 July 2022. Book is often written with reference to a LSMS-style household surveys implemented in cAPI (Survey Solutions) as most surveys are now. However, almost everything in the book applies generally to most survey types or other software packages. How to use this book The book provides a lot of information and details that are probably difficult to absorb entirely and retain for the duration of the survey. For those who are managing one of their first surveys we recommend skim reading the document focusing on the introductions to each chapter and skipping the details. They summarize the key points and provide you with a general idea of what matters to achieve survey quality. Once the survey is underway, reading the relevant chapters prior to thinking about a phase will give you all the details you need at this point. Experienced survey managers who are curious about or aiming to improve certain aspects can read the relevant chapters only. The book is relatively extensive and contains a lot of detail. It is not written to be read in one go (we applaud your stamina if you do), but rather as go-to reference that can be visited during any stage of the survey to get relevant practical advice and learn about best practices. Chapters are written to be as independent as possible of each other. This causes some points to be repeated in another chapter. Links to other chapters are added to where relevant information is References are provided if they built upon other parts of the book or if more detail is provided elsewhere. Navigate. Use the Table of Content (ToC) bar on the left to quickly navigate between different parts of the book. Clicking on a chapter in the ToC unfolds all sub-chapters. The right/left arrows on the side of the main text navigate to the next/previous chapter. Links in the text take you to external resources or other chapters of the book providing more details. Find. Use the search function to quickly find all occurrences of a word or phrase. Open the search function by clicking on the search icon on top or by typing f on your keyboard. Use the up and down arrow keys to navigate between different search matches. Edit. You can edit the book! Yes, you read this correctly. If you come across anything that you think could be improved, from correcting typos to adding content to chapters, click on the edit icon on top. This will open the underlying file on github where you can make the modifications and submit them as a pull request (you will need a GitHub account). The book is written using Rmarkdown in Bookdown. For most parts of the text, the syntax follows simple markup language, of which you can learn the basics in under 2 minutes. (For more experienced GitHub &amp; Bookdown user, you can fork the depository and make commits). If accepted, your suggestions will be added with the next version. Comment. If you have any suggestions but prefer to not edit the book, please list an issue on GitHub or email the authors. Thank you! Share. You can share the book or any chapter using the sharing icons on the top right. Each chapter and sub-chapter have specific links. Copy the link, either by right clicking on the blue # icon next to the heading and copying the link address, or by clicking on the (sub)chapter in the table of content and copying the URL from the browser. About The LSMS Guideline series The LSMS Guidebook series offers information on best practices related to survey design and implementation. While the Guidebooks differ in scope, length, and style, they share a common objective: to provide statistical agencies, researchers, and practitioners with rigorous yet practical guidance on a range of issues related to designing and fielding high-quality household surveys. The series draws on experience accumulated from decades of LSMS survey implementation, the expertise of LSMS staff and other survey experts, and new research using LSMS data and methodological validation studies. About the authors Andreas Kutka . Josefine Durazo  Kevin McGee  James Arthur Shaw  "],["introduction-to-survey-quality.html", "Chapter 1 Introduction to Survey Quality", " Chapter 1 Introduction to Survey Quality Sample surveys are a crucial data source for official statistics, impact evaluations and social economic research. Survey data informs public policy and other important decisions. Quality in surveys is paramount for good policies and sound decisions and should be the aspiration of all survey practitioners. In Part I of this book we summarize key frameworks used by survey methodologists and national statistical offices (NSOs) to conceptualize and maximize quality in surveys. Skip this part if you are not interested in the theoretical background of survey quality. We will link back to the concepts throughout the remainder of the book, so you can read only the parts that are relevant. Quality can be defined as fitness for use. For something to be fit to use it must be free from deficiencies and respond to the users needs (Juran and Gryna 1980). In a survey context, freedom from deficiencies translates to the requirement of survey data to be accurate to allow inferences about the target population. In other words, a survey statistics must accurately describe the population for it to have any value. Responsiveness to users needs translates into aspects such as timeliness, usability and completeness that are important to the user of survey data. Even the most accurate data can be unusable if it is outdated, poorly documented or not rich enough for the intended analysis. Which user attributes matter, depend on survey. As we can already see, survey quality is a complex, multidimensional concept. In Chapter 2 we look at Total Survey Error (TSE). It is the dominant framework used by survey methodologists to maximize the accuracy of survey estimates within survey constraints. We describe different sources of survey error and provide real-world examples for each error source. We explain how error affects survey statistics and how the TSE can be used as a planning criterion to reduce overall error in survey estimates. In Chapter 3 we introduce the notion of Total Survey Quality (TSQ) that combines survey accuracy with user-specific dimensions of quality. The concept is the basis of multi-dimensional survey quality frameworks that are now used by many national statistical offices as a planning and evaluation tool. We present a general strategy how TSQ can be used to design surveys that maximizes survey quality. The chapters are based on the articles by Biemer (2010) and Groves and Lyberg (2010), and the books by Biemer and Lyberg (2003) and Weisberg (2005). They are good starting points for readers who are interested in learning more about the theory of survey quality. References "],["tse.html", "Chapter 2 Total Survey Error 2.1 Error sources 2.2 Survey-related Effects 2.3 The Impact of Error 2.4 Minimizing TSE", " Chapter 2 Total Survey Error In the survey context error refers to the deviation of a survey response from its underlying true value. Probably the best-known error type is sampling error that occurs when collecting information from a sample of the population rather than the full population. Early in the history of probability sample surveys, it was recognized that there is a variety of non-sampling error sources that can equally cause a survey statistic, often referred to as survey estimator, to deviate from its underlying true value in the population, referred to as population parameter. Over time, this knowledge evolved into the concept of Total Survey Error (TSE), which has become the dominant paradigm in the field of survey methodology Groves and Lyberg (2010). TSE acknowledges that error can arise from many different sources. For example, error might be introduced by researchers formulating questions wrongly, interviewers not probing sufficiently or respondents unwilling to respond. Error may occur at any point in the survey process, including the design, sampling, implementation, processing or analysis, and may be caused by any actor including researchers, interviewers or respondents. TSE denotes the aggregate of all error sources in an estimate and is thus the difference between a survey statistic and the population parameter. A good presentation of how TSE components link to the process of generating a survey statistic has been made by Groves et al. (2004), which is shown in Figure 2.1. Figure 2.1: TSE Components Linked to Steps in the Measurement and Representational Inference Process 2.1 Error sources The TSE framework decomposes survey error into different error sources. Survey methodologists have produced a number of similar classifications. They divide TSE into sampling error and non-sampling error, that in turn can be further decomposed. We follow the classification developed by Biemer and Lyberg (2003). We briefly introduce each error source and provide real-world examples that we have observed in our work with NSOs, international organisations and survey firms. 2.1.1 Sampling error Sampling error describes the error that occurs if a sample of the population is surveyed, instead of surveying the entire population, i.e. conducting a census. It is determined by the sampling scheme (simple, multi-stage, stratified, ), the sample size and the choice of estimator. If probability sampling is being used, the sample error is randomly distributed and can be mathematically computated. It is probably for that reason that sampling error is receiving a disproportionate high level of attention by researchers and survey designers. By definition, sampling error occurs in every sample survey. Sampling error becomes problematic if it is unnecessarily large due to faulty or inefficient sampling design or is minimized at too high expenses of other error sources. If non-probability sampling methods are being used, sampling error may also introduce bias. Examples include: The so-called Random walk is an oxymoron. Interviewers can easily tweak the sequence in which they list units, so that respondents are selected that meet certain characteristics, such as being available for an interview or being friendly. Random walk is a not a probability sampling method and introduces bias. Unfortunately, some surveys still use it for a fast and cheap second-stage sampling. A HIES used convenience sampling to select individuals within collective living quarters as respondents were unable to list all individuals living there. Interviewers selected whoever was available during their visit, effectively removing individuals from the sample that were unavailable during working hours, thus introducing bias. A probability sample method could have been used instead, for example, by listing all lockers or beds or other items associated with individuals, randomly selecting items from the list and putting effort into interviewing the associated individuals. A survey part of an impact evaluation study selected a varying number of households per community, depending on the community size. In the largest communities, up to 40 households were interviewed. While this sample design does not increase sampling error nor introduces bias, it is inefficient. Beyond 20, the marginal decrease of sampling error per additional household per community is negligible. The resources spent to interview them could have been used to decrease error from other sources instead, for example by extending training or increase monitoring. 2.1.2 Specification error Specification error (referred to as Validity in Figure 2.1 occurs when there is a misalignment between the concept a survey intends to measure and the concept that is actually being measured. As a consequence, the wrong parameter could be estimated by the survey and invalid inferences be made. It is often caused by insufficient definition of concepts, poor questionnaire design, lack of cognitive testing, absence of good manuals, or concepts not being preserved during questionnaire translation or CAPI scripting. Examples of specification error are: Surveys that aim to capture income from any kind of employment sometimes ask opening questions like In the past , did NAME work as an employee for a wage, salary, commission, or any payment in kind?. Designers and data users assume that this question captures informal jobs. Often however, respondents understand this question to ask about formal employment only and answer with No, even though they have worked as day laborers for a few days. The question does not capture the concept of employment as planned by designers or interpreted by analysts. Researchers or questionnaire designers developing questions without taking into consideration local circumstances or testing that underlying concepts are understood the same way in the population. For example, questions around saving behavior tend to be delicate and highly dependent on economic circumstances. Researchers assuming that goods bought on credit are included in a loan roster, while the questionnaire did not explicitly probe for them and trainers did not train interviewers to include them. Mistakes that happen when updating translations or instruments. last-minute change is made to the questionnaire during the training, in the local language that never finds its way into the original version in the design language which is being used as data reference. Data users will interpret the question differently to how it was being asked. Household to be translated as family in the local language, referring to a different set of people. Household refers to the economic unit and does not necessarily imply family ties. Many languages lack a word for household. An expression like those living with you would often be more appropriate. Sometimes, CAPI scripting can alter the nature and functionality of questions or modules to such a degree that it affects the way they are being administered and ultimately changes results. Researchers who do not observe the CAPI questionnaire being fielded remain unaware of such differences. For example, time use modules are often designed as a grid of activities and hours of the days into which lines are drawn for the primary activity done for a certain period. We have seen this module being scripted in CAPI as series of 48 questions asking the respondent for the main activity for each 30-minute interval. The responses produced are not at all comparable. DHS and Is it OK to beat your wife question NOTE: ARTHUR TO WRITE Different household definitions, different results? 2.1.3 Coverage error Coverage error or sometimes frame error occurs if the sampling frames that are used to select the survey sample include units that are not part of the target population, exclude units that are part of the population or duplicate them an unknown number of times. It also refers to errors in auxiliary variables associated with the frame units, or if they are missing. Frame error can cause parts of the population to be under- or over-covered, distorting the sample or impact evaluation design. Since building perfect frames is often practically not possible, most surveys suffer from frame error to some degree. More severe cases of frame error often stem from a lack of quality assurance of listing exercises, the use of unverified lists compiled by third parties, or the use of outdated lists. Examples are: During listing exercises listers sometimes focus on the population-dense village centers, as it is quick and easy to list a large number of households, but omit households that live further away or are difficult to reach. As a result, remote households are under-covered in the frame. Community lists are usually being used as frame for first-stage sampling in household surveys or to inform impact evaluation design. In some contexts, these lists can be of very poor quality. We have experienced field teams searching for non-existent communities or that community characteristics grossly mistmatch the auxiliary variables used for sampling or matching. A survey for an impact evaluation in Per√∫ used household lists for the second stage sampling in treatment areas that had been compiled by the project to be evaluated. Several local dynamics played into the creation of the lists. As a result, in many villages, several members of one household were listed as separate households on the list, friends and family of project officials from outside the village were included, or beneficiaries from the village excluded. 2.1.4 Nonresponse error Nonresponse error can occur on the unit level, when a sampled survey unit (e.g. household, individual, firm, etc.) cannot be interviewed for any reason, or on the item level, when parts of the questionnaire remain unanswered because the respondent did not answer some questions for any reason. Nonresponse error can severely affect the validity of survey data. Since the reasons for nonresponse are hardly ever randomly distributed, the actual respondents may no longer be representative of the population, causing the survey estimates to be biased. Nonresponse error has been plaguing survey methodologists in the global North, where nonresponse levels generally have been very high over the past decades Surveys in the global South have so far largely been spoiled with very high response rates, but nonresponse error is increasingly becoming an issue, especially in urban areas, where availability and willingness to participate in surveys is decreasing. While various approaches have been developed that aim to correct for nonresponse error, the best way to deal with it is to put solid measures in place to avoid it as much as possible. QUOTESNEEDED. Examples of nonresponse error are: Survey teams visiting communities during working/market hours and replacing unavailable households without making sufficient revisiting attempts on different days and hours. In a survey in urban South Africa, many interviewers were scared of dogs and replaced households if they owned a dog. Dog ownership was (initially) not observed and highly correlated with other household characteristics. The below average probability of Donald Trump supporters to respond to interview requests is thought to be the main reason that surveys systematically underestimated the support for Donald Trump ahead of the 2020 US presidential elections. In a COVID response phone survey, interviewers were unable to call those panel respondents who had not paid their phone bills and had their line (temporarily) cut off. Long questionnaires that cause respondent fatigue and lead to high rates of uncomplete interviews, affecting the last sections of the questionnaire. High Dont Know (DK) rate for income related question if interviewers do not probe and explain the question sufficiently. Respondents refusing to participate in the survey. This is happening increasingly often in some context, such as urban areas. Interviewers can be trained in best practices to convince respondents to participate. 2.1.5 Measurement error Measurement error often is one of the most damaging error sources in a survey. It occurs if the recorded value is an inaccurate measure of what was to be measured. They can be due to the interviewer, the respondent, the questionnaire, protocols or their interaction. They might be intentional or unintentional. Respondents might misinterpret a question, struggle to recall some information or deliberately give a wrong response. Interviewers might administer a question incorrectly, misunderstand questions or answer option, record typos, falsify responses or cause measurement error in many other ways. Poorly designed questionnaires, ambiguous questions, underdefined concepts often are big contributing factors to measurement error, as can be the interviewing mode. Some interviewers in an LSMS survey confused KG and Gram and selected the wrong answer option in the consumption unit. As a result, the food quantities they recorded were off by a factor of 1000. Respondents who purposefully under-report household members, crops, livestock or other items, knowing that each item they report will entail a long set of follow-up questions. Interviewers recording responses against the wrong questions or for wrong items due to confusing CAPI design that fails to provide them with a good overview of the questionnaire. A school survey conducted teacher interviews and recorded teacher attendance in classroom. Reason for absence was not recorded and no protocol was put in place to ensure both exercise were conducted during different hours. In a follow-up survey that corrected for both problems, 8% of the teachers were absent from classroom for being interviewed at the time. Agricultural surveys trying to capture very detailed information on the parcel-crop-season level can be too detailed for respondents to recall, or for interviewer and respondents to get lost in the conversation. 2.1.6 Processing error Processing error refers to any error occurring post-interview including error in data entry, editing, formatting and labeling, construction of indicators, calculation of survey weights, or tabulation of results. It is often caused by unclear interview rejection mechanisms, data editors who are not qualified enough, improper keeping of records or change logs. Examples of processing error we have observed are: Interviewers fixing issues in rejected interview files without recontacting the respondent or obtaining the correct answer. Wrong or non-systematic outlier detection, such as manually summarizing variables in statistical software and looking at the 5 highest and lowest values only. A NSO replaced the outlier values in any variable with its median value prior to data publication. Not correcting for changes to the instrument when appending the data from different versions. For example, f there has been a change to answer options or item lists, this can cause value labels to be assigned wrongly for parts of the sample. Wrongly label value codes, such as household assets. 2.2 Survey-related Effects Conducting a survey in the way it is being conducted, in its context, has itself effects on survey statistics. While these effects are often implicitly included in TSE and referred to as error, Weisberg (2005) uses the term survey-related effects and explicitly includes them into the framework. Examples of survey-related effects are: Interviewer effect: Interviewer characteristics such as mannerism, tone of voice, gender, ethnicity, etc. have shown to influence the type of response they elicit. This effect on survey statistics is not due to any mistakes or wrongdoing by the interviewers, and will exist for any survey using interviewers. Mode effect: Administering a question in face-to-face interviews can produce very different response patterns compared to asking the same question in telephone interviews or self-administered interviews. This, for example, can be due to respondents attitudes being different if they receive an interviewer at home (guest) to being called by phone (solicitor/marketer). In multi-mode surveys or with modality changing between rounds, mode effect can cause significant comparability issues. Questionnaire-related effects: Answers patterns can differ with the exact question phrasing. As long as there is no specification error, there is no question phrasing that is more correct than others. Similarly, the order in which questions are asked can change outcomes. Yet, they have to be aske in some order. The same is true for the phrasing and order of answer options. A well established example is the recency effect in phone surveys, where respondents select the last answer option because they remember it best. While some of the survey-related effects can be minimized, e.g. through randomization of question or answer option order or careful field team planning, they cannot ever be eliminated completely as they are an inevitable product of a survey itself. For example, since a survey requires interviews to be conducted somehow, mode effect will always exist in a survey. In some cases, it is possible to study and quantify effects in experimental design and to make ex-post adjustments. This, however is costly and requires provisions in the survey design. In practice, usually the best way to deal with survey-related effects is to be aware of them and keep them in mind during survey design and analysis. 2.3 The Impact of Error Sample surveys aggregate individual responses to obtain statistics for the sample, referred to as survey estimator, often with the aim to infer corresponding values in the population, referred to as population parameters. Survey error causes the survey estimate to deviate from the population parameter, diminishing the accuracy of the inferences derived from the survey data. In other words, with high levels of error a survey does not accurately describe the reality of the population. Error can affect survey estimators by increasing the variance of a variable or by introducing bias. For a survey estimator to be accurate it has to have a small bias and variance, which only happens if the TSE is low for the estimate. The ways in which error affects an estimate depend on whether the error is random or systematic, and whether it is uncorrelated or correlated. Error is considered random if it does not follow any pattern. For example, if respondents have difficulty recalling an item, some respondents might give higher values while others provide lower values. Across all observations, the error would have a mean of zero and would not affect the mean value of the variable. However, random error increases the variance of a variable, which in turn reduces the reliability of a survey estimator. The higher the variance, the higher the probability that the estimator would be different if the survey were to be repeated under the exact same conditions. The results would be less reliable. Increased variance furthermore reduces the magnitude of correlations with other variables and the statistical power in hypothesis tests. If error contains a systematic tendency we speak of systematic error or bias. As an example, if interviewers tend to under-report household members to shorten interview duration, the survey estimate of household size will underestimate the actual household size in the population. In statistical terms, the sample mean would be a negatively biased estimator of the population mean. Since systematic error directly affects the mean of a variable, it reduces the validity of the estimator, in other words the estimator is not accurately measuring what it is supposed to. If the systematic error is not constant, it may furthermore increase the variance of the variable, also reducing the reliability of the estimator. Figure 2.2 illustrates how variance and bias relate to the reliability and validity of a survey estimator. Imagine the midpoint of each image to represent the true population value and the black points to be the recorded responses. The points in A are scattered around the midpoint with no particular pattern, the error is random. We have high variance but no bias. The estimator is valid but not reliable. In B, the answers are offset in the same direction and by the same distance. The error is systematic, so there is bias, but variance is small. The estimator is reliable, but not valid. C shows systematic error that is not constant. We have high variance and bias. The estimator is neither valid nor reliable. In D, there is little error, we have low variance as answers are relatively close to the actual value, and there is no bias as the deviations show no pattern. The estimate is valid and reliable. Figure 2.2: Reliability and validity in survey estimators The magnitude of the effect of error depends on whether the error is uncorrelated or correlated. Error is uncorrelated if the error for different units (e.g. households) is unrelated. An example would be occasionally typos by interviewer when recording answers, if typos do not occur more frequently with certain types of households or some interviewers. Above discussion on the effects of random and systematic error was based on uncorrelated error. As we saw, the increased variance and bias can have serious effects on statistics. If the error for different units is related, we speak of correlated error. For example, interviews conducted by one interviewer who has misunderstood a question and administered it wrongly will contain the same error, while those of another interviewer may not. Correlated error has much more damaging effects on estimators as it multiplies the variance of a variable. Biemer and Lyberg (2003) show that only a moderate intra-interviewer correlation of \\(\\rho_{int} = 0.03\\), can result in an interviewer design effect \\(\\mathit{deff_{int}}\\) as large as 2.47. In other words, the intra-interviewer correlated error alone can cause the variance of a variable to be increased by 1.5 times. Error is typically also correlated for other survey roles, such as supervisors, data monitors or editors. One data monitor, for example, might thoroughly review interviews and provide useful feedback , reducing mistakes and measurement in their field teams while another data monitor only glancing over interviews will miss many of the issues and fail to reduce measurement error. 2.4 Minimizing TSE The TSE framework implies that the TSE affecting a survey estimate should be minimized in order to maximize its accuracy. Two points are important to note here. First, the total survey error needs to be minimized, that is the aggregate of all error sources. Errors from different sources do not occur in isolation, but are interdependent as they are all part of the overall survey process. Minimizing error from one source may negatively affect error from other sources. As an example, while increasing the sample size will reduce sampling error, it may not be a good overall strategy to increase the precision of a survey estimator. A larger sample can require more interviewers to be trained and monitored, increasing other non-sampling error types that can outweigh the gains from reduced sampling error. Another example can be complex parts of a questionnaire, such as a time-use module, that have been optimized for PAPI, but may cause significant error if implemented in CAPI without being carefully adapted to the functionality of the CAPI package being used. Understanding how decisions in one phase affect error in another requires a comprehensive view of the entire survey process. In order to effectively minimize TSE, different survey phases and components have to be integrated to operate as as a coherent whole. The second point to note is that TSE is to be minimized, meaning to be reduced as much as possible within survey constraints. All surveys face constraints such as the available budget, time and human resources or ethical considerations. Attempting to eliminate all error would exceed the constraints of any survey. Even with unlimited resources, some error sources could never be eliminated fully, such as respondents unwillingness to disclose some information causing item non-response. Instead, surveys should strive to avoid the most damaging errors and control others to the extent that they are mostly inconsequential and tolerable. How can survey practitioners prioritize which errors to address? A key challenge is that it is very hard to quantify survey error or its non-sampling components. This is only possible if the underlying true population value is known or if methodological experiments can be built into the survey design, which is an option for most surveys. In practice, minimizing TSE requires a detailed understanding of the main causes of survey error, their relative impact on accuracy, means to control them and the required effort. This knowledge is often anecdotal and based on experience in survey implementing institutions or individuals who have honed their error awareness, identification and correction over time. While there are some scientific publications on this topic, they tend to be theoretical or isolate and a address a single error source. Survey practitioners involved in the day-to-day implementation tend to not publish their practices. References "],["tsq.html", "Chapter 3 Total Survey Quality", " Chapter 3 Total Survey Quality As we have seen in Chapter 2, low levels of TSE are necessary for a survey estimator to be accurate. While accuracy is necessary for quality, it is not sufficient. Survey data must also respond to user needs in order to yield useful results. Data users often take data accuracy as given, assuming that it has been controlled by data producers. Instead they prioritize properties such as data being rich in detail, easily accessible and well documented. Data that is not sufficiently detailed, difficult to access or hard to interpret may be unfit for use and be perceived to have low quality by data users, even if being accurate. The concept of total survey quality (TSQ) considers the fitness of use of a survey estimate. It combines the accuracy of an estimate with non-statistical dimensions oriented towards the data user. The definitions of survey quality used by most NSOs in Europe, North America and Oceania, as well as that of international organisations such as Eurostat, IMF, and OECD explicitly acknowledge the multi-dimensional nature of survey quality. These definitions are referred to as survey quality frameworks. There is some (often minor) variation between organisations, but most frameworks include a subset of the quality dimensions summarized in Table 3.1 (Biemer 2010). Table 3.1: Common Dimensions of a Survey Quality Framework. Dimension Description Accuracy Total survey error is minimized Credibility Data are considered trustworthy by the survey community Comparability Demographic, spatial, and temporal comparisons are valid Usability/Interpretability Documentation is clear and metadata are well-managed Relevance Data satisfy users needs Accessibility Access to the data is user friendly Timeliness/Punctuality Data deliveries adhere to schedules Completeness Data are rich enough to satisfy the analysis objectives without undue burden on respondents Coherence Estimates from different sources can be reliably combined These frameworks are commonly used as the basis of survey quality reporting. Some of the dimensions are qualitative, making it difficult to generate a single metric to summarize the overall quality of a survey. Survey methodologists have not (yet) put forward a standard quality measure. Instead, evaluations tend to identify the strength and weaknesses of a survey by dimension and assess how well it achieved the goals of each dimension. Another very useful application of survey quality frameworks is as a design principle to maximize the TSQ in a survey. References "],["introduction-to-inception.html", "Introduction to Inception", " Introduction to Inception make a good plan, and adjust throughout the project. work in progress optimal survey design, reduces error make a plan to survey quality maximization, e.g minimize TSE while keeping the other dimensions acceptable, Biemer 2010 less can be more. A reduced questionnaire doing the key variable sof interest well can e better than trying to do many and ending up doing them worse. Fewer things to train, understand and monitor. Likewise, a smaller sample with a higher response rate might be more valuable. if there are too few resources available to do the survey well, try to either obtain more or question the feasibility of the survey. plan holistically. Efficiently allocating resources to maximize survey quality requires to look at the big picture and treat each activity as part of the whole that they are as each activity affects others. Some activities might be cut that have little or no effect on survey quality, to save resources for others where it matters. E.g. to afford a longer interviewer training one might reduce field team size and extend the duration of field work, if there are no time constraints or seasonality issues. good plan: to achieve timeliness. Overlap activities that can be done together. For example, with CAPi survey and a fixed data output format you can start working on the data documentation, cleaning, or even to some extend the analysis while the survey is still in the field. Do not overlap activities that depend on each other. For example, translation of instruments should only start once the instrument and CAPI coding have been finalised. Keepign track of change in overlapping activities is cumbersome, and leads in most cases to mistakes and survey erros. make gantt of key activities and draft timeline list of depending activities, e.g. only do training after instrument has been finished. Draw upon the experiences from other surveys in the same context, subject or modality. While every survey tends to be a different, the effect can often be generalised. In other words, what worked in one survey, often works in another survey with similar parameters. Make sure to have enough survey expereince in your team! "],["schedule.html", "Chapter 4 Schedule", " Chapter 4 Schedule Make sure your schedule keeps into consideration and works around the following forseeable events: Public holidays and vacations that might limit field workers or respondents availability or affect results, e.g. increased household expenses in the week of Christmas or Eid. Seasonality effects, that need to be catered for, such as such as harvest periods or that might affect your results. Weather patters such as rainy seasons that might limit field work operations. Allow margin to cater for unforseeable events that might affect your survey, such as abnormal weather events, unexpected field worker attrition, strikes or demonstrations "],["formulation.html", "Chapter 5 Formulation", " Chapter 5 Formulation work in progress "],["personel.html", "Chapter 6 Personel", " Chapter 6 Personel work in progress remove field team effects. As shown in Chapter LINK INTERVIEWER, interviewers, supervisors, data monitors or any other level can have huge effects on the TSE. As an example, interviewers of one supervisor who thoroughly back-checks interviews with respondents will be careful to not under report household members, parcels etc, while those of a more lenient supervisor may notice that they can get away with under reporting. that is centraize processes, so they can e monitored and streamlined, to remove supervisor, or data editor effects. Examples are: - instead of reviewing interview files in field y each supervisor, let them do only the basic completion checks, and do in depth review and feedback of the interviewed by a central team of editors, that regularly debrief, receie rotate staff, so that not always the same person enters data , reviews interviews, provides feedback to one interviewer only. review the work of each data monitor, editor, superviisor, for data entry, rotate data entry staff "],["contracting-considerations.html", "Chapter 7 Contracting considerations", " Chapter 7 Contracting considerations work in progress "],["in-field-sampling-for-design.html", "Chapter 8 in field sampling (For design)", " Chapter 8 in field sampling (For design) if in-field sampling, has to be traceable, and recreateble, e.g. write numebrs in book, use kish tables, send data to HQ if there is internet remove all possibility of interviewers to ahve an influence over selection. "],["introduction-to-preparation.html", "Introduction to preparation", " Introduction to preparation work in progress "],["fieldworker-recruitment.html", "Chapter 9 Fieldworker recruitment", " Chapter 9 Fieldworker recruitment general rule, smaller teams better, easier to keep track off, higher interviewer effect, so need to streamline and monitor. special roles: for separate tasks, split roles. for example, dedicated anthropomterics staff, less content for all trainees, can specialize and get good at it, more time to train and learn properly requires parallel training. check DHS, quite comprehensive. for supervisor, data monitors, if new team, recommend to identify suitable candidates during the training when abilities and personality become clearer. Beware of experienced trainees coming from other surveys. Standards might have been lower in their previous training and they may have developed some undesirable habits that you need to untrain. Also, they might have a stronger sense of I already know that and engage less. Most surveys are significantly different for even experienced interviewers to learn the concepts and protocols. MICS:  Recruitment/Selection of pool of field staff It is key to identify individuals to invite to the training. The practise of recruitment or selection of staff to invite for training differ from country to country and from survey to survey. In some situations, the implementing agency can decide to recruit a brand-new set of staff and in others, select from an existing pool of staff, either from a previous survey or actual staff on payroll. Here follows some overall recommendations to select the adequate mixture of participants and screen individuals for the training:  Gender: The protocol demands that the individual questionnaires are administered by an Interviewer of the same sex as the respondent. Depending on the sample of individual men (all households, half, one third, etc.), one or more male Interviewers will be required on each team. If the survey does not include an Individual Questionnaire for Men, then all the Interviewers must be women. However, since it is also recommended, based on experience, that all teams include members of both sexes, it is advised that, the Supervisor or the Measurer is male (if no male interviewers). It is important that all interviews can be observed by the Supervisor, but Supervisors should be advised to leave an individual interview during the most sensitive subjects, such as sexual behaviour and victimisation or other questions where an observer of the opposite sex may make the respondent uncomfortable.  Education: Normally, secondary education of trainees is a good target to bear in mind when recruiting. There are mixed experiences with university graduates: Recent graduates are often highly motivated and can be excellent Interviewers, but in other cases graduates have proven problematic by developing their own protocols in the field (being too smart). In any case, monitor performance carefully for all field staff.  Experience: Having worked in the field on other surveys is certainly helpful for Supervisors and can be equally so for other staff. However, please be careful, as other surveys may not pay as much attention to quality as the MICS or may have instituted protocols that are not recommended for MICS and are difficult to unlearn. A typical example is for Interviewers that have worked on market research that is not always conducted according to the standards of a national statistical office or MICS.  Language: All field staff must be completely fluent in the language(s) used for the training, which is typically also the language of the survey documents, such as the instructions and manuals. When deciding on the number of Interviewers fluent in other languages, it is important to have the fieldwork plan in mind, as well as a complete understanding of languages necessary in the different parts of the country. For example, if the fieldwork plan requires that just one team is fluent in a particular language, it is important that more trainees are invited than needed with this particular language skill. The rule of 10% is a rule of thumb that applies to total number of trainees, whereas it may be appropriate to invite 1-2 extra with a special language skill.  Appearance: Fieldwork is demanding, not just on physical fitness, but also, for some, on the ability to dress appropriately.  Attitude: A good candidate will show a respectful attitude and maturity and take interest in the work.  Diversity: With the demands for languages above, there is a good chance that recruiting happens across the country. However, for various reasons, there may be a natural bias towards candidates from the capital city or other major urban areas. It is important to ensure that the opportunity to apply is given across the country, perhaps through advertising that covers all regions. Even if the language requirements can be met in the capital, some candidates tend to appear so sophisticated to a rural population that a good rapport can never be established.  Avoid: It is risky to use staff currently employed for example in the health sector, both because of the issues mentioned under education and experience above and because a large part of the survey is measuring performance of the health sector and thus there is a potential conflict of interest. An objective set of requirements, based on the above, should always be developed and be transparent applied, so that the pressure the survey managers may feel to hire certain individuals can be eliminated. It is equally important that all applicants that meet the requirements are interviewed and tested as part of the selection process. This takes a lot of work and therefore planning far in advance is necessary. Very simple testing can be applied, i.e. if the candidate reads well, writes correct answers to simple questions, and can communicate in whatever languages are necessary and indicated by the applicant. There is a further need to at least check if candidates can operate simple functions in a tablet computer. Advanced computer literacy is not necessary, but a certain comfort with computers or smart phones is valuable. Additionally, measures must be able to see well (with glasses if used) as they will be reading out measurements that may be unclear in certain lighting. "],["fieldworker-terms.html", "Chapter 10 Fieldworker terms", " Chapter 10 Fieldworker terms make it fair: pay fair. Mix of daily and piece rate. Only use peice rate if control meachnisms in place to counter act effects, only use daily rate if sufficent control that do sufficnet work. due diligence: insurance for health and accidents give incentives to do well, e.g. bonus at the end if still there (to combat attrition) and based on performance put in deterrents to bad behaviour, make sure you can fire if gross mis conduct, not hire again write up short terms and conditions, including pay, DSA allowances, contract, duration, selection process, possibility of not getting selected, required tasks, duration fo field work and places of work. Make every candidate read the terms and conditions. If they agree, they should sign a copy. This is important that expectations of all trainees are correct at the beginning of the training, so trainee attricition is reduced, and to avoid collective walkouts by trainees. "],["introduction.html", "Introduction", " Introduction work in progress "],["instruments.html", "Chapter 11 Instruments 11.1 Questionnaire 11.2 CAPI/CATI 11.3 Translation", " Chapter 11 Instruments 11.1 Questionnaire discourage Dont Know responses. Dont make them very obvious by adding them to all questions. They are a tempting solution for iinterviewers. There are different types of Dont know, often they are related to respondents not makeing an effort to recall something. Intervieers should be encouraged to get respondents to answer. Only add to questions where you accept them. Instead use special codes that are displayed once at eginning of the questionanire and trained to intervieiwers. 11.2 CAPI/CATI 11.3 Translation Many questionnaires are designed in one language but fielded in one or more other languages. The way questionnaires are translated affects results, see e.g. Seo, Chung, and Shumway (2014). Interviewers speak the language and just translate on-the-go. is a big source of measurement error and interviewer effects, so make sure to translate your instruments! Using CAPI/CATI makes it easy to provide the questionnaire to interviewers in different languages. One can normally switch language within the questionnaire. If managed well, even translating to multiple languages is neither too much effort nor very costly, and a low-hanging fruit to increase survey quality. Translations done badly can quickly spiral out of control, causing tremendous amounts of work and potential mistakes. 11.3.1 What should be translated? Into which languages should I translate? Translate into all languages in which a significant proportion of the sample will be interviewed. The size of the proportion depends on the context and available resources, but can be as low as 5%. Only translate into languages that can be read fluently by interviewers. Some local languages are spoken only or have no spelling convention, making them very hard to read. Do I have to translate the whole questionnaire? Always translate question text, answer options and instructions to the respondents, as they are being read out to the respondent, or help the interviewers record their response. Translate interviewing-facing parts such as interviewer questions, instructions or warning messages if the field teams are more comfortable using a language over the design language. If they are fluent in the design language, there are only marginal benefits. Keep questionnaire structure such as names for screens, sections, rosters or the outcome variable in the design language or translate to a common language. Having a common reference makes training, management and feedback easier as everyone is literally on the same page (e.g. Parcel Listing). Add interviewer variables at the end of the instrument to record the main language in which the interview was conducted, if an interpreter was used and what the level of understanding was. 11.3.2 Who should translate? There are a few professional firms and translators that specialize in survey translations and cover a range of common languages. For many local languages these are unfortunately not an option. A sound translation of survey instruments requires translators to: be fluent in the design/source language be native speaker in the target language have survey field experience (to know survey expressions and ways to phrase questions) have good understanding of the subject (e.g. WASH, health, education) local contextual knowledge (to know how things are referred to locally) This set of skills and experience is rarely combined in one person. Translations from professional translators can sound too formal or bulky, while translations from field staff might miss the essence of a question or a construct. In the absence of professional survey translation services, the best options are often local consultants or experienced field workers who have worked with similar types of surveys before. Ideally, translation is done by a group that combines the above listed experiences and receives input where they lack, e.g. to correctly translate main toilet types or drinking water sources. It is useful for questionnaire designers to work through the instruments with the translators to make sure complex or nuanced parts and constructs are understood. A common communication channel (e.g. Email to all, WhatsApp group,) for translators to ask questions and for designers to send clarifications to all translators is important to ensure consistent translations into multiple target languages. 11.3.3 Some translation guidelines Follow below guidelines when translating survey questionnaires. Share this list with the translation team. Preserve meaning. Literal or close translations are often inadequate, especially if the target and source language and culture are distant. Find the best way to express the same meaning in the target language. Be precise. Use expressions to describe a construct if there is no word for it in the target language. E.g. use those who live with you instead of family if there is no word for household Dont omit words that provide any type of reference, such as on average,in total, main. Keep it simple. Use language and expressions that are easy to understand for all respondents in the sample. Make sure the same parts of questions are stressed. E.g. in English, the reference period is put at the beginning of a sentence to highlight it. In other languages this might be elsewhere. Adjust to different customs and culture. Direct translations might sound overly positive, negative, polite, impolite, etc. E.g. you might have to drop or add the word please. Be consistent within and across instruments. Use the same words or expressions to translate one concept in the source language. Check consistency by searching for key expressions in the translation sheet (e.g. Ctrl+f household) and checking they have been translated the same. Use established (good) surveys as references. They have already been tested and fielded in your context and often are a useful source of nomenclature. Preserve formatting, it carries meaning. Whatever is bold, underlined or UPPERCASE in the source language should be the same in the target languages. Preserve code. CAPI/CATI text sometimes contains dynamic parts that are written in code and must not be translated, e.g. piped text in Survey Solutions %MEMBER% or &lt;html&gt; tags. Use the custom Stata command sursol transcheck to make sure all Survey Solution formatting and code is correct in the target languages. Stay local. More widely spoken languages such as Spanish or Arabic can differ significantly between countries in how they call or express certain things. Use local translators, and review and adapt if using (parts of) questionnaires from other countries. Use the CAPI/CATI or a paper questionnaire in the source language on the side as guidance. Translations will depend on the context that is often not given when working in translation sheets. Give feedback if you think the source language needs updating and get clarifications if you are not sure how to interpret something. For more practical translation guidelines, see Chapter 12 of the ESS Translation Guidelines (European Social Survey 2018). 11.3.4 Translation verification Verify translations of your instrument. Despite being much-cited and persistently being added to ToRs, back-translations are not a great means to verify translations. Most big organizations such as the European Social Survey (2018) or the US Census Bureau (Pan and Puente 2005) that run multilingual surveys and put a lot of effort into comparative survey design use and recommend the TRAPD method (Translation, Review, Adjudication, Pretesting and Documentation). It involves reviewing and comparing (at least) two independent draft translations and agreeing on one translation that is cognitively pre-tested. Insights from the pre-test feed back to the translation, and the whole process is documented. Translation, review and adjudication are normally done together by a team that combines translators, survey field experts and content subject experts. A team-based approach deals better with mistakes, idiosyncratic interpretations and translator blind spots, compared to individual translators working on their own. Surveys with tight budgets and time constraints can use a reduced version of TRAPD to verify translation. Save yourself the back translation. Instead, use independent translator(s) to systematically review all rows in your translation sheet. The reviewer(s) either approve a translation or suggest an alternative translation in a new column. Translators and reviewers discuss all discrepancies and choose a final translation. Using a translation management system as described further below, translation and review processes can overlap and do not necessarily add much time to survey preparations. Make sure to desk test your translation in the CAPI/CATI before the pre-test. Often, the context, the way questions are displayed in the interface or the sequence require translations to be updated. As a last defense, verify the translation during the training. During the questionnaire review part, project the questionnaire and read out loud each question in the design/common language. Trainees follow on their devices in the local languages they are fluent in and are likely to use in the field. For each question stress the meaning of the question and check with the trainees if the translation is accurate. Assign one person to make updates to the translation sheet on the go based on feedback from the trainees. Keep note of contentious items and address them with selected trainees on the side or after the session in order to not delay the training. 11.3.5 When to translate Translate a questionnaire too early, and juggling the inevitable many updates can quickly overtake your work and introduce mistakes. Translate too late and the instruments are not verified, poorly translated or simply not ready in the local languages at the time of the pre-test, destroying one of its main purposes. In the preparation phase, carve out enough time prior to the pre-test for translation and verification. Try to complete the questionnaire before that, so that it only undergoes minimal change once translations have started. Translating a normal socio-economic questionnaire can take several days, and so does the verification process. If there is little time, you can use a staggered approach to provide enough time for translation. Translators can already translate and verify completed sections while the remaining sections are being finalized or put into CAPI/CATI. Use a translation management system as described below to stay on top of the updates and avoid mistakes. If you are using a paper version to develop the questionnaire, do not directly translate the paper version, but wait for the electronic questionnaire to be developed. When building the CAPI/CATI tool, questions often have to be modified to function in the interface, especially around roster and dynamic question text. If already translated, these changes have to be made in many versions, and often in languages the questionnaire designer does not speak, making updates difficult and error prone (e.g. when trying to find the word NAME in local languages to replace it with dynamic text). It is a lot easier, faster and less error prone to translate and manage updates once the tool has been converted to CAPI/CATI. If you require translated paper versions e.g. for training or archiving purposes, create them using your translated CAPI instrument. Often this can happen during or after field work, once the questionnaire does not change any longer, and during less work-intense parts of the project. 11.3.6 Stay on top of updates Lets face it. Most questionnaires will still undergo some change after the translation process has started. Done wrongly, late updates to the questionnaire can easily turn into a managing nightmare and cause significant undetected differences between the source and target languages. Done correctly, one can relatively easily stay on top of questionnaire modifications, even if there are a considerable amount of change and several target languages. Do not: Do not translate in the CAPI/CATI tool directly. They tend to hold only one additional text field for each language and have no means to let you manage or document the translation and verification process. Do not translate or store translations directly in translation sheets exported from CAPI/CATI, as they will be outdated with the next update to the questionnaires. You will inevitably end up trying to juggle different parts of the questionnaires on different translations spreadsheets for different languages. Do not work with offline versions of translation sheets sent back and forth by email. As above, this quickly becomes unmanageable. Instead: Work with an online spreadsheet such as Excel or Google sheets that holds all the translations and allows questionnaire designers and translators to simultaneously collaborate on the same live document. Define clear processes, how questionnaire designers mark rows that need to be translated, reviewed or updated, and how translators record the translation status of each row. Make sure designers and translators follow the processes to make the system work. Examples are: Add columns status (fixed set of answers, e.g. to translate, to review, to update, translated, reviewed, dont translate) and comments. Designers change the column status for rows they added or updated in the source language and provide details in comments if necessary. The first translator modifies the target language and sets status to translated, the reviewer to reviewed. At the end of the process, all rows should be in the final status reviewed. Designers add a special symbol that does not occur in the questionnaire or code (e.g. @) to the beginning of the target language column for rows that need to be translated, updated, or reviewed. Translators remove the symbol after updating the target text. Use conditional colour coding to highlight rows of certain status, and filtered views to generate custom views, e.g. a view for translators containing all rows they need to work on. Updating translations If you are working with Survey Solutions, use the translate_questionnaires STATA tool to keep your translation sheet in sync with the questionnaires and to produce the translation templates to be uploaded to the Survey Solutions Designer. The tool can be adapted relatively easily to work with other CAPI/CATI packages that export and import tabular translation sheets. For CAPI/CATI software such as ODK or Kobo that use spreadsheets to build the instruments, one can integrate the translation process into the same spreadsheet used to build the instrument by adding additional columns. Use an online spreadsheet to allow simultaneous access to designers and translators, and define processes as described above. 11.3.7 On-the-fly translation As described above, translate into as many languages as possible and try to avoid on-the-fly translations as much as possible. Sometimes however, on-the-fly translations are inevitable, e.g. if local languages cover small parts of the sample population or are non-scripted. If this is the case, make sure that interviewers have practiced translating the questionnaire in front of others prior to the field, and that there has been a group consensus on how to translate the questionnaire into the local languages. Both significantly improve the quality of the translations and can be implemented using group practice during the training. For group practices, group together trainees according to the local languages they are likely to use in the field. One trainee playing interviewer administers a question in the local language, those playing respondents follow on their tablets in the source language and make corrections or suggestions on how to improve the translation. Trainees should rotate roles such that every trainee has practiced translating the entire questionnaire to others speaking the same language. If you will have to work with local interpreters to translate interviews on-the-fly, make sure that interviewers explain well the key concepts such as household or parcel, and that they are understood by the translator. Make sure that you have budgeted for local interpreters if they are likely to be needed. References "],["sample.html", "Chapter 12 Sample", " Chapter 12 Sample work in progress "],["manual.html", "Chapter 13 Manual", " Chapter 13 Manual work in progress "],["pilot.html", "Chapter 14 Pilot", " Chapter 14 Pilot "],["introduction-to-training.html", "Introduction to Training", " Introduction to Training Fieldwork training is arguably one of the most important activities of any survey and has one of the most substantial impacts on data quality. It serves several purposes: Build fieldworker capability. In-depth content training, extensive practicing and capability-based selection ensure that fieldworkers are capable of implementing the survey as designed. This is key to limit measurement and nonresponse error, as well as coverage and sampling error if the survey includes listing and in-field sampling. Standardize fieldworker behavior. Repeated practicing under supervision and continuous feedback streamlines how fieldworkers implement the survey. Reducing idiosyncratic behavior is important to mitigate the very damaging fieldworker effects. Scrutinize methodology. Particularly in surveys with limited pre-testing and piloting, fieldwork training often effectively is the most detailed review of the questionnaires, translations, CAPI and survey processes. Identifying and correcting mistakes is key to reducing specification error. Unfortunately, in many surveys fieldwork training is not implemented effectively to produce fieldworkers that are fully capable of implementing the survey, or is inefficient in being too demanding on trainers, survey budget and timeline. This part of the book gives recommendations on how to implement fieldwork training that effectively prepares field workers while not overburdening trainers or overstretching survey resources. It assumes fieldwork training to be in-person, as is custom for most socio-economic surveys with personal interviews. Chapter 15 Planning and Preparation is targeted at survey designers, managers, fieldwork managers, trainers or any other role that is involved in the planning, budgeting and preparation of the fieldwork training. It provides recommendations on the training location, timing, size, the trainers, training content and schedules, and includes checklists of the key items to prepare prior to the training. Chapter 16 Conducting the training gives useful recommendations on how an effective and efficient fieldwork training can be conducted. It provides extensive details and step-by-step guides for different training modules. It is mainly targeted towards trainers and is best during training preparation and the training itself. Chapter 17 Assess &amp; select fieldworkers describes how to implement a competitive skill-based fieldworker selection that is critical to achieve quality in surveys. The chapter is mainly targeted towards trainers and gives step-by-step guides on how to effectively implement written tests, systematically evaluate skills and select fieldworkers. Survey designers and managers may be interested in reading about the benefits of this approach that are summarized at the beginning of the chapter. TL;DR. Follow below key recommendations to ensure a successful fieldworker training. Train an excess of at least 20% of fieldworkers to allow for a meaningful competency-based fieldworker selection at the end of the training and to have a reserve of fieldworkers. If working with unknown teams, mainly new recruits, or expected high attrition, increase the excess to 40% to ensure capable fieldworkers. Assess trainees from the beginning. Use frequent written tests throughout the training to evaluate their understanding of the survey content or standardization tests to assess their ability of taking expert measurements. Observe and rate key skills in a structured way. Select fieldworkers based on capacity. Select interviewers, supervisors and other fieldwork roles towards the end of the training, based on capacity demonstrated during the training. Be careful when considering other criteria such as previous survey experience, as they may reduce the capacity level of the field teams. Dont cut on duration! Allow for at least 15 working days of fieldwork training for a LSMS-style survey with a 2-2.5 hrs questionnaire. Anything shorter is normally not enough for the complex content of a socio-economic questionnaire to be learned and sufficiently practiced. Keep it small. Try not to train more than 50 people at the same time. Learning outcomes quickly decrease with larger class sizes. If necessary, try to adjust the fieldwork plan such that fewer fieldworkers work over a longer period. Sufficient and experienced trainers. Delivering good training is more than a full-time job. Make sure there are enough trainers and that they have solid survey, context and subject experience. Sending a junior colleague without much experience to be in charge of the training is not a good idea. Be prepared. Make sure questionnaires, translations, CAPI and manuals are truly ready prior to the start of the training and have been thoroughly tested. Have the venue fully set up and all admin and logistics sorted. The quality of the training can suffer badly from insufficient preparation. Less instructional &amp; theoretical, more interactive &amp; hands-on. Avoid one-directional training methods where trainers give long presentations about theory and questionnaires. Instead, engage trainees as much as possible. Train using the manual, demonstrations, videos, real-world examples, etc. Practice, practice, practice. Frequent practice is crucial for trainees to fully internalize the content and be able to apply in the field. Do not leave (field) practice to the end. Practice in groups, front-of-class interviews and with respondents on-site or in the field throughout the training. Go beyond the questionnaire. Knowing the questionnaires alone does not make good interviewers. Trainees need to learn and practice a set of interviewing techniques such as correct probing or encouraging participation as well as pre- or post-interview tasks, such as respondent selection or responding to rejected interview files. Assess readiness for field. At the end of the training, a final field test should be conducted that mimics field conditions and where fieldworkers can be observed and feedback given without using the actual sample. If the team is not ready extend the training by a few days to revisit and strengthen problematic areas. Engage. As a survey designer or manager, do not sit back and leave it to the survey firm, fieldwork manager or trainers alone to deliver the training. Actively participate, check that the content is correctly understood, explain what matters, etc. You have put a lot of effort into the design. Make sure it is implemented correctly. "],["planning-and-preparation.html", "Chapter 15 Planning and preparation 15.1 Training size 15.2 Trainers 15.3 Content 15.4 Timing &amp; duration 15.5 Schedule 15.6 Location 15.7 Preparation", " Chapter 15 Planning and preparation The chapter presents recommendations for survey designers, survey managers and trainers on how to plan and prepare an effective and efficient fieldwork training. Fieldwork training should be planned well in advance and must be well prepared. They usually consist of a main interviewer training and separate training for supervisors, data monitors or other fieldworker roles. Survey designers, please take note of the sub-chapters on training size, trainers and Timing &amp; duration, as they are budget and timeline relevant. For survey managers and trainers all sub-chapters are relevant to plan and prepare a solid fieldworker training. 15.1 Training size It is strongly recommended to train more trainees than are ultimately needed as fieldworkers, i.e., as interviewers, supervisors, data monitors or other roles. This allows for a competency-based selection of fieldworkers at the end of the training. Training more people than needed also compensates any attrition of trainees during the training and builds a reserve of already trained persons that can be brought up to speed relatively quickly during fieldwork, should fieldworkers need to be replaced. For a meaningful selection process train at least 20% more fieldworkers than are required if you are working with experienced teams who you have worked with before and if you are expecting few trainees to drop out during the training. For example, train at least 48 people if you are aiming for 40 fieldworkers. Increase the excess to at least 40% if you are working with unknown trainees, a new implementing partner, in a new context or expect a high number of drop-outs during the training. To get 40 fieldworkers, this would require training at least 56 people. Training a smaller excess of trainees is not recommended as it prohibits a meaningful fieldworker selection process and increases the risk of having to rely on trainees that are not up to the task. Often, some trainees will drop out during the training, particularly after day 1 or 2 once there is a clearer understanding of the required effort. Mitigate early trainee attrition by keeping on standby some of the applicants that have not been invited to the training, so they can quickly replace any trainees that have not come back. You might have to bring them up to speed in extra hours and ask them to independently learn already covered material to catch up. It is not advised to bring in additional trainees after day 3, as they will have fallen too far behind. In some surveys, there have been walk-outs of trainees, often towards the end of the training, in which trainees collectively refuse to continue the training or fieldwork unless their demands are met. Usually, there is no time or budget to repeat the training with other trainees, leaving survey management in a very weak bargaining position and putting the survey itself at risk. Avoid walk-outs by setting fair fieldworker terms and conditions, making them clear to trainees prior to the training, and asking them to agree by signing a copy of the terms and conditions. In many circumstances it is beneficial to select supervisors, data monitors and any other fieldwork roles from the pool of trainees based on their performance and capabilities observed during the training (see Assessment &amp; Selection). If any of the fieldwork roles are predetermined, the designated persons should participate in the fieldwork training without being counted towards the excess of trainees (and demonstrate full understanding of the training material). Keep the number of trainees as small as possible, ideally below 50 trainees, which corresponds to around 36 fieldworkers. Learning outcomes quickly decrease with increasing class size, as each trainee receives less supervision and personal feedback and as it is more difficult for trainers to keep track of trainees performance. If possible, adjust the fieldwork plan, so that fewer fieldworkers work over a longer period of time and fewer fieldworkers need to be trained. See Chapter Fieldworker Recruitment for other benefits of having a smaller field team. If it is unavoidable to train more than 50 trainees (e.g., surveys in large country or with large sample) or a centralized training is not possible (e.g., different languages or geographically dispersed sample), it is best or can be imperative to split fieldworker training into separate rooms or even locations. In those instances it is paramount to standardize the training between different rooms/locations as much as possible in order to minimize the training effect. This requires the training to be largely guided by standardized and extremely well-prepared training materials, and effective communication between the training rooms/locations, so that all trainees benefit from relevant feedback, comments, questions, etc. raised in one room/location. On top of that, there must be sufficient capable trainers who have received a comprehensive Training of Trainers (ToT). The ToT can be a dedicated exercise or integrated as an explicit outcome of the pilot training. Several scenarios are possible if a large number of fieldworkers need to be trained: Break-out sessions. This is the first best option to train class sizes between 50-100 trainees if they can be trained centrally. Split the class into groups of manageable size around 15-20 trainees and assign dedicated trainers to each group. Train all modules that are sensitive to class size in the groups, including the questionnaire reading and practicing. Trainers must keep notes of any additional clarifications they provided, questions they received or feedback they gave. Hold daily plenary sessions with the entire class in which feedback is given based on the notes from individual groups. Modules that are less size-sensitive, such as the opening module or fieldwork logistics can be trained in the full class. Parallel training. If joint plenary session becomes unfeasible due to larger class size, the group of trainees must be split and trained either in parallel or consecutive training. If possible, try to avoid this by adjusting the fieldwork plan. In parallel training, groups are independently trained at the same location. Each group should have a resident trainer, who is responsible for the training in this group. Other trainers can rotate between groups, for example those responsible for training a certain module or task, bringing some consistency to the training. Due to groups and trainers not overlapping, it is harder to harmonize training between groups. It is therefore important that trainers debrief during the breaks and at the end of the day. Consecutive training. Different groups of interviewers are trained one after the other. A core set of trainers repeats the same training for each group, providing consistency between the groups. Since each group should start fieldwork immediately after their training, this approach only works if a staggered field start is possible (e.g. if field work is sufficiently long). Additional trainers should attend the training for each group, so that they can supervise the field start of one group while the core trainers move on to train the next one. Parallel locations. It is not recommended to simultaneously train in multiple locations, as it is difficult to harmonize fieldwork training between different locations. If unavoidable, it is essential for trainers to keep good notes, frequently communicate and debrief with trainers from other locations. 15.2 Trainers The success of a fieldwork training hinges upon having enough and competent trainers. Conducting a fieldwork training is a substantial amount of work! Doing it well even more so. Prepare the training well to avoid significant reductions in quality or trainers getting overwhelmed. Make provisions for enough trainers so they can conduct the training well. For every room, there needs to be a minimum of two active trainers at any point in time, one to lead the training and another one to support the class or do any of the supporting activities that need to happen in parallel, such as updating manuals, compiling feedback, creating practicing scenarios, etc. Larger classes require more trainers as supervision and supporting activities increase. Ideally, there should be one active trainer for every 15-20 trainees. Trainers must have the availability to actively engage in the training most of the time. Avoid the situation where trainers are physically present, but either are in meetings, focus on other tasks unrelated to the survey, or are otherwise disengaged from the training. Cater for more trainers if trainers have ongoing other responsibilities outside of the training (which is often the case). All trainers must have: A good understanding of the survey in general A detailed understanding of the questionnaires, definitions, protocols, manual, CAPI, etc. Ideally, they have been involved in the survey design process, participated in the pre-test or pilot and will be involved in managing and monitoring fieldwork. A general understanding of the survey subject, e.g., the regional agricultural systems for an agricultural survey. Good facilitation skills and the ability to lead and manage all components of the training. Collectively, the trainers of one room should also have: A good understanding of the indicators construction, the intended data use or analysis. Subject and context expertise, e.g., know in detail the local circumstances of child education, nutrition and health for a survey on early child development. Experience in implementing the survey type and implementing surveys in the context Experience in conducting fieldworker training Speak the trainees language. Do not train in a language the trainees are not fluent in. If some trainers (usually the survey designers or analysts) do not speak the training language, use an interpreter or ask other trainers to translate for them. In many instances, particularly in surveys with limited pre-testing and piloting, mismatches between survey design and field reality are uncovered during the training and adjustments need to be made quickly. For example, a locally practiced exchange labor system might not be captured accurately in the questionnaire. Trainers need the above expertise and experience to be able to identify those issues and take decisions that are analytically correct, in-line with the survey design and make sense in the local context. There is a tendency to send junior analysts or PhD students without sufficient survey or context experience to conduct the interviewer training. This can work, as long as other experienced trainers are present, but quickly leads to quality issues if they are the main training lead. Similarly, handing over a sample and questionnaire to a survey firm and letting them implement the training without monitoring them closely is usually a bad idea. Firms commonly lack an analytical background, do not know about the details of the survey design, are differently motivated, and might not have the capacity to conduct the training to the required standard. If working with a new firm, it is recommended to send your own trainers to be present at the training, support if necessary and ensure the training meets the quality standards described here. Some surveys advocate for subject experts to facilitate some of the modules. Unfortunately, they often have their own agenda and priorities and come with a varying capacity to conduct effective fieldworker training, making it sometimes a waste of time (e.g., long, theoretical presentations on details that are largely irrelevant for interviewers). If external subject experts will lead parts of your training, make sure they fully understand the relevant parts of the questionnaire and what knowledge they need to transmit to trainees. Prior to the training, discuss the scope of their engagement and review their contributions, such as presentations. Subject experts can also be given a backstopping role to answer any questions on the subject and contribute to relevant discussions. 15.3 Content To equip trainees with all the knowledge and skills necessary to do a successful job as a fieldworker, training must go beyond a simple discussion of the questionnaire and should cover the following components. See Chapter Conducting the training for details on how to train each component. CAPI use. Interviewers must understand and be able to use all functionality of the CAPI software they need to use in the field, including handling the CAPI questionnaire, creating and sending interview files, etc. it is best to keep theory to the limit and introduce functionality throughout the training as needed, so trainees can practice it immediately. Questionnaire content. All questionnaires must be covered in detail, including question text, answer types and options, how to administer questions and sections, the routing/skipping behavior, and the underlying definitions and concepts. Practicing. Repeated practicing, in groups, with respondents, on-site and in the field is crucial for trainees to fully internalize the trained material, learn how to put it into practice and to get exposure to real respondents and scenarios during the training. Interviewing techniques. Trainees must learn and practice a range of skills that are critical to collect good data, such as how to introduce yourself, the survey and convince respondents to participate, how to probe, inquire, and get clarifications, how to control the interview. Expert measurements. Many surveys include exercises such as anthropometrics, plot measurement or capacity tests, that require interviews to use specialist equipment and strictly adhere to measurement procedures for comparable measurements. Trainees must practice until they can correctly implement the exercise and until behavior across the entire field team has been standardized. Pre-interview skills. Interviewers need to learn the format in which they will receive their assigned units (e.g., households), any selection protocols they might have to follow, how to identify and locate units, when to revisit, replacement protocols, how to manage their workload, etc. Post-interview skills. It is important for interviewers to know how to correctly perform a range of activities after an interview, including checking for completion, addressing inconsistencies, leaving comments, submitting files, receiving feedback, respond to inquiries on submitted interviews, making corrections, asking questions, etc. Final field test. At the end of the training, a final field test should be conducted to identify and address any outstanding issues with questionnaires or field procedures, expose interviewers to real field conditions and determine if they are ready for field work. Supervisor training: Supervisors must learn how to manage field teams, manage the relationship to the communities and respondents, and the tasks they are responsible for in the survey, such as handling money, making logistical arrangements, checking completions, communication to survey management or data monitors, etc. Data monitor training: Data monitors need to learn all the tools and background information necessary to be effective in their roles, including how to review interviews, conduct re-interviews, review audio recordings, how to interpret summarized information, how to provide effective feedback to fieldworkers, how to compile feedback for survey management and archiving. 15.4 Timing &amp; duration Fieldwork training should be held just prior to the start of fieldwork, with a maximum of 1-3 days between training and fieldwork to allow for administrative matters, a rest day and travel to the field. An immediate fieldwork start with strong supervision and feedback in the first few days is the best way to ensure the survey is being implemented as trained and that fieldworker behavior is streamlined. The longer the break, the higher is the risk of interviewers forgetting details, making mistakes and developing idiosyncratic behavior. If fieldwork is delayed by more than a few days, conduct a short refresher training just prior to the field start to revisit and practice key points of the trained material. For longer delays, a longer refresher training will be necessary. For breaks beyond a few weeks, often a complete retraining is the only solution, also because there tends to be (some) new trainees. Schedule enough time for fieldwork training. The optimal duration depends on a range of factors, including the nature of the questionnaire, tasks required by fieldworkers, level of trainees, etc. As a general rule of thumb, for a LSMS-style survey with a 2-2.5 hours questionnaire allow around 15 working days (2.5 weeks) for the main interviewer training, including practicing and final field tests. Training duration can vary to some degree with survey factors, but keep in mind that many components such as opening session, CAPI use, interviewing skills, final field tests need be trained independently, so even for a survey with a relatively short 45-minutes questionnaire, at least 6 working days (1 week) of interviewing training should be scheduled. To some readers, this might sound excessively long. Many survey firms, especially with a background in opinion surveys, will suggest a much shorter training duration. However, in socio economic surveys, due to the substantial amount of content that fieldworkers need to learn and be able to apply, it is usually impossible to train the field teams to an acceptable level in a shorter amount of time. Do not save on training duration, unless you are very confident that all trainees will have fully internalized all material and will be able to perform well as field workers! Be prepared for possible extensions. In many surveys, trainees are not ready to go to the field at the end of the training and the training needs to be extended for a few days. If extensions have not been budgeted for, this can cause quality issues elsewhere, e.g., the survey firm having to cut corners elsewhere to compensate for increased costs. Avoid this by setting out with a realistic training schedule and putting contingencies for potential extensions into the budget and timelines. Avoid fieldworker training that lasts longer than 18 working days (3 weeks). Trainees ability to absorb and remember material and general morale decreases with training length. Also, since every day of fieldworker training is quite expensive, paying for days that are increasingly ineffective, longer training becomes increasingly inefficient and can reduce the funds available for other important survey activities. Conducting an efficient and yet effective training as described in Chapter 16 Conducting the training should allow most surveys to be trained well in less than 3 weeks. For surveys with a large number of questionnaires (e.g., school surveys) or special tasks such as anthropometrics or plot measurements that take a substantial amount of time to train (e.g., schedule 4-6 days for solid anthropometrics training), often it is unrealistic for interviewers to learn all questionnaires and tasks sufficiently well in the time available. Instead, divide questionnaires and tasks between specialized fieldworker roles. For example, anthropometrics could be collected by dedicated measurers and the main interviews to be conducted by interviewers. This way, each fieldworker role can focus on their questionnaires/tasks and learn how to implement them well. Furthermore, different fieldworker roles can be trained in parallel training sessions, which cuts overall training length and costs. Supervisor training should be held outside of the main interviewer training hours, so that supervisor (candidates) can attend the interviewer training in full. If supervisors are known prior to training, supervisor training can be held just prior to the main training for a couple of days. If supervisors are selected during the training, it needs to be held towards the end of the training in the late afternoon/evenings or during the day in modules that are not relevant for supervisors (e.g., administrative days, revision of material they demonstrated to know, etc.). The duration depends on the scope of the tasks, but generally 2-3 days are sufficient. Data monitor training is best held after the final field test so that the data collected in the field test can be used as training and practicing material. Data monitors should attend interviewer training and field testing to get a good understanding of the activities they will monitor later. Availability of trainers permitting, it can be held in parallel to the supervisor training, between training and field work, and during the first few days of field work. Generally, 2-3 days are sufficient for initial training and practicing and an additional 2-3 days of supervised work during the first few days of field work. 15.5 Schedule Prepare a realistic training schedule and track training progress on an on-going basis. Keep in mind the following general considerations to plan an effective and efficient fieldwork training: Stay flexible. Create a schedule that identifies all modules and allocates the required time. Use the schedule to track progress, ensure you are not falling behind and to plan your logistics, e.g., on what days you will need the training venue or transport. Expect timings to shift a little during the training, as some topics/activities tend to take longer or shorter than anticipated. Do not force compliance with the original schedule, but adjust the training schedule to the updated requirements of the training. Stay flexible to allow sufficient time to address issues, retrain topics, or dive into more detail as needed. Allocate time generously. It is better to finish training early one day rather than keeping trainees until late, rushing content or trainees not being ready for the field at the end. If you are falling behind schedule during the training, make a decision to extend the training as early as possible before logistical arrangements make this impossible. If training progresses faster than anticipated, use the time for additional practicing. Only shorten the training if you are certain that the trainees are fully ready for the field. Practice frequently. Do not leave (field) practicing to the end of the training. Schedule daily group practice and front-of-class interviews towards the beginning of the training to solidify newly learnt material. Practicing with respondents on-site or in the field towards the latter half of the training to prepare trainees for fieldwork. Factor in time for written tests and debriefs. Conduct regular written tests from the beginning of the training on days that new material was covered and debrief them with the class afterwards. Debriefing tests the following day also simultaneously serves as a great recap of the material learnt the previous day. Frequently change training modality, to keep trainees engaged and able to absorb and solidify content. For example, regularly break long blocks of content training with group practicing or role games to learn interviewing techniques. Rotate trainers between different modules to spice up the training and keep attention levels up. It is tiring for trainees to always hear the same voice and for trainers to lead a full day of training. Schedule time to debrief after field days. Avoid multiple field practicing or testing days back-to-back. Trainees can get used to their idiosyncratic, potentially undesired behavior. After a field practice or field test, allow for enough time to debrief, provide feedback and retrain topics that have not gone well yet, so trainees can try to do better on the next day. Typically, one day of in-class training is needed between field days. Train what is relevant. Only train content that interviewers need to know. Allocate time on topics proportionate to the direct importance for the interviewers. For example, spend less time on the general background of the survey and more on how to introduce it properly to respondents. Train when needed. Train material just before it is needed and can be practiced. Otherwise, trainees often forget and the content needs to be retrained. For example, it is often ineffective to train in detail the cover section of a questionnaire (identifiers, visit outcome, etc.) at the beginning of the training, as trainees do not know yet the protocols, questionnaire, respondents, etc. and are not familiar with CAPI yet. For the first few days of training, trainees only need to be able to enter interview files to follow in training and practice. Train the details of the cover section just before interviewers will use them for the first time, e.g., the field practice or field test. At this stage, trainees have a better overview and are able to make more sense of the section. They can also put it into practice shortly after. Split long questionnaires. First train and extensively practice on-site and in the field one part (e.g., household questionnaire) before repeating the same for the second part (e.g., agricultural questionnaire). Breaking the content into more manageable sizes makes it easier for trainees to solidify learnt material. Schedule around low energy periods. Schedule theory and important content in the mornings when trainees tend to be freshest. Trainee attention tends to decrease in the afternoon and is at its lowest after coming back from the field or directly after having a heavy lunch. During those periods, schedule hands-on and interactive modules that require greater engagement from trainees, such as practicing, interviewing techniques or written tests. Avoid training after coming back from the field. Train supervisors and data monitors outside of main training hours. They need to attend the main fieldworker training in full to understand field procedures. Typical time slots are prior to the main training (if roles are predetermined) or towards the end of the training once roles have been selected, during the late afternoon or on administrative, travel or rest days. Do not exceed 6-7 hours of training time per day and give frequent breaks, without the breaks being too long and taking over the day. For example, this can translate into a morning session from 9:00 - 13:00h, and an afternoon session from 14:00 - 17:00h, each with flexible 15-minute break between modules. Close the main training early enough to allow for supervisor training, daily trainer debriefs and preparing for the following days. Rest days. For training lasting over one week make sure to schedule one or two rest days per week. Trainees and trainers need regular breaks for good learning outcomes. The training schedule in Figure 15.1 illustrates how the fieldwork training can be organized for a survey with ~3 hr. questionnaire that consists of two parts. Each row corresponds to one calendar day since the beginning of the training and lists the modules to be covered in a day. More details on what to consider and how to deliver each module are given in the corresponding sub-chapter of Conducting the training. Column Hrs shows the approximate number of hours that should be allocated for each module in day. Modules may be broken down into several sessions per day, e.g., to allow the mixing of questionnaire content and practicing. Figure 15.1: An illustrative training schedule Click here to download an Excel version of the illustrative schedule. For an actual training schedule, usually more detail needs to be included, such as a breakdown of the questionnaire sections or topics to be covered in each module, the trainers and the location. Table 15.1 below shows what a detailed schedule may look like for day 2, 3, 8 or 9 of the training outlined above. Table 15.1: Example of detailed schedule for one training day Time Module Trainer Location 09.00 - 09.45 Review written test &amp; recap Trainer A Classroom 09.45 - 11.00 Questionnaire content: Labor Trainer B Classroom 11.00 - 11.15 Tea break 11.15 - 12.00 Group practice: Labor Garden 12.00 - 13.00 Questionnaire content: Health Trainer A Classroom 13.00 - 14.00 Lunch break 14.00 - 14.45 Group practice: Labor &amp; Health Garden 14.45 - 15.30 Questionnaire content: Education Trainer B Classroom 15.30 - 15.45 Tea break 15.45 - 16.30 Front-of-class interview: all sections Trainer A Classroom 16.30 - 17.00 Written test Classroom Please note that questionnaire content sessions are max 1.25 hrs. long and are followed by practicing sessions, so trainees can familiarize themselves with the newly covered sections. Trainers and training modality change frequently and trainees get to change the venue. More theory is being covered in the morning when trainees are freshest and more practicing in the afternoon when energy levels are lower. Active training time is 6.5 hrs. and there are frequent breaks. Given that timings tend to shift during the fieldwork training, it sometimes can be more efficient to only draw up a rough schedule prior to the training to ensure there is enough time allocated for all content and to identify which resources are required on what day, and to develop the detailed daily schedules only for the next 1-2 days of training. This way, the schedule can be easily adjusted during the training to consider the progress and address the necessities without having to redo a detailed schedule for the remainder of the training every time there is a small change. 15.6 Location Fieldwork training is best held in a location that: Has sufficient logistically capacity: Is easy to reach for trainers and trainees, has a suitable training venue, sufficient accommodation and food options, good network coverage, print shops, etc. facilitates field testing: Is close to communities that are similar to but outside of the sample. If the survey is limited to a specific region or area, it is best to conduct the fieldwork training there. facilitates field start: Allows field teams to start field work shortly after the training and trainers to supervise the first few days of field work. Find a training venue with a big hall large enough to comfortably fit the entire team (or multiple halls if parallel sessions are held) and space to breakout into smaller teams for group exercises, such as a garden, hallways, the canteen or additional rooms. It is crucial that there is good internet connectivity at the venue, either through on-site Wi-Fi, or general network connectivity. The venue should be either able to accommodate trainees and cater for food, or be easily accessible and close to restaurants or eateries. It should be quiet and free from distractions. Good venues tend to be hotels or conference centers with garden or open space. Avoid universities, schools, towns halls or other places that are busy and become easily unavailable. Book the venue early. Check availability, in case the training needs to be extended on short notice. 15.7 Preparation Avoid wasting precious training time by preparing everything that can be prepared prior to fieldwork training. Prior to the training, the following should be in place: Inform trainees of survey details and terms. To minimize attrition of trainees, make sure that trainees have been informed of and agreed to the overall survey parameters and contract terms, including the selection process, their remuneration package, insurance coverage, areas to be visited, the length of field work, the transport and lodging arrangements during field work, expected effort, etc. Often, trainees drop out from the training if their expectations were wrong. Have trainees sign a copy of the details prior to the training to indicate agreement. Contractual matters. All contracts, payment arrangements, insurance policies and other administrative matters should be prepared such that they do not delay training or fieldwork and that trainees are paid without delays and are covered by insurance. Avoid interrupting the training with contractual matters, e.g., by pulling out individual trainees to sign contracts. Instead, schedule dedicated admin slots prior to the training, after class or between training and fieldwork. For fieldworker selection at the end of the training, make sure that all contractual matters including insurance and payments can be addressed in the short time window between training and field start. Logistical/administrative support. Get a dedicated person to support with all the logistical and administrative work that has to be done during the training, e.g., setting up the venue, organizing transport, contacting local leaders, printing, purchasing equipment, signing fieldworker contracts, etc. If these activities have to be done by the trainers, they become a substantial distraction and can significantly affect training quality and pace. Projector with a BIG screen. The projected content must be visible and legible to all trainees. You should be able to connect a PC and tablet/phone to the projector. Ideally, it should be possible to quickly link trainees devices to the projector, to facilitate exercises and practicing. Test the set-up prior to the start of the training. Stable internet connection for all devices. Trainees have to sync their devices several times per day. Usually, the Wi-Fi connection of the training venue is not sufficient to connect all devices at the same time. Often, additional mobile network routers or WIFI hot-spots are required. One option is to already provide the internet connection that is planned for fieldwork, e.g., dongles, mobile phone routers or sim cards. Food and refreshments for the breaks. If food is provided by the venue, make sure it is ready in time for the break. If trainees eat out for lunch, set reasonable break times that allow trainees to finish lunch and to be back in time. Incentivize punctuality, e.g. those coming back late from the break have to sing a song. One tablet/phone per trainee. It is crucial for all trainees to learn how to navigate the questionnaire CAPI and use the device. If you do not have enough devices (since there are more trainees than interviewers) borrow devices or ask some trainees to use their private devices. Devices must meet the minimum specifications for the CAPI software (see here for Survey Solutions) and have the correct CAPI version installed. Consider using an app blocker to allow using specific apps only. Add shortcut/icons on the main screen for CAPI, the softcopy of the manual, the calculator or any other relevant app. Fully set-up CAPI software. For Survey Solutions this includes the installation of Survey Solutions on a server, interviewer accounts for each trainee, questionnaires imported to the server and assignments made. For training, often open assignments with infinite quantity (-1) are useful. Create several assignments with different scenarios if the questionnaire includes substantial pre-loading (e.g. household rosters in a panel survey). Electricity. Avoid delays due to power issues. Provide (lots of) extension cords to charge tablets during the training and/or in the evenings. Provide power banks for field days if the device charge does not last a full day. Samples of documents. If fieldworkers need to interpret, verify or extract information from any documents or items such as immunization cards, school timetables, or anti-malaria medications, get enough copies/samples to allow for hands-on practicing. If there is variation, provide different examples, e.g. different antimalarial medication brands. Measurement equipment. If fieldworkers need to use any special measurement equipment, such as height/length boards, GPS units, water testing kits, student testing booklets, etc., provide enough units, so that trainees can practice in a meaningful way. Field procedure documents. Sufficient copies of any documents other than instrument and manuals that fieldworkers need to use in the field, such as introduction letters, household tracking lists, maps, cluster completion sheets, referral forms, etc. Finalized questionnaires. The questionnaires should be reviewed, extensively pre-tested or piloted, translated and the translations verified. The CAPI version of the questionnaire should be extensively desk-tested and error free. Mistakes can cause significant issues or delays during the training. Provide printed copies of the paper questionnaire if needed. Fieldworker manuals. All manuals for interviewer, supervisor or other fieldworker roles have to be made available to trainees from the beginning of the training, so trainees can use them and get used to using them as a reference. Modifications and additions should be made to the manuals during the training wherever it was unclear or incorrect. Either hand out a printed version of the manual at the beginning and a revised version at the end, or provide regularly updated soft copies in pdf to the trainees which they can look at on their devices (which tends to work very well). Organize in-field practicing. Select practice areas/communities well in advance. They must not be included in the sample. Make all necessary administrative arrangements such as getting permissions, introduction to village officials, etc. If necessary, arrange respondents for field practices, so trainees can spend more time practicing than looking for respondents to participate (dont do this in areas/communities for the final field test). If you require certain types of respondents (e.g., children for athropometrics) check with institutions where they are concentrated, e.g., child clinics, schools, etc. Organize reliable transport for the trainees to travel to the field practice. Make sure the transport waits for the team, not the other way round. Organize on-site practicing. Arrange respondents for on-site practicing to visit the training venue. Arrange for their transport and remuneration and dont forget to also provide food/refreshment for them. Facilitation material. Microphones and speakers including batteries and fully tested, flip charts with non-permanent markers in multiple colors for drawings, name tags/stickers for all participants, printed attendance lists and feedback forms, and for trainees notebooks, pens in 2 colors to allow marking and clipboards, etc. It tends to be useful to have a printer or printing facilities in the venue to allow ad-hoc printing of e.g., exercises, tests, etc. Field work logistics and material. All necessary logistics need to be in place to allow fieldwork to commence immediately after training, including fieldworker contracts to be signed after selection, insurance policies, payment arrangements, transport, accommodation, etc. Similarly, have all fieldwork equipment ready, such as bag packs, rain coats, ID cards, battery packs, t-shirts, respondent incentives, etc. "],["conducting-the-training.html", "Chapter 16 Conducting the training 16.1 General considerations 16.2 Opening module 16.3 CAPI use 16.4 Questionnaire content 16.5 Practicing 16.6 Interviewing techniques 16.7 Expert measurements 16.8 Pre-interview tasks 16.9 Post-interview tasks 16.10 Final field test (often called pilot) 16.11 Supervisor Training 16.12 Data Monitor Training", " Chapter 16 Conducting the training This chapter is targeting trainers and provides details on how to deliver an effective and efficient interviewer training that facilitates learning without overstretching survey resources. The chapter assumes in-person training as is the case for most surveys conducted with in-person interviews. The first sub-chapter covers general considerations for the training. The remainder of the chapters is organized by training module. Each sub-chapter provides details on the rational, optimal timing and recommendations for the implementation of the respective module. See the training schedule in Chapter 15 on how individual modules relate and are best combined. Prior to the start of fieldwork training, make sure everything is fully set up. 16.1 General considerations Throughout the fieldwork training, consider the following points: Essentials only. Only train fieldworkers on things that are relevant for them and that their respective role is required to do. For example, there is no need for interviewers to learn details about the survey analysis or Survey Solutions supervisor CAPI role. Confirm with designers/analysts. The interpretation of a question or concept must be aligned all the way from how it has been designed to how it is fielded. This is not always the case. Keep note of anything you are uncertain about or anything that is under-defined and run it by designers/analysts on a daily basis. Ask them to be on standby so you receive timely responses. Get back to the trainees once you have received the response. Eliminate language barriers. Do not train in a language the trainees are not fluent in. If some trainers (usually the survey designers or analysts) do not speak the training language, use an interpreter or ask other trainers (e.g., fieldwork manager) to translate or reiterate their points in the training language. Support trainees. Especially during the first few days, trainees often get lost in the CAPI or questionnaire and are unable to follow the sessions. Trainers who are not actively leading a session should walk around the class, check that trainees are in the right place and support those falling behind. Probe for understanding. It is wrong to assume things have been understood if you just ask Any questions? and there are none. There are hardly ever any. Probe using common examples or scenarios. Ask variations of your test questions. For example, think of working profiles/jobs that are typical in the surveyed population and probe trainees if these are wage employed or self-employed. Encourage questions. From the beginning, make it clear to trainees that they should ask if they have ANY question, that it is better to ask than to remain with a doubt, and that there are no bad questions. Take questions seriously and address them. If trainees prefer not to ask in front of the class, they can raise their hand and be visited by a trainer. Involve trainees. Asking frequent questions. Let trainees read aloud questions and manuals. Ask them to correct one another. Dont always pick the most engaged trainees but try to involve everyone. Select a trainee using a random name picker, so it can be anybodys turn to answer your questions at any moment. They make it fun if displayed in front of class. Alternatively, select the person that is closest to where a paper ball lands (that you or anyone throws over their back). Select one trainee for a couple of questions to speed up things. Get to know trainees. Ask trainees to wear name tags during the first few days. Learn their names as quickly ass possible. This makes the training more personal and helps trainers to better connect trainees in-class performance with written tests or test interviews. Collect and react to feedback. At the end of each day, use feedback forms to collect anonymous feedback from trainees on how clear different sessions or trainers were, topics they did not understand or questions they have. Review feedback forms prior to the next day and adjust your training schedule or methods if needed. ??KEVIN: provide feedback form example. Break up and mix groups. If you start noticing groups of trainees sitting together who are not paying attention or are at the bottom end of performance, distribute them across the classroom. Ask low performers to sit next to high performers, so they can help when falling behind. Make it fun! Maintain a general positive and fun attitude. Play fun and exciting group games. The last person entering the venue late and unexcused can sing a song in front of the class (YouTube is full of Karaoke videos). Hold an end of training party. Use video &amp; audio. It can be a good way of demonstrating certain things like interviewing techniques or special tasks to trainees, so they can see/hear what they should and should not do and learn from examples. Use existing material if good, or collect video/audio material during pre-test, pilots and field practices to demonstrate good and bad practices to the class. Stress importance. Make it repeatedly clear to the trainees that it is important that they do a good job, that the survey result and ultimately policy and decisions depend on it. Give them a sense of purpose. Like any other human being, interviewers perform better at work if they care about what they do. Trainees are responsible for their learning. Make clear to trainees that it is up to them to ensure they have understood the covered material. They can read the manual, practice outside the class, learn from peers, ask questions, etc. to make sure they understand everything. There is a transparent and objective selection process at the end and they must pass it. Set and enforce rules. Trainees must attend and sign attendance every day. Ask trainees to be on time and respect training hours yourself by not running late or running over time. No side conversations, no (excessive) use of cell phones. Enforce in a fun but serious way, e.g., anyone breaking rules can donate to the cookie jar, has to hand in their cell phone, sing a song, etc. Zero tolerance for cheating. Make it clear from the beginning that there will be serious consequences for anybody being caught intentionally making up data or purposefully breaking protocols (ideally being immediately dismissed). Also make clear that fieldworkers have nothing to fear as long as they follow what has been taught during training and check with supervisors or fieldwork managers if they are unsure about anything. Daily trainer meeting. Trainers should meet daily at the end of the day to compare notes, evaluate progress and trainee performance, review tests and class feedback and plan the following day(s). Include supervisors if they are pre-determined. Document. Throughout the training, note down any changes to the methodology, known problems and general training details, so the details are available for data documentation and to inform for future rounds. 16.2 Opening module The opening module serves to introduce trainees to the survey and training. In many surveys, opening modules are unnecessarily long and involve too many people. Keep it as short as possible (1-2 hrs. are enough) to be able to cover some content on the morning of the first day. Cover: Opening speeches. Try to keep speeches by officials or leaders as short as politically possible and avoid, if possible, the opening of the class to depend on officials, as they might be late. Introduction to survey/organization. Give a brief introduction of the survey, its objectives, the organization, the project/study, etc. Do not go into too much detail here and limit it to what is essential for fieldworkers to know. If needed, provide more details in the manual and refer trainees to it. Introduction of trainers and trainees. All trainers and core survey team should briefly introduce themselves and their roles. Avoid introduction rounds for trainees if the class size is large, they are dull and ineffective. Play a fun ice breaker instead. Ask all trainees and trainers to write their name on a name tag and carry it during the training. Training plan. Run through the rough training schedule, daily working hours, explain need for punctuality, that there will be frequent test and evaluation of skills, that trainees are responsible for their learning, that all questions are encouraged, that there is zero tolerance for cheating, that trainees have to follow the procedures and rules covered in this training and forget those of others surveys they might have worked for, if they were different. Survey details and terms. Run through the overall survey parameters and contract terms, including the selection process, their remuneration package, insurance coverage, areas to be visited, the length of field work, the transport and lodging arrangements during field work, expected effort, etc. Importantly, this should be a refresher, NOT be the first time trainees learn details about their pay, other terms or the field work. Make sure all trainees have understood and signed a copy of the general terms prior to the training. Administrative/contractual matters. If you need to address administrative or contractual matters with trainees, such as contract signing, registering for insurance, signing out tablets, etc., try to do this as much as possible before the training starts (e.g. from 8-10 AM), or in a dedicated slot during which trainers do not need to be present. At the beginning, introduce the survey, sketch out the training and field work plan and set the training rules. Keep it short so you can focus on content during this day. 16.3 CAPI use This chapter assumes Survey Solutions to be used, but the same principles apply to other CAPI packages. CAPI, like any other software, is best learned by using it. Covering all functionality in a long theoretical session tends to be very ineffective as any functionality that is not immediately put into practice by trainees is forgotten and needs to be retrained later in the training. It is more productive and a better use of time to initially only cover the basic functionality that is necessary to proceed with the training, and to gradually introduce additional functionality as required throughout the training. For example, trainees dont need to learn how rejected interviews work until they learn about post-interview activities in the latter part of the training. Limit the functionality you train to what is relevant from the perspective of an interviewer and what will be used in the actual survey. For example, trainees do not need to learn how the Supervisor interface works, what exactly happens at HQ, or how assignments work or what a Yes/No question type is if there is none in the questionnaire. The fewer slides you have in the CAPI training and the more demo and practice you do the better. Train functionality by demonstrating it on the big screen and asking trainees to follow on their own device. Make sure the class can follow and wait for those falling behind to catch up. Those lost should check with their neighbors or raise their hand, so one of the trainers can come to assist. For example, show on the big screen how to login to the Interviewer app. Wait until all trainees are logged in on their tablets. Show on the big screen where to find an assignment card, explain what it is and show how to create a file. Wait for everyone to open an interview file on their tablet, and so on. Clarify any questions or doubts trainees might have on the way. You can train the questionnaire directly in CAPI, without training it on paper first, if you are using Survey Solutions or another CAPI software that provides a good overview over the questionnaire, makes routing behavior clearly understandable and allows for free navigation between different parts of the questionnaire. Note, this is usually not the case for software showing one question per screen. Training the questionnaire directly in CAPI has the advantages that trainees only need to learn the questionnaire in the format they will ultimately use in the field (e.g., no need to learn how skips work on paper). Trainees will also practice CAPI along the way, making them much earlier confident in using the CAPI version of the questionnaire and the CAPI software in general. New trainees do not notice any difference, and experienced trainees who are used to paper questionnaires usually stop missing them after a day or two. Train the basic CAPI functionality just before trainees need to use it for the first time. Usually, around 1 hour is enough. If you are training the questionnaire on paper, train the CAPI basics just before trainees practice for the first time on the tablets, usually on the first or second day. If you are training the questionnaire directly on CAPI, introduce the CAPI basics just before starting with the questionnaire content, usually on the morning of the first day. For Survey Solutions, cover the following topics in the initial introduction: A general overview, including how the Interviewer App and HQ server interact, what assignments and interview files are, what synchronizing does. Keep this part short. How to log-in to the Interviewer app. Make sure tablets and accounts have been set up prior to the training. How to create an interview file from their assignment. A general overview of the Interviewer App and how to navigate through the questionnaire. The outline of a question, the question number, the question text (what to read and what not), the instructions, the answer options (different by type). Question and section enablement. It is sufficient to work on the first section of the questionnaire. The complete screen, marking an interview as complete. The started and completed screen on the dashboard, reopening an interview file, discarding an interview file. Having completed the above steps, trainees should be able to follow in the questionnaire during the questionnaire content modules. The remaining CAPI functionality is best trained throughout the training when trainees need to use the functionality for the first time. During the questionnaire content modules, train the different question types, roster forms, static texts, sections and sub-sections as they come up, e.g., list questions in the household member module, yes/no questions in assets, etc. Trainees need to recognize and understand the difference between single select, multi-select, numeric, list and date questions, and understand the rosters. Prior to practicing or the first written tests (if done in Survey Solutions), train how to synchronize to receive assignments, create new interview files, review interviews on the complete screen, marking them as complete and synchronize to send a file. Conducting the test on Survey Solutions tends to be useful, as trainees need to correctly synchronize to get the assignment and submit their test (send the completed interview file). Prior to group or field practicing, train how to add comments to questions and at synchronization, so that trainees can leave questions and comments directly in the interview files when practicing. After field practicing or the final field test, train how to receive rejected interviews, respond to comments, and resubmit rejected interviews, so trainees can practice post-interview activities using interview files they have created themselves. 16.4 Questionnaire content A single, joint reading of a questionnaire is not sufficient for trainees to learn and understand the various concepts, definitions, and scenarios that are necessary to correctly administer a socio-economic questionnaire. Some implementing agencies, such as survey firms used to implementing opinion surveys, sometimes do not recognize the required effort and vastly under allocate training time for questionnaire content. Trainees must understand in detail the entire questionnaire(s), including the underlying definitions and concepts. Usually, this makes for a substantial amount of material to be covered, which if not trained well, can be quite dull and an ineffective use of time. Consider below points to make the questionnaire content modules interesting and effective: Avoid long content sessions. Covering the content of an entire questionnaire usually takes many training days. Avoid training content for more than 2 hours at a time. Long content sessions tend to be an information overload for trainees and make it impossible for them to internalize the content. Instead, spread the content training over more days and let trainees practice the newly trained material at least twice per day, e.g., after every long questionnaire section (or a few short ones). Less instructional, more interactive. Avoid one-directional training methods where trainers give long presentations or read questions without really interacting with trainees. This tends to be a very ineffective way of getting across questionnaire content. Instead, involve the trainees as much as possible. For each question (or a few at a time), select one trainee to read aloud the question and any instructions, answer options or corresponding manual entry. Ask the trainee to explain in their own words what they have understood and how the question is to be administered. Make sure the rest of the class can hear them. Correct them if needed. Ask the rest of the class for input. Check if the trainee has understood by probing with an example. Give feedback on tone of voice and way of asking the question. Ask the trainee to read out the question again if needed. Explain the big picture. At the beginning of each section (or part of the questionnaire if you have small sections), provide a brief general overview to the class, including the overall structure and routing behavior, things to be aware off, key concepts and definitions, the targeted respondent(s), etc. It is better to do this section by section (rather than once at the beginning of the training), so trainees can immediately see and later practice what has been trained, and can thus remember it more easily. Go into detail. Many questions have to be covered in more depth for interviewers to be able to administer them correctly. This can include explanations on the underlying concepts and definitions, how to categorize responses into the given answer options, common scenarios that require further probing, what error messages mean, what conditions open the question, how the question relates to other questions, how to sense check responses, etc. Make it tangible. Definitions for survey concepts such as household, wage employed, parcel, etc. can be very theoretical and hard for trainees to translate into the real world. When training anything theoretical or abstract, always try to illustrate it with examples and scenarios that are common in the surveyed population to make it easier to understand for trainees. For example, to explain what wage-employed and self-employed are, list jobs or work profiles that are common and show which category they fall into. Use the manual. The interviewer manual should hold all conventions on how to correctly administer the questionnaire. It must always be up-to-date to be correct. Also, trainees must develop the habit of consulting the manual to get additional information or to clarify doubts, or it will never be used. Ask trainees to keep the manual open on paper or as a soft copy on their device. After reading each question, ask trainees to check if there is a corresponding manual entry and to read it. One of the trainers should update the manual directly during the training if the entry is unclear or insufficient. Ask for input from the class if needed and confirm with them if the updated entry is clear. Distribute updated soft copies of the manual to the team every morning/evening. Use your time effectively. There will always be a few rare cases in the population that the questionnaire does not cover well. Trainees love pointing those out, e.g. But what if .? If these scenarios are frequent, they might require you to update the questionnaire or manual. If not, tell trainees to leave comments in the questionnaire if the situation arises in the field, and move on with the training, so that you have enough time to focus on the important parts. If longer discussions arise in class, clarify and stop the discussion. If you are not sure how to answer, get back to them the next day after having read up or consulted with the design team. Multi-language questionnaires. If the questionnaire has been translated into other languages, display the questionnaire in the training language on the big screen and ask each trainee to follow the questionnaire on their tablet in the local language they are most likely to use. For each question, ask it to be read in the training and local languages. This serves to verify the translation and to familiarize trainees with the questionnaire in the local language. CAPI. If training the questionnaires directly in CAPI (recommended), show and explain any new relevant CAPI functionality as you work your way through the questionnaire. On the big screen, demonstrate how the questionnaire should be filled in, how different question types function (e.g., text questions with patterns, search in combo box, etc.), highlight instructions or other parts that are not meant to be read out, what responses trigger what enablement or validation conditions, the overall organization of a section and how to navigate it, etc. Ask trainees to follow on their tablets. Demonstrate. Show how questions or (parts of) sections should be administered by demonstrating the part of the interview in front of class, either as trainer yourself, or by asking experienced trainees to act as respondent and interviewer. This is particularly useful for complex modules or parts that require inquiring, e.g., probing for the age of household members, a child nutrition module, parcel sketch, etc. Trainees can follow and record the responses on their tablet. Give immediate feedback, highlight any mistakes or parts done well, and provide additional information if necessary. Update the questionnaire &amp; CAPI. Depending on the level of pre-testing, piloting and subsequent change, the questionnaire, CAPI and translations may still contain issues that are uncovered during the questionnaire content training. Ideally, trainers not actively leading the training can make immediate corrections, as it tends to be less time consuming and allows corrected versions to be used sooner during the training. Be consistent and follow updating protocols when making changes, e.g., update the live version of the paper questionnaire, the CAPI and the translation sheet. Do NOT simply update the questionnaire on CAPI or local language only, as this will lead to inconsistencies, mistakes and survey error. Ask the class for input if useful, e.g., how to phrase a question, or what answer categories exist. If there are many corrections, ask a dedicated person in order to not slow down the training. Use samples. If interviewers need to extract information from documents, certificates or packages, or have to evaluate or rate items, it is important to expose them to a range of different scenarios and streamline their behavior. Bring a set of diverse samples to the training and practice in a structured way. One option is to run through scenarios together, e.g., by projecting examples of school timetables on the big screen and asking trainees to record the required information on their tablets. Another option is to set up work stations with different scenarios and ask trainees to rotate through the stations, complete the required task and record the information on their tablets. Trainers can staff individual stations and provide personal feedback to each trainee. 16.5 Practicing Trainees should frequently practice learnt material. Daily group practice and front-of-class interviews towards the beginning of the fieldworker training are effective ways for trainees to learn and solidify newly covered parts of the questionnaires and to break-up the monotony of questionnaire content training. Practicing with respondents on-site or in the field towards the latter half of the training exposes trainees to real-world scenarios and respondents, and prepares them for fieldwork. Each practicing modality serves a different purpose. It is not recommended to use them interchangeably, but to combine them throughout the course of the training. 16.5.1 Group practice Towards the beginning of the training, while new questionnaire content is being learnt, frequent practice is important for trainees to familiarize themselves with the questionnaire, learn how to administer the questions and practice the use of CAPI. Good ways of practicing during this phase are group practice or front-of-class interviews. Hold practice sessions every day that new theoretical content has been introduced, so trainees can put it to practice and internalize it better. Do not wait to practice until the questionnaire has been covered fully or until the end of the day if a lot of material has been covered. Instead hold 2-4 practice sessions per day at natural breaks (e.g., between sections) to practice the material/sections covered just prior. The change of training modality helps to maintain attention levels. Immediately repeating material helps trainees to understand and internalize it and helps to uncover remaining doubts. Practicing more frequently becomes unfeasible as each practice requires time to setup. Group exercise can be effective for small groups of 2-8 trainees. Change group composition between exercises. When allocating trainees to groups, mix trainees and break up clusters that formed in the class, so that low/high performers and those paying more/less attention are distributed. Try to be efficient and not to waste time when creating groups. For pairs, ask trainees to practice with their peers in the row in front/behind. For groups, work out the number of groups n needed, and repeatedly count loud from 1 to n, pointing at a trainee for each number. Each number corresponds to a group, and each number/group can meet in a different point of the training venue, e.g. All number 4s, meet at the blackboard. All number 3, at the door. Ask groups to practice the questionnaire sections that have been covered since the last practicing session. If there is time, groups can also practice all sections learned to date. They should not work through parts of the questionnaire that have not been covered yet. Give scenarios for trainees to ensure they practice scenarios that are likely to occur in the local context and cover all parts of the questionnaire. For example, when practicing the labor section, ask them to complete it for an employed individual, self-employed, supporting family worker, etc. ??NOTE, add example scenario box? Within each group, one trainee should read and ask the questions while another one answers them. After a few questions/sections the roles should be rotated, so each trainee had the opportunity to act as an interviewer or respondent. All trainees in a group should follow on their own tablet and record the answers, which at the end should be identical for all trainees within a group in theory. Ask groups to submit their interviews after the feedback session to motivate them to get it correct. Feedback after practicing sessions tends to be thin. Asking the class if there were any doubts, questions or comments often results in general silence. In order to receive better feedback, ask groups to actively come up with feedback points, note them on a sheet of paper including the trainees names, and to submit those to the trainers at the end of the exercise. Feedback points can be any question or doubt they have about questionnaire, definitions, scenarios, etc. or any mistakes they have found. You can set a minimum number of feedback points, e.g., 3 during the beginning of the training when things are not clear yet, and 1 towards the end of the training. Trainers should observe groups that have been allocated to them, correct mistakes and help struggling trainees on the spot, but also take note of the issues they observed for the feedback session. They should consider any issues, including the way questions were read or answered, the intonation, misunderstandings, questionnaire navigation, CAPI use, etc. While observing, trainers should also rate and record trainees skill levels and give individual feedback where necessary. Debrief with the entire class immediately after the exercise. Ask groups for their written feedback and discuss and answer their questions in class involving input from other teams/trainees. Ask trainers to provide feedback from their observations, explaining what was done wrong and also how to do it correctly. Ask trainees who have been observed to do something well to demonstrate to the class (e.g., how to explain a concept/question), so that all can learn from good examples. 16.5.2 Front-of-class interview Asking trainees to conduct (parts of) an interview in front of the class is a good way to demonstrate how interviews should (not) be conducted and evaluate trainee skills. It also shows if the class is able to spot any mistakes made and allows trainers to provide feedback on interviewing to the entire class. It is a good means of practicing after a good part of the questionnaire has been covered, and before practicing with respondents on-site. This exercise tends to take much longer than a normal interview, so either practice only parts of the questionnaire, or schedule several hours. Project one tablet to the big screen on which the responses for the interview will be recorded. It is best to not change the tablet during the exercise. For front-of-class interviews, select one trainee to come to the front and act as interviewer in a mini-role play. Select one at random that has not yet been to the front. It usually makes sense to rotate trainees after a few questions (e.g., 5-10), so more trainees get the opportunity to practice in the front and make the exercise less monotonous. Ask the next trainee to prepare in the meantime and wait in front in order, so the rotation of trainees does not take up too much time. Select someone to act as respondent throughout the whole exercise. This can be another trainee, or preferably one of the trainers or a real respondent. Using a trainer has the benefit that they can direct the interview with their answer and probe if some scenarios are understood. Using real respondents exposes the trainees to scenarios and respondents that are closer to what they expect to find in the field. For real respondents, hire a person that is similar to the surveyed population to visit the training venue for the afternoon/morning. Ask the interviewing trainee to administer the questions as if they were in a real interview and to record the responses on the tablet that is projected onto the big screen. The respondent should give normal responses with some difficult scenarios. Ask the rest of the class to follow the interview and raise their hand if they have a question or comment about the interview in front. As a trainer, pause the interview and give immediate feedback if the interviewer could have done better. Consider any aspects of the interview process, including tone of voice, reading speed, way of inquiring, etc. Ask the class for input, e.g., if they spotted the mistake or have any ideas how to do it better. Ask the trainee to repeat the question taking the input on-board. Also highlight things they have done well, so it is clear to the class that this is the way the questionnaire should be administered. All trainees should record all answers on their own tablet. At the end, their interview files should look the same. Asking the class to submit their interview files after the exercise is a good way to encourage them to follow closely and make sure they record everything. 16.5.3 On-site respondents Practicing with real respondents on-site is a great way to give trainees more and early exposure to real-world scenarios, while keeping the logistical effort and required training time low. Trainees learn how to apply the theory they learnt and how to handle real respondents. Another advantage of this modality is that trainers can observe all or most trainees conducting interviews, which is nearly impossible during field practicing when they are scattered across many homes. The best timing for an on-site practice is after the questionnaire has been covered completely and be practiced in group practicing, and prior to the first field test. Hire a group of respondents to visit the training venue or some other convenient location to be interviewed there. Depending on the number of trainees, around 5-10 respondents are usually enough. They should be similar to respondents in the surveyed population, e.g., cultivate land and keep livestock in agricultural surveys, or have jobs or small businesses in labor force surveys. Make sure to compensate respondents adequately, pay for the transport and refreshments or to invite them to the training lunch should it be provided. For the practicing, group trainees into as many groups as you have respondents, ideally not more than 4-6 interviewers per respondent. Each group should conduct a full interview with the respondent. Each trainee should follow on their own device and record answers, and they should take turns in asking the questions, e.g., a few questions or a section at a time. Groups should support each other if individuals fall behind, have questions or make mistakes. Ask groups to submit their interviews after the feedback session to motivate them to get it correct. If there is sufficient time, groups can switch respondents and conduct a second or third interview. Collect feedback, observe and debrief as described above for group practice. 16.5.4 Field practice The purpose of field practice is for trainees to get first experience in implementing the questionnaire with real respondents. There is no need to completely mimic field conditions as one does in dress rehearsals. Rather, aim to maximize the number of interviews each trainee can conduct and the number of respondents and scenarios they are exposed to. Prepare field practice well, observe trainees and collect good feedback to make it a success. Field practice often is implemented inefficiently with a lot of time being wasted on logistics that could have been prepared in advance: Teams waiting for transport, community introduction, looking for respondents, etc. It is not uncommon that during a whole day of field practicing, trainees only get to practice for 1 hour or could not practice at all as they did not find any respondent. Prepare well, so trainees can practice how to interview, not how to wait. Field practice sessions should be held after the questionnaire has been trained and practiced in class, and before the final dress rehearsal at the end of the training. Often it also makes sense to conduct it prior to sessions on household tracking, receiving rejecting interviews, logistics etc., so that content is broken down more and the field test can focus on the understanding and implementation of the questionnaire only. If the questionnaire is really long, e.g., a LSMS-ISA style questionnaire with long household and agricultural questionnaires, it can be beneficial to conduct more frequent field practices for different parts of the questionnaire. Try to be efficient when conducting field practices. If relatively short or only parts of a questionnaire are tested, often half-day field practices are sufficient to give some exposure to trainees and identify sufficient issues to be addressed prior to the next field practice. Since it is usually difficult to conduct class room training after the team has come back from the field, it is often helpful to conduct the field practice in the afternoon, so you can use the morning. Visit communities that are not too far from the training venue and large enough to have enough respondents for all trainees. Communities should have similar characteristics to the surveyed communities, but be outside of the sample. Contact or visit the communities the day prior to the field practice, do the community introduction and arrange for respondents at the time of your expected arrival, e.g., with the help of a community leader. Compensating respondents for their participation often helps with their availability and willingness to participate and is totally fine during field practicing. Again, the aim is for trainees to practice the questionnaire as much as possible with respondents. Try to minimize other logistical issues that could cause delays. Make sure all tablets are fully charged, synchronized and have battery packs available if needed. Arrange for food and drinks for trainees in the field or ask them to bring it. Make sure your transport arrives on time, knows the way and does not need to stop for petrol. Try to arrive early in communities, as respondents tend to become unavailable during lunch hours. Group trainees in pairs of two, trying to take skill levels into account and pairing high performers with low performers. Allocate each pair with a respondent. Agree on a meeting place for trainees to return to and an hour when they need to be back by the latest. Ideally, there is time for more than one interview per pair. Similar to the on-site group practices, trainees should take turns interviewing the respondent and both record all answers and submit the interview file at the end of the day. Ask them to write down any issues they experience while they conduct the interview. Working in pairs, one of them can take notes while the other one is interviewing. For each interview, each pair should list at least one question or doubt they had, and something about their interview or respondent that was noteworthy and is useful to share with the class. Try to observe as many of the pairs as possible, help and correct where necessary, mark their skills (LINK), and note down any issues you have observed as general feedback for the class. Focus on trainees that are borderline of being selected or that have not been marked yet. Collect as much feedback as possible from trainees before you leave the community. It becomes much harder later and the quality of the feedback decreases rapidly. As pairs of trainees arrive back at the meeting place, ask for their written notes, debrief quickly with each pair, clarify any issues they had, and record general feedback for the class. Often trainers assume that the questionnaire is working well and that interviewers are able to administer it correctly because they did not report any issues during feedback sessions. This assumption is wrong. Many issues cannot be identified by interviewers, are forgotten or not reported. Those reported are biased towards trainees concerns, e.g., they are quick to report something that caused extra effort from their side. Trainers must observe interviews being conducted and compile their own feedback. To receive meaningful feedback from trainees, trainees must note down issues or questions as they experience them and must be debriefed shortly after conducting interviews. After each field test, hold a general debrief session with the entire class in which you respond to the written feedback from the team, answer any questions, and provide feedback from your observations. This might involve short retraining of concepts or questions that have been ill understood, practicing or role plays. Clarify all issues prior to the next field test. It is normal for quite a few things to go wrong in the first field test and to improve quickly. Sometimes, you can debrief to some extent directly in the field in an empty classroom, a community center, the shadow of a big tree, etc. This way interviews are still fresh in everyones head and there is no need to go back to the classroom after the transport back from the field. If you are required to debrief the same day in the classroom, make sure to leave the field in time and to allow for a long break, as everyone tends to be really tired after field practice, especially after the transport back. Design your training schedule such that field practicing days are followed by classroom days, so you can have a detailed debrief session in the classroom with the big screen and all other required facilities. 16.6 Interviewing techniques In addition to understanding the questionnaire content, interviewers must master a range of interviewing techniques that are crucial to collect good quality data. Some examples are: Introduction and interview request: Being able to convince respondents to participate in the survey is key to keeping unit non-response error low, particularly in urban areas or other settings with high refusal rates. Interview continuation: Being able to keep respondents engaged in lengthy interviews and stop them from prematurely ending the interview is important to minimize item non-response and measurement error. Inquiring: Respondents often do not understand or misinterpret a question, give incomplete answers, respond I dont know or do not want to disclose some information. Interviewers need to be able to get respondents to answer all questions in a neutral and non-leading manner. Controlling interviews: Respondents sometimes digress, give lengthy explanations, or talk about other things. Interviewers need to be able to lead the conversation in a pleasant and courteous manner to maintain the respondents cooperation. Unfortunately, these interviewing techniques are rarely trained. Instead, it is assumed that interviewers automatically know them or will learn them over the course of the survey. In reality, while some interviewers are experienced or naturally good at it, many others struggle or resort to undesirable practices such as making wrong promises to obtain consent or leading responses. This can exacerbate interviewer effects and lead to high levels of measurement error and unit and item non-response. Teaching trainees that are not good a task the skills of those that are mitigates these effects, e.g., see Groves and McGonagle (2001). Teaching interviewing techniques goes beyond theory. All features of communication are relevant for the interaction between interviewers and respondents, including the choice of words, tone of voice, volume, as well as attitude, facial expressions or gestures. To learn a technique, trainees have to see and hear what they should and shouldnt do, and learn from good examples. To be able to put techniques into use, trainees need to repeatedly practice and receive feedback until they are able to (re)act correctly and quickly. Which interviewing techniques are best trained depend on the survey. Interviewers can face different challenges in the field. For example, refusals may be rare in a household survey, but very frequent in an enterprise survey, requiring dedicated training on how to address them. In addition, best practices often are sensitive to the culture and context, e.g., how to inquire if respondents are hesitant to disclose their income. Therefore, the training agenda on interviewing techniques usually needs to be tailored to each survey. 16.6.1 Identify challenges and responses To establish which interviewing techniques are most relevant for the survey and should be trained, first compile a list of the key challenges that interviewers are expected to face in the field and need to be prepared for. Focus on those that are most common and have the biggest effect on survey error. Consider any response, concern or action by the respondent or any other situation that can negatively affect data quality if interviewers do not apply the right technique. Examples are: respondents refusing an interview stating reasons related to time, etc. other members trying to listen to sections to be administered in privacy respondents unable to quantify consumption amounts in units long and damaging breaks if tablet needs to be restarted error messages flagging gross mismatch in harvest quantities, land size and sale prices Record different ways in which a challenge may manifest itself or how the interviewer can recognize them in the field, using colloquial terms or used by respondents where it applies. As an example, respondents may say I am really busy, I need to leave or I need to cook/wash/ when refusing and stating reasons related to time. For each challenge, identify good interviewer responses. These can include verbal responses actions or procedures. Often, there is no single response to a challenge that works equally well with all respondents and in all circumstances. Good interviewers often have a repertoire of responses and tailor them to observable features of the respondent or the context. For example, if a respondent refused based on time and appears to be rushed or pre-occupied about something (e.g., fighting children) the best response tends to be to ask for a different moment, e.g., by saying I see that now it is difficult. When would be a good moment for me to come back?. However, if the respondent refuses because they have to do a chore but seems generally collaborative, it is often good to offer to do the interview while they do their activity. For each challenge, also record bad practices or responses that interviewers should avoid, if there are any. As an example, if respondents do not know the quantity consumed of an item, a bad probing practice would be to say Was it maybe 1 kg?, as this is leading the respondent to answer 1 KG. Allocate challenges and responses into separate modules by topic, importance and anticipated time required to train. It is usually good to have at least one module on Introduction and interview requests, one on Inquiring and controlling interviews and one on Challenges observed during field testing. You might need additional modules on key questions or sections, such as Estimating land size or Conducting consumption module. Ideally, challenges and responses should be compiled prior to the training to facilitate planning and make training sessions more productive. Use a combination of any of the following sources to identify challenges and responses: experience from previous surveys with similar context, populations and subject focus groups with experienced interviewers observations and feedback from pre-test or pilots If you are unable to prepare prior to the training, use challenges and responses observed during field practice sessions or hold short focus groups with the class at the beginning of each module. See HERE a core set of challenges and responses that are common to many surveys. &amp;&amp; NOTE: WOULD IT MAKE SENSE TO COMPILE ONE? 16.6.2 Train &amp; Practice Schedule modules on interviewing techniques just prior to field practice and final field tests so trainees can apply the techniques soon after. Additionally, you might need to schedule some modules after field practice and field test to practice new situations that came up during field practice or retrain those that do not yet work well. Modules between 45 min and 1.5 hrs. tend to be sufficient to cover one core topic. For each module, first train interviewers on what challenges are likely to occur and what the best responses are. Project the list of challenges, categories and good responses on the big screen. Work through each category, identifying together with the class the ways in which challenges may manifest themselves and any tips and tricks for a good response. Also cover bad practices that interviewers should avoid. Ask trainees that are good at a technique (experienced or good interpersonal skills) to demonstrate the best practices in front of class in a role play. Give feedback if necessary and ask the class for input to get consensus and make sure responses are appropriate for culture and context. Try to keep this part short. After having covered the theory, drill trainees until they are able to respond to a challenge quickly, correctly and in a natural manner. This is best done by asking trainees to simulate the interviewer-respondent interaction in front of class in a role play, providing immediate feedback and letting them repeat if necessary. Depending on class size and skill level, it might be best to divide the class into large groups, each supervised by a trainer. To practice, select one trainee to act as interviewer and if needed another one to act as respondent. Usually, it is more useful for the trainer to act as respondent as it allows them to better drive the conversation. Ask the selected trainee to come to the front or just stand up and speak towards the class, which is often faster. Explain the scenario and challenge and ask the trainee to re-enact the interviewer response as if it was a real field situation, speaking loudly so the entire class can hear. Give feedback on what they have done well and what not. Ask the class for input. If the response could be improved, ask the trainee to repeat the response. Collect feedback, correct them and ask them to repeat until the trainee does it well. Practice with as many trainees as possible or until the response generally works well. This can be quite a lot of fun and is a good way of breaking longer theoretical sessions. Two examples to illustrate: To practice introduction to the survey, select one trainee, ask them to stand up, tell them that they just knocked at a door, a certain type of person opened, that they should introduce themselves and ask for participation. Check that they are giving all necessary information, say nothing wrong, are easy to understand, use the right tone of voice, have good facial expressions and gestures, etc. Collect feedback from the class. Ask the trainee to repeat their introduction until it is correct in content and well presented. To practice asking for participation, ask a selected trainee to make an introduction and ask for participation as above. Act as respondent who is unwilling to participate, stating one of the challenges discussed in the training, such as I am really busy now or What do I get out of this?. Ask the trainee to respond and convince you to participate. Again, together with the class, check if the trainees response was quick, correct and natural. Give feedback on possible improvements and let the trainee repeat their response until satisfactory. 16.7 Expert measurements As part of some surveys, fieldworkers have to take measurements that require a relatively high level of expertise and thus need special attention during the fieldwork training. Examples include: Anthropometric measurements, i.e., height/length and middle upper arm circumference (MUAC) Plot size measurements, soil sample or crop cuttings in agricultural surveys Learning outcome assessments in school surveys, measuring reading, writing and math skills Medical tests, such as taking blood pressure or measuring hemoglobin levels Water quality tests For these types of measurements to be accurate and precise, fieldworkers usually have to be able to correctly handle specialized equipment and strictly follow testing procedures. For example, in learning outcome assessment tests, interviewers need to simultaneously handle testing booklets, a timer, and the CAPI questionnaire, while guiding a young child and maintaining a neutral testing environment. For water quality tests, interviewers need to handle the water testing equipment and respect the procedures from taking the water sample to testing and reading the results. Learning how to implement any of those tasks to sufficiently high standards usually takes several days. Trainees need to learn the theory, extensively practice to gain confidence and be drilled until they are able to take the measurements quickly and always correctly. In addition, for some measurements such as anthropometrics, repeated standardization exercises need to be conducted to streamline trainees behavior and ensure reliability and validity of the measurements. Often, the available training time does not allow for trainees to learn and practice both interviewing and taking the measurements. In those instances, it is best to specialize fieldworker roles, so that some will conduct interviews, while others will focus on taking measurements only. Training for measurement specialists can happen in parallel to the main interviewer training, either from the beginning of fieldwork training (e.g., if a special profile such as trained nurses is required), or of any point during the training, e.g., once a picture emerges which trainees are better suited for which role. To effectively and efficiently train measurement experts, follow the four steps below. 16.7.1 Theory First, cover the theory including the background and purpose, the functionality and use of the equipment, the detailed steps of the measurement procedure, things to be aware of, avoid, how to answer common questions by respondents, etc. Make this part tangible for trainees by including demonstrations by experts, or videos showing the right use of the tools and the measurement procedures. Test the trainees level of understanding of the theory using written tests. 16.7.2 Practice components Second, practice as much as possible all components of the measurement process, such as handling testing equipment, taking the sample or reading the measurement. Drill trainees until they can implement all components routinely, quickly and correctly. Being fully familiar with all tools and routines breaks down the overall measurement process and helps trainees to not get overwhelmed when first practicing with a subject or samples. Since testing subjects (e.g.,, children) are hard to come by and testing kits can have expensive per-unit costs, it is usually also more efficient to practice individual components first. As an example, for anthropometrics, the following components can be practiced: Set-up and handling of equipment. Scales, height and length boards need to be set up on uneven terrain and be correctly packed, unpacked and carried through communities. Scales might have to be calibrated or calibration checked. Measurement procedures. For example, depending on the available scales, to take the weight of a young child, one measurement procedure is for the caretaker to step onto the scales first, take Tara, hand the child to the caretaker and take the reading. This process can be easily practiced by trainees e.g., by handing over a backpack. Taking and reading measurements. Especially for length, measurers only have a moment to take and read the measurement, as children want to move and fight the position in which they are held. Depending on the length board, taking and reading the measurement quickly is far from straightforward and requires a significant amount of practice. You will be surprised by the amount of error that can be eliminated at this stage by repeated practicing. Practice by asking trainees to measure things of known length and gradually decrease the time available per item. Compare individual measurements to the known length. 16.7.3 Practice full measurements Third, once trainees have a good command of all components, bring it all together by practicing to take measurements of actual subjects or real samples. Provide enough and diverse subjects/samples, so trainees can practice repeatedly and are exposed to different scenarios/cases. For example, for anthropometrics, invite children and care takers to the training venue or visit health centers or other institutions with sufficient number of children as testing subjects. Observe trainees and provide immediate feedback to individuals or the entire class. Trouble shoot any issues and re-practice individual components if they can be improved. Practice until trainees can implement the entire measurement procedure correctly and have developed a routine for it. 16.7.4 Standardize Fourth, conduct standardization exercises to improve the accuracy and precision of trainees measurements. In standardization exercises, their measurements are compared against those of an expert, and individual measurements against one another. The exercises serve to identify and eliminate as much as possible any idiosyncratic measurement practices trainees might have causing measurements to deviate from the true value, and as tests for the final selection of fieldworkers. Hold standardization exercises towards the end of the training, once trainees can implement the measurement procedures reliably and quickly. Often, it is useful to hold at least two separate exercises, with a break of 1-2 days in between to retrain and practice. For a standardization exercise, set up stations with subjects/samples. For example, for anthropometrics this can be children with care takers and sets of scales, length and height board. Ideally, stations have variation in characteristics, e.g., children of different age groups, below and above 2 years, with and without braided hair, etc. The number of stations required depends on the number of trainees. It should be large enough for each trainee to get enough practice and exposure to different scenarios. For larger groups of trainees, it might be best to split trainees into groups and let each group rotate through a subset of stations. Consider the burden on the persons who form part of the station, especially on children for anthropometric measurements. First, the expert measurer should take two independent measurements at each station and record the values. Their measurements serve as the benchmark against which trainee measurements are compared. If the exercise serves as the test, trainees should not be able to observe this process. Subsequently, trainees should rotate between all stations and take their own measurements. Use different starting points or directions to speed things up and avoid queues at individual stations. Trainees should record their measurements at each station, together with the station name/ID and their own name. This can be done on paper and later entered into a spreadsheet, or ideally in a short, customized CAPI questionnaire, Google Form or the like. As for the practice, observe trainees and provide immediate feedback. Try to identify why their measurements are deviating, what they need to do to correct it, e.g., stretch the childs legs more to not under-measure their length. Ideally, two independent rounds of measurement should be taken, so that each trainee has measured each station twice. The difference between the first and second measurement for each trainee at each station is an indication of the precision/reliability of the measurements. The difference between the measurement taken by trainees at a station and that taken by the expert is indicative of their accuracy/validity. Calculate and interpret the technical error of measurement (TEM) to assess the variation of measurements taken by each trainee. For more details, see e.g., WHO/UNICEF guide on anthropometry in children &lt; 5. Useful tools - WHO/UNICEF guide on anthropometry for children &lt; 5 - Video on anthropometric measurements (French) - Video on LandInfo module for soil characterization and identification 16.8 Pre-interview tasks Prior to starting an interview, interviewers usually have to take some actions and use tools that need to be covered in the training. Examples are: Identification &amp; tracking of respondents Selection protocols, if there is in-field sampling Identifier system, questionnaire cover, assignment cards, creating interview file Revisiting and replacement protocols Usually, it is best to train those pre-interview tasks and tools towards the end of the training, just before interviewers will get to use the tools or practice the tasks in the field. By immediately putting the learnt material into practice, trainees learn the tools and tasks more quickly. At this point during the training, they are also already familiar with the CAPI tool and overall questionnaire content, making things like questionnaire identifiers and revisiting protocols less abstract. Training these items directly at the beginning of the training tends to be less ineffective, since trainees will learn a lot of other material and forget before being able to try it out. Good moments to train pre-interview tasks are usually the days prior to field practice and/or final field tests. Most of the tasks are best trained hands-on with concrete examples. Prepare samples of the tools to be used during the training. For example, to train tracking sheets, household identifiers, assignments and how they link, show the tracking sheet for one PSU, together with corresponding assignments, and demonstrate how they relate. Use the PSUs of the final field test to reduce the preparation effort (tracking sheets, assignments, etc. Need to be prepared anyway for final field test), and to make it very tangible for trainees (since they will use them the following day). Make sure trainees can practice sufficiently in the training if any of the tasks is complicated, such as e.g., the use of Kish grids for in-field selection of units. 16.9 Post-interview tasks In many surveys, interviewers have to conduct certain tasks after completing an interview, such as checking for completion, responding to feedback or inquiries, correcting mistakes, revisiting respondents, etc. These tasks are often underdefined and insufficiently trained, which can result in significant measurement and processing error. For example, interviewers may fix interviews by simply changing recorded responses to make issues go away, without actually addressing the underlying problem. Interviewers need to learn what they must and must not do in a dedicated module towards the end of the training. For surveys conducted in Survey Solutions, one of the main post-interview tasks is to review and respond to rejected interview files. This is used as an example to illustrate how these tasks can be trained. The same principles also apply to other tasks. Reviewing and responding to rejected interview files is best trained practically, by looking at actual examples. Ideally, trainees can practice with interview files they have produced themselves. Not only does this make it less theoretical, but it can also show trainees that they actually made some mistakes, what they were and how to prevent them in the future. Use interview files from the field practice or even better, the final field test, as interviews will generally be of better quality and be produced under close to field conditions. Prior to training this module, trainers (and data monitors if pre-determined or as part of their training) should carefully review submitted interviews and identify issues. Normally, at the beginning there tends to be at least one per interview. They can make some up for practicing purposes if there are none. They should write comments and feedback as would be written during fieldwork and reject the interviews to the interviewer. Try to do this with at least one file per trainee (or pair of trainees working together during field test). During the training, first give a quick general introduction to explain the rejected files tab on the interviewer dashboard, how to open a rejected file, how to read the rejection comment, how to see all comments by the supervisor, how to react to rejected files, respond to comments and resubmit them by marking them as complete and synchronizing. Then, project a rejected interview to the main screen and ask the respective trainee/pair to come to the front. They should explain what led to their mistake and what they should do if this occurred in the field. Ask them to respond to the issues and resubmit the file on screen. Ask the class to help and comment. Repeat this with as many files/trainees as possible and useful. Ask all trainees who did not solve their file on the big screen to solve it on their tablet and resubmit them. This exercise is also a great way to identify and streamline the rules around responding to rejected files, revisiting respondents, etc. Update the manual if necessary. 16.10 Final field test (often called pilot) At the end of the training, a final field test should be conducted in which all survey questionnaires and protocols are tested under real-field conditions prior to field work. It is an important exercise to: Expose interviewers to field-like conditions and give them the opportunity to practice without affecting the sample Determine if the field team is ready for field work Identify and address any outstanding issues with questionnaire or field procedures Some surveys do not conduct a separate pilot prior to the training and refer to the final field test as pilot. In those cases, the final field test importantly also serves as a reality check for questionnaires and protocols, since they have not been tested in full under field conditions before. The final field test should be conducted at the end of the training and prior to the field work. Usually, 2-3 days of final field testing are necessary but also sufficient. If short debriefing/feedback sessions can be held in the late afternoon of each day, they can be on sequential days. If not, schedule debriefing/feedback days between field days. Allow for at least one classroom day after the final field test for an extensive feedback session and to teach the class on how to deal with rejected files, inquiries from HQ or any other post-interview tasks. Additionally, you might need to allow for additional time for admin, rest days or travel to the field location. To create conditions as close to the field as possible, the final field test must be conducted with units that are similar to those in the survey sample, but crucially that are not part of the sample. For household surveys, this usually means conducting the final field test in un-selected communities within the survey areas and often requires the team to travel from the training location closer to the field. For surveys part of RCTs, this means practicing within the treatment areas. Ideally, locate the final field test such that the entire field team can be supervised and debriefed together in a central location. This ensures that all field workers have received the same input and reduces team effect. Split the final field test into different locations if there is a large variability in the sample that affects questionnaires or protocols. If doing so, try to streamline as much as possible the supervision and feedback given to all groups, e.g., by trainers exchanging notes before providing feedback to their respective group. Often it is easiest to select a set of testing PSUs (e.g., communities) during the sample design and have them undergo the same preparation steps as the sampled units, e.g., assignment of IDs, procurement of reference data, add them to the CAPI questionnaire, do community sensitization, etc. For surveys with a separate listing exercise or panel surveys, conduct the dress rehearsal for all stages using the same units and keep the data, so you can properly prepare and test each stage of the survey. It can be very laborious and cumbersome to prepare separate data. One important aspect of the final field test is for field workers to practice the selection, identification and tracking of survey units such as households, convincing respondents to participate and to learn how to make appointments and to manage time. Do not pre-arrange respondents as one would do during field practice. Equip interviewers with the same assignments or tracking sheet format they would receive during field work. If you do not have lists or names for the rehearsal units (e.g., you have the household names for sampled communities, but not for practicing communities), send someone to the community prior to compile lists that you can use to produce the assignments/tracking lists. Allocate interviewers to their expected field teams and let the supervisors assume their management role, including the assignment of cases to interviewers. For the final field test, it is often beneficial for interviewers to conduct interviews in pairs. This reduces the number of cases required in any of the visited communities, increases the probability of finding available respondents and allows interviewers to help one another. Each pair can conduct multiple interviews per day and take turns in administering the questionnaire. As in the field tests, both interviewers should record all answers on their own tablet and submit the interview in the end. For the final field test, you might need to relax replacement protocols and add additional replacement units, so interviewers can practice conducting some interviews even if nonresponse is high. Make sure field teams arrive early in the communities, so that interviewers have enough time to locate cases and conduct interviews before respondents start cooking/having lunch and become unavailable. As for the field practice, make all necessary logistical arrangements to prevent delays or hungry/thirsty field workers in the field. Collect feedback and debrief as described for field practice. Ask all interviewers to submit their interviews at the end of the day. Use the data to finalize your data system and quality control system, e.g., by updating the data checks. Review submitted interviews using the standard processes of the quality control system. In addition, visually review as many cases as possible and compile feedback for interviewers and the quality assurance team. The data collected during dress rehearsal is a great way to learn and practice receiving feedback with the team, see chapter feedback. 16.11 Supervisor Training A short training for supervisors is required to prepare them for their role. It should be held outside of the hours of the main fieldworker training, so supervisors can attend both and are also fully trained as interviewers. If supervisors are pre-determined, a short supervisor training can be held prior to the start of the main fieldworker training that covers all tasks that do not built upon the main fieldworker training, such as administrative and logistical matters. If supervisors are selected from the pool of trainees, try to select and train all necessary content prior to the final field test, so supervisors can assume and practice their role during the final field test and be observed by trainers. Depending on the tasks their role entails, 1-2 days of supervisor training tend to be sufficient. Use the afternoons, evenings or the hours of any modules in the main fieldworker training that are not relevant for supervisors. If supervisors are required to conduct separate interviews (e.g., community), significantly more time of training is required. Prior to the training, make sure supervisor tasks are clearly defined and written up in the supervisor manual. The content of the supervisor training tends to be very survey dependent. It can include topics such as: PSU tracking and community introduction Assignment of units to interviewers and workload management Check health of interviews (any unanswered questions, errors, comments?) Checking PSU completion Observation of interviews, conducting back-checks Administrative and logistical matters, such as handling of finances, accommodation, transport etc. Use the final field test to train tasks that are best learnt by doing. This tends to significantly reduce the number of issues experienced during the beginning of field work, such as incomplete submitted interviews or incomplete PSUs. For example, to train supervisors on checking the health of completed interviews, review with all supervisors the interviews submitted from the final field test. One supervisor can review and run through the check-list for a few files, with the rest of the supervisors observing and providing feedback, before another one takes over. To look at the PSU completion, review with all supervisors the completion status of each of the final field test PSUs. It is recommended that in-depth data monitoring tasks beyond checking completion and health of the interviews are conducted by central data monitors to reduce data monitoring effect. Should supervisors nonetheless need to assume (parts of) their role, they also need to be trained as data monitors. See details in the following chapter. 16.12 Data Monitor Training Data monitor training is an important component of quality assurance in surveys. It serves to: Prepare data monitors for their role, so they can correctly identify issues in the received data, conduct back checking exercises, take the right actions and provide meaningful feedback. Homogenize behavior of data monitors as much as possible, to ensure all field teams receive the same level of scrutiny and to reduce the data monitor effect. Improve data monitoring process. The issues occurring in a survey and the actions required by data monitors tend to differ from survey to survey. Extensive practice using actual cases from the survey context is a great way to refine the tools, processes and tasks for data monitors, improving the overall review process. The data monitor training is best held after the final field test, once a good number of interviews created under near-field conditions are available for practicing. Importantly, this also allows data monitors to attend the main fieldworker training in full, so they can have a solid understanding of the content and the activities they will be monitoring. If data monitors are selected as part of the fieldworker selection, the training can also only start after it. Data monitors must be able to start their work once fieldwork has started and the first cases are available to be reviewed, typically on day 2 of fieldwork. It is important that fieldworkers receive timely feedback, especially for the first few cases, and that there are no backlogs of un-reviewed cases building up. In most cases, a total of 2-3 days of data monitoring training is sufficient, if you supervise and work closely with them for the first few days of fieldwork. The training can be held in the afternoons or evenings of the last few days of fieldwork training, or during sessions that are not relevant for data monitors, such as administrative matters for fieldworkers. If data monitors stay at a central location, you can also train on the day the fieldteams travel to the field and the first day(s) of fieldwork before cases become available to be reviewed. Prior to the training, clearly define data monitors tasks, procedures they need to follow, check list, etc. and write everything in a short manual focusing on the essentials (see an example here). Add to the manual a list of scenarios data monitors will face frequently and the actions they should take. For example, If GPS question is missing -&gt; trouble shoot tablet's location setup with interviewer. If not working -&gt; report to survey management. If working -&gt; Check if they can revisit household. If yes -&gt; reject interview. If no -&gt; comment in interview file, explaining reasons. Alternatively, list common issues and responses in a Google sheet or similar that is made available to data monitors and that they can refer to. Add to this list and refine responses throughout the training and beginning of fieldwork. At the beginning of the data monitor training, cover all theory and components of the tasks/procedures. While the details depend on the survey, this may include: general overview of the quality assurance process and their role how to use any tool or software, such as Survey Solutions Supervisor interface, dashboards, etc. how to check completion or use field check tables, etc. how to review cases, what to check for, how to interpret error messages, etc. how to back check cases, conduct re-interviews, conduct audio audits, etc. how to provide feedback to field teams and survey management Keep presentations to a minimum and train hands-on as much as possible. For example, while explaining how to review an interview file in Survey Solutions, data monitors can follow on their own devices, so they can immediately try out and learn how to find comments, unanswered questions, approve/reject, etc. Provide copies of the material that data monitors need to work with, such as interview cases, back check questionnaires, reports, etc. As a next step, practice with data monitors so they can learn and internalize their tasks. This should be done using real interviews that are not part of the sample, such as the interviews from field practices, or ideally of the final field test, as they more closely resemble the data from actual fieldwork. Using actual interviews makes the exercise more tangible for data monitors, as they can see the type of issues that are expected to occur and learn how to react to them. Use the practicing to update and refine the review process, i.e., the list of key issues data monitors need to check, the type of feedback they should provide and the actions they need to take. Practicing should be done such that data monitors behavior starts to be harmonized. An easy to prepare and very effective way to practice is to first run through a couple of cases together. Project the necessary material on the big screen in front, and ask one data monitor to come to the front and work through the case on the big screen. Ask the others to observe and provide input. Provide immediate input if they have done something wrong, missed something, but also if they have done things correctly. Discuss any points of doubt and update procedures if necessary. Repeat this with other cases and other data monitors until all data monitors have worked through several cases. Also effective, but more demanding in the preparation, is to provide the same cases to all data monitors, ask them to work through them and to compare results afterwards. As an example, provide each data monitor with the audio recording of the same interview and an auditing form, ask them to listen to the interview and fill in the form. Once complete, compare the forms in the group, discuss any discrepancies and reiterate important points. Practice until data monitors are able to correctly implement all tasks, until you have run out of practicing cases, or until field work has started. Once the first data is received from the field, it is important that data monitors shift to work on the actual cases from the sample. Often, however, at this stage data monitors are not fully ready to implement all tasks in a harmonized way. Therefore, it is paramount that data monitors are closely supervised by one of the trainers during the first few days and that good communication channels are put into place. If working from a central location, data monitors should ideally work from the same room so they can quickly exchange thoughts and one of the trainers can rotate, observe and provide feedback if necessary. If working from different locations, a common group chat or channel on WhatsApp, Teams, etc. should be set up to facilitate quick and easy communication and ensure all data monitors receive the same information. Data monitors should immediately communicate on the common channel if they have any doubts about a case or scenario or if they have made any noteworthy observations. Update the manual, tools or processes if necessary. Conduct spot checks of some of the cases that have been worked through by data monitors. Hold regular debriefs with all data monitors to provide feedback. References "],["assess-select.html", "Chapter 17 Assess &amp; select fieldworkers 17.1 Written tests 17.2 Evaluating skills 17.3 Standardization tests 17.4 Fieldworker selection", " Chapter 17 Assess &amp; select fieldworkers A competitive competency-based fieldworker selection is critical to achieve quality in surveys. It is therefore highly recommended to train an excess of fieldworkers, to continuously assess trainee performance throughout the training and to select interviewers, supervisors and other fieldwork roles from the pool of trainees at the end of the training, based on their capacity. The selection should be informed by their understanding of the survey content as evaluated in written tests, by other relevant skills and characteristics observed during the training, or in some cases by standardization tests. This chapter provides a step-by-step guide on how to evaluate trainees and select fieldworkers. Assessment and selection of fieldworkers during the training serves several important functions: Improve overall fieldworker capacity. Most importantly, selecting fieldworkers based on their ability to perform well in their role ensures a high level of capability among fieldworkers. Other selection criteria, such as experience or time with the organization often are insufficient or even counterproductive in identifying capable field staff, as experienced fieldworkers might have to unlearn undesired habits from other surveys, pay less attention during the training or have a developed a sense of entitlement. Continuously assessing trainees during the training provides a solid picture of their capacity to work on this specific survey. Increased attention during training. With tests from the first day and knowing about the competitive, merit-based selection, trainees understand that it matters to pay attention, participate and learn material independently. This is especially true for experienced staff who otherwise often dont pay full attention as they know that they will be hired anyway. Regular tests make it clear to trainees what they do not know. Knowing their relative score during the training also motivates low performers to increase their efforts. Higher attention and participation translate into higher learning outcomes and better performance. Tailored feedback/training. Test results reveal early areas of the content that are poorly understood and need more attention in the training. Reviewing and debriefing written tests often also provides valuable insights on where misunderstandings stem from or why mistakes are made. Trainers can tailor the training and feedback to address the issues, improving the overall level of understanding. Verification of methodology. Writing and debriefing tests often reveals unidentified gaps in the questionnaire design, such as underdefined concepts or conflicting definitions, and checks if explanations in the manual are useful or not. It is a great way to check if interviewers understanding is aligned with questionnaire design. Fieldworkers do better. As humans, we simply do better at work if we know that it matters. The tests and selection help to convey the idea to fieldworkers that it is really important for them to do their job well. Builds a reserve of replacement fieldworkers. Trainees who did not get selected (but were good enough) can replace fieldworkers that drop out during fieldwork. Since they have undergone the full training, they can be brought up to speed relatively quickly with a refresher training, some practicing and strong supervision during the first few days. For many surveys, fieldworkers are selected by the organization or firm implementing the survey that might follow their own procedures. In those instances, be cautious as the selection criteria and processes might not result in optimal outcomes. Try to exert some control over the selection process, e.g., by explicitly spelling out in the ToR the necessity to train an excess of trainees and select fieldworkers based on capability in a transparent and objective process, that you can back check. 17.1 Written tests Regular written tests play an important part in the evaluation of trainee performance. They are a very useful means to systemically test trainees understanding of questionnaire content and training material and to identify general areas of weakness that require to be reinforced during the training. Conduct written tests on a regular basis throughout the training, e.g., daily or every other day, whenever new theoretical content has been covered or debriefs have been held. For written tests to be able to fulfill their above-described functions, it is also crucial that they are conducted from the very beginning of the training. Do not leave written tests to the end of the training, as it is usually already too late to adjust the training schedule, alert low performers to increase their efforts or retrain content that is generally poorly understood. 17.1.1 Designing tests Written tests should probe the understanding of the questionnaire content and manual covered in the training so far, including protocols, definitions, answer options, as well as for relevant general skills such as calculating a percentage, average or converting quantities in units. If possible, tailor tests to the topics that were discussed during the (previous) day, so that tests also probe if trainees paid attention. Repeat questions or topics from previous tests that a large share of trainees answered wrongly. Write questions that are very clear, can be answered unambiguously and be easily marked. Do not over-rely on simple multiple-choice questions when writing tests as they tend to be relatively easy to answer and are not good at testing the actual level of understanding. For example, questions asking if something is true/false can be answered correctly 50% of the time by randomly selecting an answer. Instead, use questions that force the trainee to actively come up with an answer or require a combination of answers to be selected to be correct. Avoid open-ended questions, unless you have enough resources to review and score them for all trainees. Dont make the questions too obvious. Sprinkle irrelevant or misleading information into the questions and answer options. Add Dont have enough information, All of the above or None of the above as answer options to make it harder. Make sure the wrong answer options you add are plausible or relate to something else trainees might have heard elsewhere during the training. Occasionally write questions with only correct or wrong answer options. Aim for tests that take 15-30 minutes to answer, so you get enough data points while not taking up too much training time. This translates to around 10-15 difficult questions, some of them multi-response. If possible, try out the tests among trainees (or ideally even with designers) before administering it. This helps to identify ambiguous test questions and often also reveals previously unidentified gaps in the questionnaire design, such as underdefined concepts or conflicting definitions. Below examples illustrate possible ways of phrasing effective test questions. A first possibility is to describe a scenario that has been covered in the training or is explained in the part of the manual that should have been read already by trainees, followed by an actual question of the questionnaire. Q1. The household has a daughter Musa who is 7 months old. What do you record in question A4. How old is Musa in completed years? 1........................................................................1 0........................................................................2 0.7......................................................................3 0.58 (7/12 months).......................................................4 Nothing, leave empty and add a comment ..................................5 None of the above........................................................6 Another useful way to phrase test question is to write a list of statements that probe a definition or protocol from different angles and to ask trainees to select all the correct or wrong statements. Q6. Select all statements that are correct according to our parcel definition. A parcel  is a group of plots used for the same purpose............................1 is a continuous piece of land............................................2 can be separated by a path, canal or other boundary &lt; 1 meter wide.......3 must not be separated by any path, canal or other boundary................4 can be used for different purposes at the same time......................5 must be managed by the same person.......................................6 All of the above.........................................................7 A similar method is to list items with different attributes and ask trainees to select all items that apply, for example all individuals who should be considered as household members, all transactions that correspond to sales of livestock products, parcels that should be listed, etc. Q46. Select all of the following people who worked for a wage or salary according to our definition. Sumini, worked one day as harvest helper for neighbor, paid in crops.....1 Neni, a teacher employed by the government...............................2 Diah, selling homemade cakes by the side of the road.....................3 Anis, working in her parents shop (same household).....................4 Mimin, works in a restaurant, paid per day, no written contract..........5 Anton, works as a guard, paid monthly, has contract......................6 It tends to be useful to add numbers, letters or names to the answer options that are visible to the trainees. This makes it easier to refer to different answer options during the debrief sessions, e.g. Why do we consider Sumini to have worked for a wage or salary?. Useful tools - Tests for an agricultural survey conducted in Survey Solutions ?? ANY OTHER EXAMPLES? 17.1.2 Conducting the test There are several possible modal ties to conduct the written tests in the interviewer training: Test printed on paper. Works well if you have printing facilities in the training venue. Do not print tests prior to the training, so you can update them during the training to include points that have been in the focus on the (previous) day. Projector and paper. Project questions on the big screen and read them out. Ask trainees to take an empty sheet of paper, write their name on top and to record each question number with the answer next to it. This method is very flexible and useful for spontaneous tests as it allows last-minute updates to the questions. Get trainees pens with a different color for marking. Directly on CAPI. This approach works well if a CAPI software is used that allows questionnaires to be built and updated easily, such as Survey Solutions. Write the test as a questionnaire in CAPI, assign it to trainees at the beginning of the test. Ask them to complete one test on their tablet and send it back. Often you can copy questions from the actual questionnaire and modify them for the test. Answers are available instantly and can be marked using code (also directly inside the questionnaire). A very useful side effect of this approach is that trainees learn how to handle important functionality in CAPI, including receiving assignments/questionnaires and submitting interviews. You need solid internet connection for all trainees to sync. Google forms or similar. Several tools exist online that can be used to quickly write tests, deploy them on the trainees devices and make the results immediately available. Most of the tools are online-based and require good internet connectivity. To conduct the written tests, disperse trainees throughout the training venue, so that they cannot copy from one another nor talk. For tests conducted electronically, trainees only need their tablet and can do without a desk. Walk around and invigilate the class to make sure trainees do not copy or help one another. Help those that have technical issues and provide clarifications (to the entire class) if there is any general doubt about the quiz. Decide if trainees can refer to the manual or their notes during the test or not. Usually there is benefit in allowing them to do so, as it reinforces the manual as a reference and tests if trainees can work out the correct answer with the tools that they will have available in the field. Give sufficient time to trainees to complete the test, even if they have to refer to the manual a few times. Keep in mind that those that take a bit longer are not necessarily worse interviewers. You can decide to set an exact time limit or stop the test once most trainees have submitted their test and there is no or little progress among those that have not. Tests can be held at different hours of the day, each with their own advantages and disadvantages. Conducting it in the morning has the advantage that trainees are freshest and will have had a chance to revise in the evening if they were falling behind or absent. However, it is probably best to review the test shortly afterwards, before more material is covered during the day. This can put pressure on marking the test. Holding the test at the end of the day makes good use of the late afternoons that often tend to be unproductive. It also stops trainees from disappearing during the day as the test effectively also serves as an attendance call. Trainers have the opportunity to mark in the evening and give feedback the following morning, which is a good way to recap the key points of the previous day. Trainees who have completed their test can already leave the venue and do not have to wait for everyone to finish. A disadvantage of conducting the tests in the afternoons is that trainees do not get the chance to learn and practice the material at their own pace in the evening. 17.1.3 Marking tests It is important to mark tests quickly, so timely feedback can be given to trainees. For tests conducted on paper, a fast way to mark the tests is to collect completed test papers and redistribute them to other trainees during the feedback session. Give clear guidance on how to mark each question as you go through the test with the class, e.g., ask to put a tick or x in a different color on the left of the question number and to write the total number of ticks in the top left corner at the end. During the feedback sessions, ask for a show of hands of those who marked a question as correct or wrong to get a feeling of how well a question was understood. At the end, collect the test papers, cross check the marking and record the score for each trainee. For tests conducted on CAPI or any other electronic tool, you can export the data and write a short script to mark the test. The script should for each trainee calculate the score and for each question, give the percentage of correct answers, tabulate wrong answers and list the names of trainees who answer it wrong. Make sure (a draft of) the script is ready, so you can hold the feedback session shortly afterwards. In some tools, you can specify the correct answers and do the marking directly in the tool. If using Survey Solutions, this can be done by creating variables and displaying the score in supervisor level questions or questions conditional on supervisor level questions. Keep systematic record of the scores, ideally in a spreadsheet. For each test, record the total number of points possible, and the points achieved by each trainee. Do not use the average between individual test scores to calculate a total score and ranking across all tests to date, as tests tend to be of different length and difficulty. Instead, calculate the total score as the sum of points achieved over the sum of total points possible in all tests to date. 17.1.4 Providing feedback For each test, hold a feedback session, ideally shortly after the test, but no later than the following morning. Do better than just going through the correct answers. Actively involve trainees who got questions wrong to learn where misunderstandings are coming from and to ensure that the corrections are understood at the end. Involve the rest of the class to correct and explain in their own words. One way to achieve this is to project the test onto the screen and work through it question by question. For each question, ask one of the trainees who answered it incorrectly to come to the front, read the question, and answer it in front of class, explaining their reasoning and cognitive steps. The class should not help the trainee. Once they have answered the test question, ask the class if the answer was correct or wrong, the reasons why and to provide feedback. Step in if the class response is incorrect. Make sure the trainee in front understands where the error was in their thinking. Sometimes discussions arise if test questions were ambiguous or if trainees feel like they have been marked incorrectly. Do not let discussion get carried away. Intervene, repeat what is correct and why and move on. If there were any issues with the test question or the marking, exclude the question from the overall score, tell the class and end the discussion. Make sure to correct any issues in the manual or questionnaire if they were the reasons for the misunderstanding, as they would likely cause confusion again if they went unaddressed. 17.2 Evaluating skills Understanding the questionnaire alone does not make good fieldworkers. Other trainee skills and characteristics can be equally important and should be taken into account for the fieldworker selection, e.g., trainees performance in interviewing techniques, ability to use CAPI, friendliness or likability, etc. These skills and characteristics are not captured in written test and must be observed during fieldworker training. Sometimes, this is done implicitly, in an unstructured manner. As an example, while deciding which of the trainees with similar test scores to keep, trainers might drop a trainee that has been observed to struggle with CAPI. This, however, risks the evaluation to be subjective and not comparable between trainees, undermining the transparency and objectiveness of the selection process. For a comparable and comprehensive ranking of skills and characteristics, a more structured approach is needed, especially when dealing with larger class sizes, multiple trainers evaluating, or evaluated skills being one of the core selection criteria. This sub-chapter provides a step-by-step guide on how to evaluate skills using a skill matrix. 17.2.1 Identify key skills To produce a comparable and comprehensive ranking, first identify a few key skills and characteristics that matter most for the survey and cannot easily be probed for in the written test. Examples are: Any of the trained interviewing skills CAPI and tablet use Sound of voice, intonation, reading speed (especially for phone surveys) Capability to self-organize Friendliness and likability Determinism (especially if high unit non-response rate is expected) Trustworthiness and reliability (especially if working alone without supervision) Thoroughness Team leading and organizing (if selecting supervisors from trainees) Interaction with children and mothers (for anthropomorphic or student tests) A different set of skills and characteristics might be relevant for different roles. For example, supervisors should be selected from among trainees that are trustworthy, reliable and have organizational capabilities. Data monitors need to be reliable and thorough, do not mind repetitive tasks and be able to work independently. Trainees should be scored against each of the selected key skills during the training. Be realistic, prioritize and select only the skills that matter most (max 3-4), so it is actually possible to score all trainees. If there is limited time or capacity or no need to score against different skills, you can score trainees against a single dimension, the overall interviewing skills. When coming up with a single score, trainers can evaluate different skills, and note down any outliers together with the overall score, e.g., overall score 3, but very weak in introduction to respondent. 17.2.2 Design a scoring system You need to use a common scoring system to evaluate skills across trainees, especially if more than one trainer is evaluating trainees. Prepare a detailed scoring system with descriptions of each score. Make sure trainers know it and refer to it during scoring. Table 17.1 shows an example scoring system: Table 17.1: A skill scoring system. Score Rating Description 5 Excellent, exceptionally mastery Can be expected to perform extremely well. Can act as an exemplary role during training and field work. 4 Very good, above average More than adequate for effective performance. Possess a high skill level. No counterproductive behavior or deficiencies. No major deficiencies. 3 Good, acceptable, average Should be adequate for performance requirements. Possess an acceptable skill level. No major counterproductive behavior or deficiencies. 2 Weak, less than acceptable Insufficient for performance requirements. Does not possess sufficient skill level. Some counterproductive behavior or deficiencies. 1 Poor, unacceptable Significantly below performance requirements. Does not possess the skill. Counterproductive behavior. Many deficiencies. 17.2.3 Create a skill matrix Create a skill matrix. In its most basic form, it is a spreadsheet with trainees in rows and skills as columns. Each cell contains how a trainee has been scored for a skill. An illustrative example is displayed in Figure 17.1. You can download the Excel version here. Figure 17.1: Exemplary skill matrix for trainee evaluation It is useful to also add a comments column to record additional details on the scores or any other observations regarding a trainee. Ideally, the skill matrix also contains the results for all written tests and the overall score and rank, so you can get a quick overview of trainees performance. Color coding works great to quickly generate an overview of the relative performance of individual trainees. You could also calculate an aggregate score across all skills or across skills and test results to obtain a single metric for each trainee (not done in example). Keep in mind that you might want to weigh individual components differently, or exclude some components. For example, in the illustrative matrix in Figure 17.1, it would make no sense to include the score for Supervising into an aggregate that scores trainees suitability for interviewer role. If using an aggregate metric, always also refer to the individual components when taking decisions. Fill the matrix with scores. Different options are possible, depending on your set-up: Trainers can record scores in their notes or print outs of the matrix and copy their scores into the common spread sheet at the end of each day. This is easiest to set up, but might create clashes with some trainees being scored more than once and others not at all. Build the skill matrix in an online spread sheet such as Google Docs. Trainers record their scores directly on the spreadsheet. Create a short online form, such as Google Form with questions for the trainees name, skill, score and a comment to explain the score if needed. Make the form available to trainers to score during the day. You can link the form directly to a skill matrix on Google Sheets. This takes slightly longer to set up, but helps to keep track of who scored whom and the reasons for the score. 17.2.4 Rate skills There is only a limited amount of time available during fieldworker training to do a comprehensive rating of trainees skills. Start rating as early as possible and rate skills in all phases of the training. During the theoretical part of the training, engage as much as possible with trainees and score them against skills that you can rate based on the interaction. For example, you can score their ability to read out questions or intonation of their voice easily during the questionnaire content modules. Also, trainers not actively leading a module can observe some skills, such as the ability to use the CAPI software or tablets. During practicing modules, on-site or in the field, assign to each trainer a set of trainees to be observed and scored. Trainers should observe each trainee long enough to get a good enough impression to be able to score them against one or more skills, and then move on to the next trainee. Front-of-class interviews are also a great opportunity to score the trainee acting as interviewer. Regularly check the skill matrix for completion, and target trainees that have not been evaluated yet. If you have enough time, try to ensure that each trainee has been rated by more than one trainer to create a consensus decision. Give feedback to trainees. If you scored them low on a skill, explain why and that they should try to improve on it throughout the training. Be aware of changes over time as trainees have (hopefully) acquired skills during the training. Scores from the beginning of the training for one trainee are not necessarily comparable to those for another at the end. Try that your scores are reflective as much as possible of the skill levels at the end of the training, as you are ultimately interested in trainees potential to perform in the field, not their learning curve throughout the training. Reconfirm extreme scores, especially bad ones towards the end of the training, to make sure they still hold before taking decisions. Do the same for trainees that you think are borderline in or outside of the selection towards the end. 17.3 Standardization tests Written tests and observing skills are usually insufficient to properly assess trainees capability of accurately taking measurements such as anthropometrics or plot size. Often, it is advisable to select as fieldworkers only those trainees that have passed a standardization test, that requires them to measure the actual value of a subject/sample (height, weight, plot size) within an acceptable margin of error. To implement a standardization test, follow the guidelines for standardization exercises. Trainees should not be able to observe the expert taking their measurements and no feedback should be given to trainees during the test. Calculate the technical error of measurement (TEM) and set suitable cut-off values that determine if a trainee has passed or not. For example, for length/height in anthropometry, WHO/UNICEF recommend to use TEM &lt; 0.6cm for accuracy and TEM &lt; 0.8cm for precision. You can conduct a second standardization test after 1-2 days of additional practicing for those trainees who have failed the first one. 17.4 Fieldworker selection The selection of fieldworkers should be objective, transparent and based on capacity demonstrated during the training. Ideally, the selection process is not limited to interviewers, but is also used for supervisors, data monitors or other fieldworker roles, especially when working with new teams or survey implementing agencies. In some settings, long-term supervisors or monitors have shown to lack key skills or have a low level of understanding of the survey content compared to interviewers as they do not attend the training, pay less attention or assume they already know the content. Should it be inevitable to work with pre-determined supervisors or data monitors, make sure they are incentivized to attend, pay attention and participate to the training, and that they need to demonstrate their understanding of the material in order to be able to take on their roles. 17.4.1 Timing Usually, it is best to select fieldworkers towards the end of the training, either before or after the final field test, depending on the survey context. Selecting them after allows more time for trainers to observe trainees and for trainees to show their strength in the field. Those that will not be selected and act as an interviewer reserve will have had some field experience, making it easier to bring them up to speed if they need to replace drop-outs. On the other hand, selecting fieldworkers just prior to the final field test tends to make the final field test more effective, as trainers can focus their supervision and feedback on the actual fieldworkers, and as field teams can practice in their anticipated compositions and adjustments can be made if necessary. In some instances, it might be necessary to dismiss some trainees prior to the final selection, e.g., if they disrupt or slow down the training. If doing so, make sure to only dismiss trainees who clearly have unacceptable skill levels or personality traits (e.g., unreliability). Be careful not to prematurely drop trainees that might improve. Many need more time or start to improve once they realize that their relative performance levels are low. Often, slower trainees end up to be among the better interviewers. If you are training specialized fieldworker roles in parallel training streams, it is advisable to only select trainees into different streams after a few days of general training, once picture of trainees capabilities and personalities has emerged. For example, anthropometric measurers need to have good interpersonal skills to be able to handle young children and caretakers, but do not need to fully understand all concepts in a long household questionnaire. Enough trainees should be selected and trained in each stream to allow an independent selection process at the end of each training stream. 17.4.2 Criteria The fieldworker selection should be based on trainees capability to perform the role in the given survey, measured using objective criteria including test results, skill ratings or the passing of standardization tests. In some cases, other objective characteristics may be relevant such as spoken languages or gender to achieve the required field team composition. Do not select based on subjective criteria such as personal liking or criteria that are not immediately relevant such as having a degree or previous experience as fieldworker. How you weigh individual criteria highly depends on the project and training outcome and has to be decided on a case by cases basis. Do not exclusively rely on the overall test scores by simply selecting the trainees with the best scores. This would select based on comprehension alone and ignores other important skills and characteristics. The selection should be comprehensible and justifiable using the applied criteria. For example, if one trainee has been selected over another one who performed better in the tests, there should be objective reasons, such as higher scores in relevant key skills. You can decide to make the selection and criteria public to the trainees after the selection. 17.4.3 Selection process The following selection algorithm tends to work well for the final selection of fieldworker. You can adapt it to meet the requirements of your survey: Drop under-performers. First, discard trainees with unacceptable performance or skill levels. They should not be used as field workers unless they receive special training after the training to bring them to acceptable levels. Select all special roles among those who score highest in the required special skills. For example, select data quality control officers among those who have understood the material well, are very reliable and thorough. Select supervisors among those who have good leadership and organizational skills, but do not necessarily need to be top of the class in the test results if they are not involved in the data monitoring. Select anthropometrics specialists who did best in anthropometrics training. Select interviewers. Rank the remaining trainees based on test scores, skill ratings, and other criteria you might have and select from top. Often you might find that you have too few or too many trainees with adequate skill levels. Ideally you can adjust your field work model accordingly. If there are too few, only select those capable and use fewer or smaller teams for longer, or extend the training to bring them up to speed. If more trainees qualify than required, you can think of using larger or more teams for a shorter amount of time, but be aware that larger field teams are harder to monitor. Allocate selected staff into teams. Double check the teams balance well and adjust steps 2 and 3 if not. For team allocation, keep in mind any special requirements you might have, such as gender balance, languages, etc. Try to balance the skill levels within teams by mixing top and bottom of class, so stronger trainees can support and act as a reference for weaker trainees. Make sure teams know who should learn from whom, and that they should support one another. Also be aware of personal traits and group dynamics when allocating into teams. Keep reserve. Keep trainees who have not been selected but have acceptable test results and skill levels as a reserve to replace fieldworkers dropping out during field work. Inform them that they are on a waiting list and might be called should others leave. They need a short refresher training and strong supervision during the first few days to bring them up to speed. "],["introduction-1.html", "Introduction", " Introduction work in progress problems : one their own, far away, monitored, can be demotivating, little feedback, increasing idiosyncratic behavior leading to interviewer effects, measurement error "],["listing.html", "Chapter 18 Listing", " Chapter 18 Listing work in progress "],["monitoring.html", "Chapter 19 Monitoring", " Chapter 19 Monitoring work in progress "],["effective-feedback.html", "Chapter 20 Effective Feedback", " Chapter 20 Effective Feedback work in progress "],["avoiding-nonresponse.html", "Chapter 21 Avoiding nonresponse", " Chapter 21 Avoiding nonresponse strong revisiting protocols make it hard for interviewers, remove any incentives, have to give phone numbers confirm, etc follow up on non-repsonse "],["notes.html", "Chapter 22 Notes", " Chapter 22 Notes MICS has cluster control sheets first few days, trainers shoudl observe field work and provide field work. "],["introduction-2.html", "Introduction", " Introduction work in progress "],["data-editing.html", "Chapter 23 Data Editing", " Chapter 23 Data Editing work in progress ISCO codes, best done in batch by dedicated persons. "],["weighting.html", "Chapter 24 Weighting", " Chapter 24 Weighting "],["archiving.html", "Chapter 25 Archiving", " Chapter 25 Archiving work in progress "],["documentation.html", "Chapter 26 Documentation callout box examples", " Chapter 26 Documentation work in progress - report on Standard errors - ideally callout box examples Did you know that  this book is work in progress and in a pre-publish state. something long and longer ahhds oh yeahaaahjkl Did you know that  this book is work in progress and in a pre-publish state. something long and longer ahhds oh yeahaaahjkl Warning  this book is work in progress and in a pre-publish state. something longer Warning  this book is work in progress and in a pre-publish state. something longer and longer ajetztabera test Useful tools: a b c Useful tools: a b c DO NOT SHARE! The book is work in progress and in a pre-publish state. "],["abbreviations-and-acronyms.html", "Abbreviations and Acronyms", " Abbreviations and Acronyms CAPI Computer Assissted Personal Interviewing CATI Computer Assissted Telephone Interviewing DK Dont Know HIES Household Income and Expenditure Survey LSMS Living Standard Measurement Survey NSO National Statistical Office PAPI Pen-and-Paper Personal Interviews QA Quality Asssurance ToC Table of Content ToR Terms of Reference ToT Training of Trainers TSE Total Survey Error TSQ Total Survey Quality "],["references.html", "References", " References "]]
