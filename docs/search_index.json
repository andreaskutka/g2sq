[["preface.html", "The Go-to Guide to Survey Quality Preface About this book How to use this book About The LSMS Guideline series About the authors 0.1 callout box examples", " The Go-to Guide to Survey Quality Josefine Durazo, Kevin McGee, James Arthur Shaw, Andreas Kutka Preface Hello there. Were glad to see you However, you have come a bit early. This book is still work in progress and in a pre-publish state. PLEASE DO NOT SHARE. We look forward to seeing you back here in a few months, when everything is ready. Version 0.1. Last updated on 06 April 2022. About this book What is survey quality and how can it be achieved? This book gives practical advice expereinces and practices that have worked in. based upon our experience in designing and implementing large scale socio-economic surveys in a variety of contexts. recognises the multi-dimensionality of survey quality and follows the total survey error paradigm total survey quality optimization The book is targeted towards a range of survey practitioner roles. Survey designers, sponsors will find guidance on design decisions that are relevant for quality in chapters X and Y. They might also benefit from reading the introduction chapters to the following chapters. Survey implementer will find tips &amp; tricks and best practices in chapters Z and ZZ to maximize quality for all main steps of survey implementation. Individual sections provide details for individual key roles, including trainers, translators, etc. The book provides detailed steps and best practices for all survey phases. Survey practitioners are not encouraged to implement everything, as this would be beyond most surveys budget and time constraints, and as there might be alternative methods with equally good outcomes. Survey practitioners can use the detailed information for those components that form part of their quality assurance plan, or use the detail to reflect on or improve own practices. The book is a companion and go-to reference for survey practitioners aiming to improve the quality of the surveys they design and implement. The book follows the survey life cycle, and explaining the best practices and providing tips and tricks on how to achieve high quality surveys. The book is very comprehensive on each of the survey phases. Readers are not expected to implement all how-to-guidelines listed in the book. Instead, the book is meant to be a collection best practices to achive high quality in surveys. Survey practitioners are encouraged to evaluate which improvements can be implemented within the context and constraints of their respective survey. Follows the total survey quality framework, and touches base on the survey design, implementation and analysis. The book is written as companion for survey managers to help them take the right decisions, organize their team and works streams and how to obtain the right types of inputs in every phase of the survey to achieve high quality outputs. While providing a lot of detail in some parts, the book does not aim to be a comprehensive how-to-survey guide. Further guidelines and the input of experts are needed to achieve high quality surveys. The book is divided into 6 parts. Part I provides a short introduction of key concepts of survey quality. It is the only theoretical part of the book. Parts II - IV follow the different stages of a survey life cycle from inception, design, preparation, training, fieldwork to post-field. Each part is comprised of several chapters. The first chapter to each part is an introduction that outlines the content of the remaining chapters and provides a cheatsheet version of our key recommendations, called TL;DR. References for each chapter are added to the bottom. In the Appendix, you can find the list of Abbreviations and Acronyms, as well as a complete list of all references used throughout the book. The book is a living document that is being expanded and revised on an ongoing basis. The last update was made on 06 April 2022. We encourage How to use this book The book provides a lot of information and details that are probably difficult to absorb entirely and retain for the duration of the survey. For those who are managing one of their first surveys we recommend skim reading the document focusing on the introductions to each chapter and skipping the details. They summarize the key points and provide you with a general idea of what matters to achieve survey quality. Once the survey is underway, reading the relevant chapters prior to thinking about a phase will give you all the details you need at this point. Experienced survey managers who are curious about or aiming to improve certain aspects can read the relevant chapters only. The book is relatively extensive and contains a lot of detail. It is not written to be read in one go (we applaud your stamina if you do), but rather as go-to reference that can be visited during any stage of the survey to get relevant practical advice and learn about best practices. Chapters are written to be independent of each other. Links to other chapters are added to where relevant information is References are provided if they built upon other parts of the book or if more detail is provided elsewhere. Use the search function to quickly find all occurrences of a word or phrase. Open the search function by clicking on the search icon on top or by typing f on your keyboard. Use the up and down arrow keys to navigate between different search matches. You can edit the book! Yes, you read this correctly. If you come across anything that you think could be improved, from correcting typos to adding content to chapters, click on the edit icon on top. This will open the underlying file on github where you can make the modifications and submit them as a pull request (you will need a GitHub account). The book is written using Rmarkdown in bookdown. For most parts of the text, the syntax is following simple markup language, of which you can learn the basics in under 2 minutes. (For more experienced github &amp; Bookdown user, you can fork the depository and make comits). If accepted, your suggestions will be added with the next version. If you have any suggestions but prefer to not edit the book, please list an issue on GitHub. Thank you! You can share the book or any page you like using the sharing icons on the top right. Each chapter and subchapter have specfic links. Copy the link, either by right clicking on the blue # icon next to the heading and copying the link address, or by clicking on the (sub)chapter in the table of content and copying the url from the browser. About The LSMS Guideline series The LSMS Guidebook series offers information on best practices related to survey design and implementation. While the Guidebooks differ in scope, length, and style, they share a common objective: to provide statistical agencies, researchers, and practitioners with rigorous yet practical guidance on a range of issues related to designing and fielding high-quality household surveys. The series draws on experience accumulated from decades of LSMS survey implementation, the expertise of LSMS staff and other survey experts, and new research using LSMS data and methodological validation studies. About the authors Josefine Durazo  Kevin McGee  James Arthur Shaw  Andreas Kutka  0.1 callout box examples Did you know that  this book is work in progress and in a pre-publish state. something long and longer ahhds oh yeahaaahjkl Did you know that  this book is work in progress and in a pre-publish state. something long and longer ahhds oh yeahaaahjkl Warning  this book is work in progress and in a pre-publish state. something longer Warning  this book is work in progress and in a pre-publish state. something longer and longer ajetztabera test DO NOT SHARE! The book is work in progress and in a pre-publish state. "],["introduction-to-survey-quality.html", "Chapter 1 Introduction to Survey Quality", " Chapter 1 Introduction to Survey Quality Sample surveys are a crucial data source for official statistics, impact evaluations and social economic research. Survey data informs public policy and other important decisions. Quality in surveys is paramount for good policies and sound decisions and should be the aspiration of all survey practitioners. In Part I of this book we summarize key frameworks used by survey methodologists and national statistical offices (NSOs) to conceptualize and maximize quality in surveys. Skip this part if you are not interested in the theoretical background of survey quality. We will link back to the concepts throughout the remainder of the book, so you can read only relevant parts. Quality can be defined as fitness for use. For something to be fit to use it must be free from deficiencies and respond to the users needs (Juran and Gryna 1980). In a survey context, freedom from deficiencies translates to the requirement of survey data to be accurate to allow inferences about the target population. In other words, a survey statistics must accurately describe the population for it to have any value. Responsiveness to users needs translates into aspects such as timeliness, usability and completeness that are important to the user of survey data. Even the most accurate data can be unusable if it is outdated, poorly documented or not rich enough for the intended analysis. Which user attributes matter, depend on survey. As we can already see, survey quality is a complex, multidimensional concept. In Chapter 2 we look at Total Survey Error (TSE). It is the dominant framework used by survey methodologists to maximize the accuracy of survey estimates within survey constraints. We describe different sources of survey error and provide real-world examples for each error source. We explain how error affects survey statistics and how the TSE can be used as a planning criterion to reduce overall error in survey estimates. In Chapter 3 we introduce the notion of Total Survey Quality (TSQ) that combines survey accuracy with user-specific dimensions of quality. The concept is the basis of multi-dimensional survey quality frameworks that are now used by many national statistical offices as a planning and evaluation tool. We present a general strategy how TSQ can be used to design surveys that maximizes survey quality. The chapters are based on the articles by Biemer (2010) and Groves and Lyberg (2010), and the books by Biemer and Lyberg (2003) and Weisberg (2005). They are good starting points for readers who are interested in learning more about the theory of survey quality. References "],["tse.html", "Chapter 2 Total Survey Error 2.1 Error sources 2.2 Survey-related Effects 2.3 The Impact of Error 2.4 Minimizing TSE", " Chapter 2 Total Survey Error In the survey context error refers to the deviation of a survey response from its underlying true value. Probably the best-known error type is sampling error that occurs when collecting information from a sample of the population rather than the full population. Early in the history of probability sample surveys, it was recognized that there is a variety of non-sampling error sources that can equally cause a survey statistic, often referred to as survey estimator, to deviate from its underlying true value in the population, referred to as population parameter. Over time, this knowledge evolved into the concept of Total Survey Error (TSE), which has become the dominant paradigm in the field of survey methodology Groves and Lyberg (2010). TSE acknowledges that error can arise from many different sources. For example, error might be introduced by researchers formulating questions wrongly, interviewers not probing sufficiently or respondents unwilling to respond. Error may occur at any point in the survey process, including the design, sampling, implementation, processing or analysis, and may be caused by any actor including researchers, interviewers or respondents. TSE denotes the aggregate of all error sources in an estimate and is thus the difference between a survey statistic and the population parameter. A good presentation of how TSE components link to the process of generating a survey statistic has been made by Groves et al. (2004), which is shown in Figure 2.1. Figure 2.1: TSE Components Linked to Steps in the Measurement and Representational Inference Process 2.1 Error sources The TSE framework decomposes survey error into different error sources. Survey methodologists have produced a number of similar classifications. They divide TSE into sampling error and non-sampling error, that in turn can be further decomposed. We follow the classification developed by Biemer and Lyberg (2003). We briefly introduce each error source and provide real-world examples that we have observed in our work with NSOs, international organisations and survey firms. 2.1.1 Sampling error Sampling error describes the error that occurs if a sample of the population is surveyed, instead of surveying the entire population, i.e. conducting a census. It is determined by the sampling scheme (simple, multi-stage, stratified, ), the sample size and the choice of estimator. If probability sampling is being used, the sample error is randomly distributed and can be mathematically computated. It is probably for that reason that sampling error is receiving a disproportionate high level of attention by researchers and survey designers. By definition, sampling error occurs in every sample survey. Sampling error becomes problematic if it is unnecessarily large due to faulty or inefficient sampling design or is minimized at too high expenses of other error sources. If non-probability sampling methods are being used, sampling error may also introduce bias. Examples include: The so-called Random walk is an oxymoron. Interviewers can easily tweak the sequence in which they list units, so that respondents are selected that meet certain characteristics, such as being available for an interview or being friendly. Random walk is a not a probability sampling method and introduces bias. Unfortunately, some surveys still use it for a fast and cheap second-stage sampling. A HIES used convenience sampling to select individuals within collective living quarters as respondents were unable to list all individuals living there. Interviewers selected whoever was available during their visit, effectively removing individuals from the sample that were unavailable during working hours, thus introducing bias. A probability sample method could have been used instead, for example, by listing all lockers or beds or other items associated with individuals, randomly selecting items from the list and putting effort into interviewing the associated individuals. A survey part of an impact evaluation study selected a varying number of households per community, depending on the community size. In the largest communities, up to 40 households were interviewed. While this sample design does not increase sampling error nor introduces bias, it is inefficient. Beyond 20, the marginal decrease of sampling error per additional household per community is negligible. The resources spent to interview them could have been used to decrease error from other sources instead, for example by extending training or increase monitoring. 2.1.2 Specification error Specification error (referred to as Validity in Figure 2.1 occurs when there is a misalignment between the concept a survey intends to measure and the concept that is actually being measured. As a consequence, the wrong parameter could be estimated by the survey and invalid inferences be made. It is often caused by insufficient definition of concepts, poor questionnaire design, lack of cognitive testing, absence of good manuals, or concepts not being preserved during questionnaire translation or CAPI scripting. Examples of specification error are: Surveys that aim to capture income from any kind of employment sometimes ask opening questions like In the past , did NAME work as an employee for a wage, salary, commission, or any payment in kind?. Designers and data users assume that this question captures informal jobs. Often however, respondents understand this question to ask about formal employment only and answer with No, even though they have worked as day laborers for a few days. The question does not capture the concept of employment as planned by designers or interpreted by analysts. Researchers or questionnaire designers developing questions without taking into consideration local circumstances or testing that underlying concepts are understood the same way in the population. For example, questions around saving behavior tend to be delicate and highly dependent on economic circumstances. Researchers assuming that goods bought on credit are included in a loan roster, while the questionnaire did not explicitly probe for them and trainers did not train interviewers to include them. Mistakes that happen when updating translations or instruments. last-minute change is made to the questionnaire during the training, in the local language that never finds its way into the original version in the design language which is being used as data reference. Data users will interpret the question differently to how it was being asked. Household to be translated as family in the local language, referring to a different set of people. Household refers to the economic unit and does not necessarily imply family ties. Many languages lack a word for household. An expression like those living with you would often be more appropriate. Sometimes, CAPI scripting can alter the nature and functionality of questions or modules to such a degree that it affects the way they are being administered and ultimately changes results. Researchers who do not observe the CAPI questionnaire being fielded remain unaware of such differences. For example, time use modules are often designed as a grid of activities and hours of the days into which lines are drawn for the primary activity done for a certain period. We have seen this module being scripted in CAPI as series of 48 questions asking the respondent for the main activity for each 30-minute interval. The responses produced are not at all comparable. DHS and Is it OK to beat your wife question NOTE: ARTHUR TO WRITE Different household definitions, different results? 2.1.3 Coverage error Coverage error or sometimes frame error occurs if the sampling frames that are used to select the survey sample include units that are not part of the target population, exclude units that are part of the population or duplicate them an unknown number of times. It also refers to errors in auxiliary variables associated with the frame units, or if they are missing. Frame error can cause parts of the population to be under- or over-covered, distorting the sample or impact evaluation design. Since building perfect frames is often practically not possible, most surveys suffer from frame error to some degree. More severe cases of frame error often stem from a lack of quality assurance of listing exercises, the use of unverified lists compiled by third parties, or the use of outdated lists. Examples are: During listing exercises listers sometimes focus on the population-dense village centers, as it is quick and easy to list a large number of households, but omit households that live further away or are difficult to reach. As a result, remote households are under-covered in the frame. Community lists are usually being used as frame for first-stage sampling in household surveys or to inform impact evaluation design. In some contexts, these lists can be of very poor quality. We have experienced field teams searching for non-existent communities or that community characteristics grossly mistmatch the auxiliary variables used for sampling or matching. A survey for an impact evaluation in Perú used household lists for the second stage sampling in treatment areas that had been compiled by the project to be evaluated. Several local dynamics played into the creation of the lists. As a result, in many villages, several members of one household were listed as separate households on the list, friends and family of project officials from outside the village were included, or beneficiaries from the village excluded. 2.1.4 Nonresponse error Nonresponse error can occur on the unit level, when a sampled survey unit (e.g. household, individual, firm, etc.) cannot be interviewed for any reason, or on the item level, when parts of the questionnaire remain unanswered because the respondent did not answer some questions for any reason. Nonresponse error can severely affect the validity of survey data. Since the reasons for nonresponse are hardly ever randomly distributed, the actual respondents may no longer be representative of the population, causing the survey estimates to be biased. Nonresponse error has been plaguing survey methodologists in the global North, where nonresponse levels generally have been very high over the past decades Surveys in the global South have so far largely been spoiled with very high response rates, but nonresponse error is increasingly becoming an issue, especially in urban areas, where availability and willingness to participate in surveys is decreasing. While various approaches have been developed that aim to correct for nonresponse error, the best way to deal with it is to put solid measures in place to avoid it as much as possible. QUOTESNEEDED. Examples of nonresponse error are: Survey teams visiting communities during working/market hours and replacing unavailable households without making sufficient revisiting attempts on different days and hours. In a survey in urban South Africa, many interviewers were scared of dogs and replaced households if they owned a dog. Dog ownership was (initially) not observed and highly correlated with other household characteristics. The below average probability of Donald Trump supporters to respond to interview requests is thought to be the main reason that surveys systematically underestimated the support for Donald Trump ahead of the 2020 US presidential elections. In a COVID response phone survey, interviewers were unable to call those panel respondents who had not paid their phone bills and had their line (temporarily) cut off. Long questionnaires that cause respondent fatigue and lead to high rates of uncomplete interviews, affecting the last sections of the questionnaire. High Dont Know (DK) rate for income related question if interviewers do not probe and explain the question sufficiently. 2.1.5 Measurement error Measurement error often is one of the most damaging error sources in a survey. It occurs if the recorded value is an inaccurate measure of what was to be measured. They can be due to the interviewer, the respondent, the questionnaire, protocols or their interaction. They might be intentional or unintentional. Respondents might misinterpret a question, struggle to recall some information or deliberately give a wrong response. Interviewers might administer a question incorrectly, misunderstand questions or answer option, record typos, falsify responses or cause measurement error in many other ways. Poorly designed questionnaires, ambiguous questions, underdefined concepts often are big contributing factors to measurement error, as can be the interviewing mode. Some interviewers in an LSMS survey confused KG and Gram and selected the wrong answer option in the consumption unit. As a result, the food quantities they recorded were off by a factor of 1000. Respondents who purposefully under-report household members, crops, livestock or other items, knowing that each item they report will entail a long set of follow-up questions. Interviewers recording responses against the wrong questions or for wrong items due to confusing CAPI design that fails to provide them with a good overview of the questionnaire. A school survey conducted teacher interviews and recorded teacher attendance in classroom. Reason for absence was not recorded and no protocol was put in place to ensure both exercise were conducted during different hours. In a follow-up survey that corrected for both problems, 8% of the teachers were absent from classroom for being interviewed at the time. Agricultural surveys trying to capture very detailed information on the parcel-crop-season level can be too detailed for respondents to recall, or for interviewer and respondents to get lost in the conversation. 2.1.6 Processing error Processing error refers to any error occurring post-interview including error in data entry, editing, formatting and labeling, construction of indicators, calculation of survey weights, or tabulation of results. It is often caused by unclear interview rejection mechanisms, data editors who are not qualified enough, improper keeping of records or change logs. Examples of processing error we have observed are: Interviewers fixing issues in rejected interview files without recontacting the respondent or obtaining the correct answer. Wrong or non-systematic outlier detection, such as manually summarizing variables in statistical software and looking at the 5 highest and lowest values only. A NSO replaced the outlier values in any variable with its median value prior to data publication. Not correcting for changes to the instrument when appending the data from different versions. For example, f there has been a change to answer options or item lists, this can cause value labels to be assigned wrongly for parts of the sample. Wrongly label value codes, such as household assets. 2.2 Survey-related Effects Conducting a survey in the way it is being conducted, in its context, has itself effects on survey statistics. While these effects are often implicitly included in TSE and referred to as error, Weisberg (2005) uses the term survey-related effects and explicitly includes them into the framework. Examples of survey-related effects are: Interviewer effect: Interviewer characteristics such as mannerism, tone of voice, gender, ethnicity, etc. have shown to influence the type of response they elicit. This effect on survey statistics is not due to any mistakes or wrongdoing by the interviewers, and will exist for any survey using interviewers. Mode effect: Administering a question in face-to-face interviews can produce very different response patterns compared to asking the same question in telephone interviews or self-administered interviews. This, for example, can be due to respondents attitudes being different if they receive an interviewer at home (guest) to being called by phone (solicitor/marketer). In multi-mode surveys or with modality changing between rounds, mode effect can cause significant comparability issues. Questionnaire-related effects: Answers patterns can differ with the exact question phrasing. As long as there is no specification error, there is no question phrasing that is more correct than others. Similarly, the order in which questions are asked can change outcomes. Yet, they have to be aske in some order. The same is true for the phrasing and order of answer options. A well established example is the recency effect in phone surveys, where respondents select the last answer option because they remember it best. While some of the survey-related effects can be minimized, e.g. through randomization of question or answer option order or careful field team planning, they cannot ever be eliminated completely as they are an inevitable product of a survey itself. For example, since a survey requires interviews to be conducted somehow, mode effect will always exist in a survey. In some cases, it is possible to study and quantify effects in experimental design and to make ex-post adjustments. This, however is costly and requires provisions in the survey design. In practice, usually the best way to deal with survey-related effects is to be aware of them and keep them in mind during survey design and analysis. 2.3 The Impact of Error Sample surveys aggregate individual responses to obtain statistics for the sample, referred to as survey estimator, often with the aim to infer corresponding values in the population, referred to as population parameters. Survey error causes the survey estimate to deviate from the population parameter, diminishing the accuracy of the inferences derived from the survey data. In other words, with high levels of error a survey does not accurately describe the reality of the population. Error can affect survey estimators by increasing the variance of a variable or by introducing bias. For a survey estimator to be accurate it has to have a small bias and variance, which only happens if the TSE is low for the estimate. The ways in which error affects an estimate depend on whether the error is random or systematic, and whether it is uncorrelated or correlated. Error is considered random if it does not follow any pattern. For example, if respondents have difficulty recalling an item, some respondents might give higher values while others provide lower values. Across all observations, the error would have a mean of zero and would not affect the mean value of the variable. However, random error increases the variance of a variable, which in turn reduces the reliability of a survey estimator. The higher the variance, the higher the probability that the estimator would be different if the survey were to be repeated under the exact same conditions. The results would be less reliable. Increased variance furthermore reduces the magnitude of correlations with other variables and the statistical power in hypothesis tests. If error contains a systematic tendency we speak of systematic error or bias. As an example, if interviewers tend to under-report household members to shorten interview duration, the survey estimate of household size will underestimate the actual household size in the population. In statistical terms, the sample mean would be a negatively biased estimator of the population mean. Since systematic error directly affects the mean of a variable, it reduces the validity of the estimator, in other words the estimator is not accurately measuring what it is supposed to. If the systematic error is not constant, it may furthermore increase the variance of the variable, also reducing the reliability of the estimator. Figure 2.2 illustrates how variance and bias relate to the reliability and validity of a survey estimator. Imagine the midpoint of each image to represent the true population value and the black points to be the recorded responses. The points in A are scattered around the midpoint with no particular pattern, the error is random. We have high variance but no bias. The estimator is valid but not reliable. In B, the answers are offset in the same direction and by the same distance. The error is systematic, so there is bias, but variance is small. The estimator is reliable, but not valid. C shows systematic error that is not constant. We have high variance and bias. The estimator is neither valid nor reliable. In D, there is little error, we have low variance as answers are relatively close to the actual value, and there is no bias as the deviations show no pattern. The estimate is valid and reliable. Figure 2.2: Validity and realiability in survey estimators The magnitude of the effect of error depends on whether the error is uncorrelated or correlated. Error is uncorrelated if the error for different units (e.g. households) is unrelated. An example would be occasionally typos by interviewer when recording answers, if typos do not occur more frequently with certain types of households or some interviewers. Above discussion on the effects of random and systematic error was based on uncorrelated error. As we saw, the increased variance and bias can have serious effects on statistics. If the error for different units is related, we speak of correlated error. For example, interviews conducted by one interviewer who has misunderstood a question and administered it wrongly will contain the same error, while those of another interviewer may not. Correlated error has much more damaging effects on estimators as it multiplies the variance of a variable. Biemer and Lyberg (2003) show that only a moderate intra-interviewer correlation of \\(\\rho_{int} = 0.03\\), can result in an interviewer design effect \\(\\mathit{deff_{int}}\\) as large as 2.47. In other words, the intra-interviewer correlated error alone can cause the variance of a variable to be increased by 1.5 times. Error is typically also correlated for other survey roles, such as supervisors, data monitors or editors. One data monitor, for example, might thoroughly review interviews and provide useful feedback , reducing mistakes and measurement in their field teams while another data monitor only glancing over interviews will miss many of the issues and fail to reduce measurement error. 2.4 Minimizing TSE The TSE framework implies that the TSE affecting a survey estimate should be minimized in order to maximize its accuracy. Two points are important to note here. First, the total survey error needs to be minimized, that is the aggregate of all error sources. Errors from different sources do not occur in isolation, but are interdependent as they are all part of the overall survey process. Minimizing error from one source may negatively affect error from other sources. As an example, while increasing the sample size will reduce sampling error, it may not be a good overall strategy to increase the precision of a survey estimator. A larger sample can require more interviewers to be trained and monitored, increasing other non-sampling error types that can outweigh the gains from reduced sampling error. Another example can be complex parts of a questionnaire, such as a time-use module, that have been optimized for PAPI, but may cause significant error if implemented in CAPI without being carefully adapted to the functionality of the CAPI package being used. Understanding how decisions in one phase affect error in another requires a comprehensive view of the entire survey process. In order to effectively minimize TSE, different survey phases and components have to be integrated to operate as as a coherent whole. The second point to note is that TSE is to be minimized, meaning to be reduced as much as possible within survey constraints. All surveys face constraints such as the available budget, time and human resources or ethical considerations. Attempting to eliminate all error would exceed the constraints of any survey. Even with unlimited resources, some error sources could never be eliminated fully, such as respondents unwillingness to disclose some information causing item non-response. Instead, surveys should strive to avoid the most damaging errors and control others to the extent that they are mostly inconsequential and tolerable. How can survey practitioners prioritize which errors to address? A key challenge is that it is very hard to quantify survey error or its non-sampling components. This is only possible if the underlying true population value is known or if methodological experiments can be built into the survey design, which is an option for most surveys. In practice, minimizing TSE requires a detailed understanding of the main causes of survey error, their relative impact on accuracy, means to control them and the required effort. This knowledge is often anecdotal and based on experience in survey implementing institutions or individuals who have honed their error awareness, identification and correction over time. While there are some scientific publications on this topic, they tend to be theoretical or isolate and a address a single error source. Survey practitioners involved in the day-to-day implementation tend to not publish their practices. References "],["tsq.html", "Chapter 3 Total Survey Quality", " Chapter 3 Total Survey Quality As we have seen in Chapter 2, low levels of TSE are necessary for a survey estimator to be accurate. While accuracy is necessary for quality, it is not sufficient. Survey data must also respond to user needs in order to yield useful results. Data users often take data accuracy as given, assuming that it has been controlled by data producers. Instead they prioritize properties such as data being rich in detail, easily accessible and well documented. Data that is not sufficiently detailed, difficult to access or hard to interpret may be unfit for use and be perceived to have low quality by data users, even if being accurate. The concept of total survey quality (TSQ) considers the fitness of use of a survey estimate. It combines the accuracy of an estimate with non-statistical dimensions oriented towards the data user. The definitions of survey quality used by most NSOs in Europe, North America and Oceania, as well as that of international organisations such as Eurostat, IMF, and OECD explicitly acknowledge the multi-dimensional nature of survey quality. These definitions are referred to as survey quality frameworks. There is some (often minor) variation between organisations, but most frameworks include a subset of the quality dimensions summarized in Table 3.1 (Biemer 2010). Table 3.1: Common Dimensions of a Survey Quality Framework. Dimension Description Accuracy Total survey error is minimized Credibility Data are considered trustworthy by the survey community Comparability Demographic, spatial, and temporal comparisons are valid Usability/Interpretability Documentation is clear and metadata are well-managed Relevance Data satisfy users needs Accessibility Access to the data is user friendly Timeliness/Punctuality Data deliveries adhere to schedules Completeness Data are rich enough to satisfy the analysis objectives without undue burden on respondents Coherence Estimates from different sources can be reliably combined These frameworks are commonly used as the basis of survey quality reporting. Some of the dimensions are qualitative, making it difficult to generate a single metric to summarize the overall quality of a survey. Survey methodologists have not (yet) put forward a standard quality measure. Instead, evaluations tend to identify the strength and weaknesses of a survey by dimension and assess how well it achieved the goals of each dimension. Another very useful application of survey quality frameworks is as a design principle to maximize the TSQ in a survey. References "],["introduction-to-inception.html", "Introduction to Inception", " Introduction to Inception make a good plan, and adjust throughout the project. work in progress optimal survey design, reduces error make a plan to survey quality maximization, e.g minimize TSE while keeping the other dimensions acceptable, Biemer 2010 less can be more. A reduced questionnaire doing the key variable sof interest well can e better than trying to do many and ending up doing them worse. Fewer things to train, understand and monitor. Likewise, a smaller sample with a higher response rate might be more valuable. if there are too few resources available to do the survey well, try to either obtain more or question the feasibility of the survey. plan holistically. Efficiently allocating resources to maximize survey quality requires to look at the big picture and treat each activity as part of the whole that they are as each activity affects others. Some activities might be cut that have little or no effect on survey quality, to save resources for others where it matters. E.g. to afford a longer interviewer training one might reduce field team size and extend the duration of field work, if there are no time constraints or seasonality issues. good plan: to achieve timeliness. Overlap activities that can be done together. For example, with CAPi survey and a fixed data output format you can start working on the data documentation, cleaning, or even to some extend the analysis while the survey is still in the field. Do not overlap activities that depend on each other. For example, translation of instruments should only start once the instrument and CAPI coding have been finalised. Keepign track of change in overlapping activities is cumbersome, and leads in most cases to mistakes and survey erros. make gantt of key activities and draft timeline list of depending activities, e.g. only do training after instrument has been finished. Draw upon the experiences from other surveys in the same context, subject or modality. While every survey tends to be a different, the effect can often be generalised. In other words, what worked in one survey, often works in another survey with similar parameters. Make sure to have enough survey expereince in your team! "],["schedule.html", "Chapter 4 Schedule", " Chapter 4 Schedule Make sure your schedule keeps into consideration and works around the following forseeable events: Public holidays and vacations that might limit field workers or respondents availability or affect results, e.g. increased household expenses in the week of Christmas or Eid. Seasonality effects, that need to be catered for, such as such as harvest periods or that might affect your results. Weather patters such as rainy seasons that might limit field work operations. Allow margin to cater for unforseeable events that might affect your survey, such as abnormal weather events, unexpected field worker attrition, strikes or demonstrations "],["formulation.html", "Chapter 5 Formulation", " Chapter 5 Formulation work in progress "],["personel.html", "Chapter 6 Personel", " Chapter 6 Personel work in progress remove field team effects. As shown in Chapter LINK INTERVIEWER, interviewers, supervisors, data monitors or any other level can have huge effects on the TSE. As an example, interviewers of one supervisor who thoroughly back-checks interviews with respondents will be careful to not under report household members, parcels etc, while those of a more lenient supervisor may notice that they can get away with under reporting. that is centraize processes, so they can e monitored and streamlined, to remove supervisor, or data editor effects. Examples are: - instead of reviewing interview files in field y each supervisor, let them do only the basic completion checks, and do in depth review and feedback of the interviewed by a central team of editors, that regularly debrief, receie rotate staff, so that not always the same person enters data , reviews interviews, provides feedback to one interviewer only. review the work of each data monitor, editor, superviisor, for data entry, rotate data entry staff "],["contracting-considerations.html", "Chapter 7 Contracting considerations", " Chapter 7 Contracting considerations work in progress "],["introduction.html", "Introduction", " Introduction work in progress "],["instruments.html", "Chapter 8 Instruments 8.1 Questionnaire 8.2 CAPI/CATI 8.3 Translation", " Chapter 8 Instruments 8.1 Questionnaire discourage Dont Know responses. Dont make them very obvious by adding them to all questions. They are a tempting solution for iinterviewers. There are different types of Dont know, often they are related to respondents not makeing an effort to recall something. Intervieers should be encouraged to get respondents to answer. Only add to questions where you accept them. Instead use special codes that are displayed once at eginning of the questionanire and trained to intervieiwers. 8.2 CAPI/CATI 8.3 Translation Many questionnaires are designed in one language but fielded in one or more other languages. The way questionnaires are translated affects results, see e.g. Seo, Chung, and Shumway (2014). Interviewers speak the language and just translate on-the-go. is a big source of measurement error and interviewer effects, so make sure to translate your instruments! Using CAPI/CATI makes it easy to provide the questionnaire to interviewers in different languages. One can normally switch language within the questionnaire. If managed well, even translating to multiple languages is neither too much effort nor very costly, and a low-hanging fruit to increase survey quality. Translations done badly can quickly spiral out of control, causing tremendous amounts of work and potential mistakes. 8.3.1 What should be translated? Into which languages should I translate? Translate into all languages in which a significant proportion of the sample will be interviewed. The size of the proportion depends on the context and available resources, but can be as low as 5%. Only translate into languages that can be read fluently by interviewers. Some local languages are spoken only or have no spelling convention, making them very hard to read. Do I have to translate the whole questionnaire? Always translate question text, answer options and instructions to the respondents, as they are being read out to the respondent, or help the interviewers record their response. Translate interviewing-facing parts such as interviewer questions, instructions or warning messages if the field teams are more comfortable using a language over the design language. If they are fluent in the design language, there are only marginal benefits. Keep questionnaire structure such as names for screens, sections, rosters or the outcome variable in the design language or translate to a common language. Having a common reference makes training, management and feedback easier as everyone is literally on the same page (e.g. Parcel Listing). Add interviewer variables at the end of the instrument to record the main language in which the interview was conducted, if an interpreter was used and what the level of understanding was. 8.3.2 Who should translate? There are a few professional firms and translators that specialize in survey translations and cover a range of common languages. For many local languages these are unfortunately not an option. A sound translation of survey instruments requires translators to: be fluent in the design/source language be native speaker in the target language have survey field experience (to know survey expressions and ways to phrase questions) have good understanding of the subject (e.g. WASH, health, education) local contextual knowledge (to know how things are referred to locally) This set of skills and experience is rarely combined in one person. Translations from professional translators can sound too formal or bulky, while translations from field staff might miss the essence of a question or a construct. In the absence of professional survey translation services, the best options are often local consultants or experienced field workers who have worked with similar types of surveys before. Ideally, translation is done by a group that combines the above listed experiences and receives input where they lack, e.g. to correctly translate main toilet types or drinking water sources. It is useful for questionnaire designers to work through the instruments with the translators to make sure complex or nuanced parts and constructs are understood. A common communication channel (e.g. Email to all, WhatsApp group,) for translators to ask questions and for designers to send clarifications to all translators is important to ensure consistent translations into multiple target languages. 8.3.3 Some translation guidelines Follow below guidelines when translating survey questionnaires. Share this list with the translation team. Preserve meaning. Literal or close translations are often inadequate, especially if the target and source language and culture are distant. Find the best way to express the same meaning in the target language. Be precise. Use expressions to describe a construct if there is no word for it in the target language. E.g. use those who live with you instead of family if there is no word for household Dont omit words that provide any type of reference, such as on average,in total, main. Keep it simple. Use language and expressions that are easy to understand for all respondents in the sample. Make sure the same parts of questions are stressed. E.g. in English, the reference period is put at the beginning of a sentence to highlight it. In other languages this might be elsewhere. Adjust to different customs and culture. Direct translations might sound overly positive, negative, polite, impolite, etc. E.g. you might have to drop or add the word please. Be consistent within and across instruments. Use the same words or expressions to translate one concept in the source language. Check consistency by searching for key expressions in the translation sheet (e.g. Ctrl+f household) and checking they have been translated the same. Use established (good) surveys as references. They have already been tested and fielded in your context and often are a useful source of nomenclature. Preserve formatting, it carries meaning. Whatever is bold, underlined or UPPERCASE in the source language should be the same in the target languages. Preserve code. CAPI/CATI text sometimes contains dynamic parts that are written in code and must not be translated, e.g. piped text in Survey Solutions %MEMBER% or &lt;html&gt; tags. Use the custom Stata command sursol transcheck to make sure all Survey Solution formatting and code is correct in the target languages. Stay local. More widely spoken languages such as Spanish or Arabic can differ significantly between countries in how they call or express certain things. Use local translators, and review and adapt if using (parts of) questionnaires from other countries. Use the CAPI/CATI or a paper questionnaire in the source language on the side as guidance. Translations will depend on the context that is often not given when working in translation sheets. Give feedback if you think the source language needs updating and get clarifications if you are not sure how to interpret something. For more practical translation guidelines, see Chapter 12 of the ESS Translation Guidelines (European Social Survey 2018). 8.3.4 Translation verification Verify translations of your instrument. Despite being much-cited and persistently being added to ToRs, back-translations are not a great means to verify translations. Most big organizations such as the European Social Survey (2018) or the US Census Bureau (Pan and Puente 2005) that run multilingual surveys and put a lot of effort into comparative survey design use and recommend the TRAPD method (Translation, Review, Adjudication, Pretesting and Documentation). It involves reviewing and comparing (at least) two independent draft translations and agreeing on one translation that is cognitively pre-tested. Insights from the pre-test feed back to the translation, and the whole process is documented. Translation, review and adjudication are normally done together by a team that combines translators, survey field experts and content subject experts. A team-based approach deals better with mistakes, idiosyncratic interpretations and translator blind spots, compared to individual translators working on their own. Surveys with tight budgets and time constraints can use a reduced version of TRAPD to verify translation. Save yourself the back translation. Instead, use independent translator(s) to systematically review all rows in your translation sheet. The reviewer(s) either approve a translation or suggest an alternative translation in a new column. Translators and reviewers discuss all discrepancies and choose a final translation. Using a translation management system as described further below, translation and review processes can overlap and do not necessarily add much time to survey preparations. Make sure to desk test your translation in the CAPI/CATI before the pre-test. Often, the context, the way questions are displayed in the interface or the sequence require translations to be updated. As a last defense, verify the translation during the training. During the questionnaire review part, project the questionnaire and read out loud each question in the design/common language. Trainees follow on their devices in the local languages they are fluent in and are likely to use in the field. For each question stress the meaning of the question and check with the trainees if the translation is accurate. Assign one person to make updates to the translation sheet on the go based on feedback from the trainees. Keep note of contentious items and address them with selected trainees on the side or after the session in order to not delay the training. 8.3.5 When to translate Translate a questionnaire too early, and juggling the inevitable many updates can quickly overtake your work and introduce mistakes. Translate too late and the instruments are not verified, poorly translated or simply not ready in the local languages at the time of the pre-test, destroying one of its main purposes. In the preparation phase, carve out enough time prior to the pre-test for translation and verification. Try to complete the questionnaire before that, so that it only undergoes minimal change once translations have started. Translating a normal socio-economic questionnaire can take several days, and so does the verification process. If there is little time, you can use a staggered approach to provide enough time for translation. Translators can already translate and verify completed sections while the remaining sections are being finalized or put into CAPI/CATI. Use a translation management system as described below to stay on top of the updates and avoid mistakes. If you are using a paper version to develop the questionnaire, do not directly translate the paper version, but wait for the electronic questionnaire to be developed. When building the CAPI/CATI tool, questions often have to be modified to function in the interface, especially around roster and dynamic question text. If already translated, these changes have to be made in many versions, and often in languages the questionnaire designer does not speak, making updates difficult and error prone (e.g. when trying to find the word NAME in local languages to replace it with dynamic text). It is a lot easier, faster and less error prone to translate and manage updates once the tool has been converted to CAPI/CATI. If you require translated paper versions e.g. for training or archiving purposes, create them using your translated CAPI instrument. Often this can happen during or after field work, once the questionnaire does not change any longer, and during less work-intense parts of the project. 8.3.6 Stay on top of updates Lets face it. Most questionnaires will still undergo some change after the translation process has started. Done wrongly, late updates to the questionnaire can easily turn into a managing nightmare and cause significant undetected differences between the source and target languages. Done correctly, one can relatively easily stay on top of questionnaire modifications, even if there are a considerable amount of change and several target languages. Do not: Do not translate in the CAPI/CATI tool directly. They tend to hold only one additional text field for each language and have no means to let you manage or document the translation and verification process. Do not translate or store translations directly in translation sheets exported from CAPI/CATI, as they will be outdated with the next update to the questionnaires. You will inevitably end up trying to juggle different parts of the questionnaires on different translations spreadsheets for different languages. Do not work with offline versions of translation sheets sent back and forth by email. As above, this quickly becomes unmanageable. Instead: Work with an online spreadsheet such as Excel or Google sheets that holds all the translations and allows questionnaire designers and translators to simultaneously collaborate on the same live document. Define clear processes, how questionnaire designers mark rows that need to be translated, reviewed or updated, and how translators record the translation status of each row. Make sure designers and translators follow the processes to make the system work. Examples are: Add columns status (fixed set of answers, e.g. to translate, to review, to update, translated, reviewed, dont translate) and comments. Designers change the column status for rows they added or updated in the source language and provide details in comments if necessary. The first translator modifies the target language and sets status to translated, the reviewer to reviewed. At the end of the process, all rows should be in the final status reviewed. Designers add a special symbol that does not occur in the questionnaire or code (e.g. @) to the beginning of the target language column for rows that need to be translated, updated, or reviewed. Translators remove the symbol after updating the target text. Use conditional colour coding to highlight rows of certain status, and filtered views to generate custom views, e.g. a view for translators containing all rows they need to work on. Updating translations If you are working with Survey Solutions, use the translate_questionnaires STATA tool to keep your translation sheet in sync with the questionnaires and to produce the translation templates to be uploaded to the Survey Solutions Designer. The tool can be adapted relatively easily to work with other CAPI/CATI packages that export and import tabular translation sheets. For CAPI/CATI software such as ODK or Kobo that use spreadsheets to build the instruments, one can integrate the translation process into the same spreadsheet used to build the instrument by adding additional columns. Use an online spreadsheet to allow simultaneous access to designers and translators, and define processes as described above. 8.3.7 On-the-fly translation As described above, translate into as many languages as possible and try to avoid on-the-fly translations as much as possible. Sometimes however, on-the-fly translations are inevitable, e.g. if local languages cover small parts of the sample population or are non-scripted. If this is the case, make sure that interviewers have practiced translating the questionnaire in front of others prior to the field, and that there has been a group consensus on how to translate the questionnaire into the local languages. Both significantly improve the quality of the translations and can be implemented using group practice during the training. For group practices, group together trainees according to the local languages they are likely to use in the field. One trainee playing interviewer administers a question in the local language, those playing respondents follow on their tablets in the source language and make corrections or suggestions on how to improve the translation. Trainees should rotate roles such that every trainee has practiced translating the entire questionnaire to others speaking the same language. If you will have to work with local interpreters to translate interviews on-the-fly, make sure that interviewers explain well the key concepts such as household or parcel, and that they are understood by the translator. Make sure that you have budgeted for local interpreters if they are likely to be needed. References "],["sample.html", "Chapter 9 Sample", " Chapter 9 Sample work in progress "],["manual.html", "Chapter 10 Manual", " Chapter 10 Manual work in progress "],["introduction-1.html", "Chapter 11 Introduction 11.1 Key Recommendations", " Chapter 11 Introduction Longer Training reduces interviewer effect (quotation needed, see e.g. Fowler, Floyd J., and Thomas W. Mangione. 1985.) Few survey components affect data quality more than fieldwork training. Yet, unfortunately in many surveys, fieldworker training is neglected, under budgeted or implemented so poorly that there is little learning happening. No field work monitoring tool or exercise can fix a training that has failed to produce fieldworkers who are fully capable of accurately implementing the survey. This chapter describes how a successful interviewer training can be conducted. If you only have a few minutes, read the key points in subsection 1(LINK). If you are looking for more detail, read the relevant subchapters before you get to this point in the training preparation or delivery. If you are after a quick cheatsheet version, read our key recommendations on training in Chapter @ref{training-tldr}. Chapters X to Z are written with an in-person interviewer training in mind. Chapter 12 Planning and Preparing provides you with the details you need to plan and prepare a good interviewer training, including what content it should cover, when it should be held for how long, how many trainers and trainees should be involved and some check lists for logistics. Read this prior to budgeting and planning your training. Chapter Y gives details on how to conduct an effective interviewer training. It gives some general considerations and hands-on gives tips and tricks for the different session types such as questionnaire reading, on-side or in field practicing. It is targeted towards trainers and is best read prior to the training or before conducting a particular session type. As you will learn in this chapter, you should train an excess of interviewers, assess them on an ongoing basis and select them at the end of the training. Chapter @ref{trainingassessment} assessment gives important considerations for this process and step-by-step guide on how to do this. Chapter ZZ gives considerations for special training arrangements, including remote training, training of trainers or online training. 11.1 Key Recommendations This tl;dr is for readers who want to see our cheatsheet version of the chapters on training. If you are looking for more details, we think youll benefit from reading the rest of the chapters. Follow below key points to ensure a successful interviewer training. Train an excess of 20-40% of field workers and select them based on skills at the end of the training, even if working with experienced teams. Trainees take the training more seriously, pay attention, and learn and the overall skill level of field workers is higher. Continuously evaluate trainee performance and capacity in written tests, role plays, questions in class and observing them while practicing. Dont cut on duration! Allow for at least one full week of training per 1.5 hours questionnaire length. Anything less is normally not enough for the complex content of a socio-economic questionnaire to be learned and practiced. Keep it small. Try not to train more than 50 persons at a time, so you get to know the strengths and weaknesses of the trainees. Adjust field work to take longer to allow a smaller field work team. Make it interactive. Avoid one-directional presentations that overwhelm trainees with too much information. Involve trainees and frequently change the training modality. Practice. Frequent practicing is crucial for trainees to fully internalize the content and be able to apply in the field. Practice on-site and in the field. Be prepared. Make sure questionnaires, manuals and tests are truly ready prior to the start of the training. Have the venue fully set up and all admin and logistics sorted. Focus. Only teach what interviewers need to know to do their work well. Spend more time on things that are crucial to the survey. Instead of giving a 2hr presentation on the analysis of the data, give a short introduction and practice with interviewers how to introduce the study in the field. Enough trainers. Delivering good training is more than a full time job. Many things need to happen in parallel to the training being delivered. More trainers are usually better, and best if stakeholders, designers and analysts are among them. Engage. As a survey designer, do not do your emails on the side table and leave it to the survey firm, fieldwork manager or trainers alone to deliver the training. Actively participate, check that the content is correctly understood, explain what matters, etc. You have put a lot of effort into the design. Make sure it is implemented correctly. Trainers need expertise. Sending a junior colleague without much experience to be in charge of the training is not a good idea. You need solid survey, context and subject experience among the trainers to deliver a solid training. "],["planning-and-preparing.html", "Chapter 12 Planning and Preparing 12.1 Training Content 12.2 Duration &amp; Schedule 12.3 Material 12.4 Trainees 12.5 Trainers 12.6 Logistics", " Chapter 12 Planning and Preparing 12.1 Training Content To equip trainees with all the knowledge and skills necessary to do a successful job as interviewers, training must go beyond a simple discussion of the questionnaire and cover the following components: CAPI: Try to focus on the interviewer perspective and limit it to the functionality that is relevant for the interviewer in this study. Keep the theory to a limit and practice as much as possible. It is better to gradually introduce new functionality as needed rather than have an extensive theoretical session at the beginning. Questionnaire. Question types, questionnaire formatting, navigation, routing, understand all questions and answer options. How to administer each question and section. Underlying key concepts and definitions. Practicing. Repeated practicing, alone, in groups and with respondents is required, so that interviewers are fully familiar with the questionnaire content, know how to navigate it in the CAPI tool, can administer it correctly and get exposure to real respondents and scenarios during the training. Protocols. Exercises such as capacity tests, anthropometrics, plot measurement or behavioral games require strict adherence to protocols for comparable measurements. Trainees must fully internalize protocols and practice them until behavior across the entire field team has been homogenized. Assignment management. How to receive household assignments. How to find and locate households. When and how often to revisit. Replacement protocols. How to plan interviews and manage your workload. Interviewing skills. How to introduce yourself and the study as well as convince respondents to participate. How (not) to ask questions. How to not lead questions. How to probe, inquire, get clarifications. How to control the interview. Manage respondents time. How to deal with impatient respondents. Post-interview skills. How to check for completion. How to address in-consistencies. How to leave comments. How and when to submit files. How to receive feedback and respond to inquiries on submitted interviews. How to (not) make corrections. How to ask questions. A full dress rehearsal at the end of the training that mimics real field conditions and asks interviewers to track and interview households. 12.2 Duration &amp; Schedule The best timing for field worker training is just prior to the start of field work, as interviewers still remember the trained material and can solidify it by practicing it immediately in the field. The more time passes, the more interviewers develop idiosyncratic behavior, driving the interviewer effect. Usually, you need to cater for 1-3 days between training and field work to allow for admin, give a rest day to the team and for them to travel to the field. Should fieldwork be delayed beyond a few days after training, a short refresher training just prior to the field start is useful to refresh trained material. If the field start is being delayed further, a longer refresher training or complete retraining will be necessary. Schedule at least 1 full week (6 days) of interviewer training per 1-1.5 hr questionnaire time. To some readers, this might sound long, and many survey firms, especially when conducting opinion surveys will suggest much shorter training times. In most cases it is impossible to train the field teams to a higher standard in a shorter amount of time. Do not save on training days, unless you are very confident that you all trainees will have fully internalized the required material and be able to perform well as field workers. Socio-economic questionnaires are long and complex. They require interviews to understand a large number of questions and to fully internalize complex concepts and definitions such as employment types, or land management systems, etc., and to correctly apply those in the field. On top of that, interviewers need to learn and practice a range of skills such as convincing respondents to participate or continue, troubleshooting inconsistencies, responding to feedback from supervisors, etc. In many surveys, trainees are not ready to go to the field at the end of the training and the training needs to be extended last minute. If extensions have not been budgeted for, firms will have to cut corners elsewhere to compensate for increased costs. Avoid this by setting out with a realistic training duration and putting contingencies for potential extensions into the budget and timelines. Prepare a solid schedule and try to stick to it during the training. Track your progress on an on-going basis and make adjustments as early as possible if you are falling behind. A few consideration for drawing up a functional training schedule: Do not exceed 6-7 hours of training session per day and give frequent breaks without the breaks taking over the day. For example, a morning session from 8.30 AM - 12 PM, and an afternoon session from 1 PM - 5 PM, each with a flexible 15 minutes break. Frequently change training modality, e.g. by putting practicing sessions between questionnaire reading sessions. Avoid as much as possible long sessions of only questionnaire content, PowerPoint, theory only. Trainee attention levels tend to be lower after lunch and in the afternoon. During these times, schedule hands-on, more interactive sessions that require greater engagement from trainees, such as practicing or quizzes. Be efficient. Only train content that interviewers need to know. Allocate time on topics proportionate to the direct importance for the interviewers. Keep it together. Train material only in context, once it is needed and can be practiced. Example, practice How to introduce and ask for participation the day before the first field practice or agricultural key definitions when going through the respective questionnaire. Stay flexible. You could draw up a very detailed plan with exact timings for each session, but in reality the timing of sessions tends to shift a bit during the training. Use the schedule as guidance to track progress, but stay flexible to allow sufficient time to address issues, retrain topics, or dive into more detail as needed. Allocate time generously. It is better to finish training early one day rather than keeping trainees until late, being behind schedule or rushing content. If you are falling behind schedule during the training, extend the training as early as possible so all logistical arrangements can be adjusted. Specialize fieldworkers. If your survey requires a lot of different questionnaires (e.g. in school surveys) or special tasks to be conducted (e.g. medical samples), it is often too much material for all interviewers to learn everything. Instead, divide the field teams into different roles that each completes a different set of questionnaires/tasks. During the training, those questionnaires/tasks can be trained in parallel sessions, allowing for more in-depth training. Train field supervisors data monitors or similar roles outside of the hours of the interviewer training, so they can attend the interviewer training in full and know everything interviewers are supposed to (not) do. Typical time slots are towards the end of the training once supervisors have been selected during the late afternoon or evenings, or during administrative days or rest days. Schedule classroom days after field days so that you can hold in-class debrief sessions with the big screen. Below is an example training and piloting schedule for a 1hr-long questionnaire collecting information on agriculture and a few other socio-economic indicators. The schedule allows for practicing of learned material, changes modality frequently, and includes daily quizzes and feedback sessions. Day Session Hrs Location 1 Welcome &amp; introduction to the project 1.0 on-site NA Introduction to Survey Solutions Interviewer 1.0 NA NA Questionnaire: A: HOUSEHOLD 1.0 NA NA Questionnaire: B: AGRICULTURE &amp; key concepts/definitions 2.0 NA NA Quiz A 1.0 NA 2 Review Quiz A (0.5 - 1 hr) 0.5 on-site NA Questionnaire: B: AGRICULTURE 2.0 NA NA Questionnaire: C: LIVESTOCK 2.0 NA NA Group practicing sections A - C (can be split) 2.0 NA NA Quiz B 0.5 NA 3 Review Quiz B (0.5 -1 hr) 0.5 on-site NA Questionnaire: D: EMPLOYMENT &amp; BUSINESS 2.0 NA NA Questionnaire: E: OTHER INCOME 1.0 NA NA Questionnaire: F: HOUSING 1.0 NA NA Questionnaire: G: SHOCKS 0.5 NA NA Questionnaire: H: NUTRITION &amp; FS 0.5 NA NA Group practicing sections D - H (can be split) 2.0 NA NA Quiz C 0.5 NA 4 Review Quiz C (0.5 - 1 hr) 0.5 on-site NA Questionnaire: I: DISABILITY 0.5 NA NA Questionnaire: J: GROUPS 0.5 NA NA Questionnaire: K: CONTACT 0.5 NA NA Questionnaire: R: RESULT 0.5 NA NA Practicing with respondents on-site 3.0 NA NA Quiz D 0.5 NA 5 Review Quiz D 0.5 on-site NA Questionnaire: S: START 1.0 NA NA Role play: Introduce the study and ask for participation 1.0 NA NA Quiz E 0.5 NA NA Field practice (afternoon, 1 interview per team of 2) 4.0 field 6 Review Quiz E 0.5 on-site NA Debrief field practice 1.0 NA NA Identifying households, replacement protocols 1.0 NA NA Role play: Difficult situations e.g. respond to being asked to leave, ask for income, etc. 1.0 NA NA Responding to rejected interviews 2.0 NA NA Field worker and data monitor selection NA NA 7 Rest day NA NA 8 Pilot day 1 (at least 1 interview per interviewer) 6.0 field NA Debrief 1.0 NA NA Afterwards: Supervisor Training 2.0 on-site 9 Pilot day 2 (at least 1 interview per interviewer) 6.0 field NA Debrief 1.0 NA NA Afterwards: Supervisor Training 2.0 on-site 10 Debrief field test NA on-site NA Administrative day 4.0 NA NA Supervisor training 4.0 NA You can download the sample schedule as an Excel from here. 12.3 Material Come prepared. Try to prep training materials as much as possible to have more time to focus on training. Questionnaires. often not sufficiently tested, not clear. Make sure they have been tested. Quiz Manual Data system 12.4 Trainees To ensure that all field workers possess the skills and level of understanding required to collect quality data, it is crucial to train more staff than are needed, which allows for competency-based selection of field workers at the end of the training. See more in Chapter xxx. For a meaningful selection process, train at least 20% more fieldworkers than are required if most of the trainees are experienced and have already worked on very similar surveys, or up to 40% more than required if most are new recruits or have not worked on similarly complex surveys. Often, some trainees will drop out during the training, particularly after day 1 or 2 once there is a clearer understanding of the required tasks. Mitigate the attrition by keeping some of the applicants that have not been invited to the training on standby, so they can quickly replace trainees that have not come back. You might have to bring them up to speed in extra hours and ask them to independently learn already covered material to quickly come up to speed. Alternatively, you can also start with a higher excess of trainees. Learning outcomes quickly decrease as training size increases. If possible, try to keep training size below 50-60 trainees (equivalent to 30-50 field workers), by adjusting the fieldwork plan, so that fewer interviewers work over a longer period of time. See chapter XXX on other benefits for a smaller team. If more than 60 trainees need to be trained, make sure to have additional trainers and break into smaller group sessions where possible. Whenever trainees are separated into groups, put extra effort into preparing the training materials, so all groups receive the same inputs. Also set up a functioning feedback cycle, so that all trainees benefit from relevant feedback, comments, questions, etc raised in one group and that any updates are communicated to all trainees. The same applies for repeated training, or training split otherwise, e.g. regional training. Trainees should attend all training days and must catch up in other ways if it has been inevitable to miss one session or day. Especially when working with unknown teams of trainees, it is beneficial to not determine roles up-front, but to select trainees into field worker roles (interviewer, supervisor, etc) based on their skills and knowledge. Long term supervisors sometimes develop a sense of entitlement or superiority, dont pay attention as much and as a result dont know the material as much as interviewers. Sometimes they also dont hold their position due to other factors. See more on sections FIELD WORKERS and SELECTION. Beware of experienced trainees coming from other surveys. Standards might have been lower in their previous training and they may have developed some undesirable habits that you need to untrain. Also, they might have a stronger sense of I already know that and engage less. Most surveys are significantly different for even experienced interviewers to learn the concepts and protocols. WARNING! In some surveys, there have been walk-outs of trainees, often towards the end of the training, in which trainees collectively refuse to continue the training or to work as field workers unless their demands are met. Usually, there is no time or budget to repeat the training with other trainees, leaving survey management in a very weak bargaining position and putting the survey itself at risk. Avoid walk-outs by setting fair terms and conditions (see details in Section) and explaining them in detail to trainees prior to the training. Ask trainees to sign a copy of the terms prior to the training. 12.5 Trainers Conducting a field worker training well is a substantial amount of work. Apart from leading a training session, there are hundreds of other things that need to happen in parallel, such as updating the questionnaire, translation, CAPI, manual, preparing training sessions or quizzes, compiling feedback, marking quizzes, scoring trainees, observing exercises, etc. The trainer workload can be reduced substantially by having thoroughly vetted instruments [LINK TO INSTRUMENTS] and being prepared well for training [LINK TO CHAPTER PREP]. Even so, having too few trainers will hasten their burn out and almost certainly data quality will suffer as a result. Make sure to provide enough trainers and that they have the capacity and time to engage. For a training with 50 trainees you need at least 2 trainers that can engage full time. If trainers need to do other tasks outside of the training (which is usually the case), cater for more trainers, so that you have at least 2 who are engaged at any point in time. Avoid the situation where trainers are physically present at the table in front of the venue, but stare into their laptops without following the training. The group of trainers should comprise the following capacities: Comprehensive and solid understanding of the instruments, definitions and protocols A good understanding of the indicators construction, the intended data use or analysis. Ideally trainers have been involved in the survey design process. During the training, decisions often need to be made or feedback given about how to treat certain scenarios. Those decisions need to be analytically correct. If survey designers are not present, questions need to be compiled, sent to them and answered quickly, so feedback can be given the following day. Experience in implementing the relevant survey type Experience in implementing surveys in the country and have a good understanding of the country context Experience in implementing interviewer training Speak the training language. Do not make trainers (or trainees) speak a language they are not fluent in. If some trainers (usually survey designers or analysts) do not speak the training language, translate for them. Make sure the group of trainers collectively have the above capacities. There is a tendency to send junior analysts or PhD students without sufficient survey experience to conduct the interviewer training of a survey. This can work, as long as other experienced trainers are present, but quickly leads to quality issues if they are the main training lead. Similarly, handing over a sample and questionnaire to a survey firm and letting them implement the training without close engagement is usually a bad idea. Firms commonly lack the analytical background, do not know about the details of the survey design, are differently motivated, and might not have the capacity to conduct the training to the required standard. If you are the main survey stake holder and are working with a new firm, be present, engage and ensure the training meets quality standards described here. Apart from the technical tasks, there usually is a significant amount of logistical arrangements to be made during a training, ranging from organizing refreshments or transport, setting up tablets, fixing internet, printing, finding respondents, organizing contracts etc. Expect this to take up a good amount of time of the field work manager. Depending on the context, you might need an additional logistical support staff that takes on some of those tasks and ensures a smooth implementation of the training. 12.6 Logistics Training time is precious, yet often it is wasted with logistical issues that could have been addressed in advance. Prepare the venue and logistics prior to the training to allow for a smooth training without delays. For the trainees: Make sure they have learned about the survey details and terms before the training, including the areas to be visited, the population to be interviewed, the transport and lodging arrangements during field work, the length of field work, the interviewer selection process, their remuneration package, insurance, contract details etc. Provide them in written form and have each trainee sign it prior to the training. Schedule any contract signing for a time or day that does not interfere with the training. Pulling trainees in and out during a session reduces its effectiveness. For the training venue: Find a venue with a big hall large enough to fit the entire team and space to breakout into smaller teams for group exercises, such as a garden, hallway, cantine and additional rooms. You will need a projector with a BIG screen. Make sure it is visible to all trainees and that you can project different tablets onto the big screen without any delays. Test the set-up prior to the start of the training. Stable internet connection for all tablets. Trainees have to sync their tablets several times per day. Usually, the WIFI connection of the training venue is not sufficient to connect all 60-70 tablets at the same time. Often, additional mobile network routers or WIFI hotspots are required. One option is to already provide the internet connection that is planned for field work, e.g. the dongles, mobile phone routers or sim cards with data packages. Make sure not to choose a training venue without internet connection or with scarce network coverage. Electricity and extension cords to charge all devices. White board and markers for drawings Microphones (if needed) Arrangement for food and refreshments for the breaks. If food is provided, make sure it is ready in time for the break. If trainees eat out, set reasonable break times that allow trainees to be back in time. Incentivise punctuality, e.g. the last one to come back from break has to sing a song. Sticker for the name tags for trainees. 12.6.1 For CAPI surveys: One device for each trainee. It is crucial for trainees to learn how to navigate the questionnaire and use the device. If you do not have enough devices (since there are more trainees than interviewers) borrow devices or use the private devices of the trainees. Devices must meet the minimum specifications for the software (see here for Survey Solutions). If coordinates need to be recorded, the devices has to have a GPS chip The correct version of the CAPI software installed, with a shortcut/icon on the main screen A soft copy of the manual with a shortcut on the main screen. You can also provide a printed hard copy of the manual, but bear in mind that the manual might be updated during the training A shortcut to the calculator on the main screen You may or may not want to lock other applications of the device using app lockers. The CAPI software is fully set up. For Survey Solutions, this includes: Installation of Survey Solutions on a server Interviewer accounts for each trainee The questionnaire imported Assignments made to each interviewer account. For training, often an open assignment with an infinite quantity (-1) is useful. If the questionnaire relies on more substantially prefilling (i.e. beyond basic information to locate the household/individual), then create several assignments with different scenarios for the prefilling. 12.6.2 For on-site or in-field practicing: Organize reliable transport options and make sure they arrive on time, so the team does not need to wait. Select practice areas/villages well in advance. Make all necessary administrative arrangements like getting permissions, introduction to village officials, etc. [ ] Arrange respondents for field practicing, so trainees can spend more time practicing than looking for respondents to participate For on-site practicing, arrange for respondents to visit the training venue, including the transport, food/refreshment and remuneration. "],["conduct-the-training.html", "Chapter 13 Conduct the Training 13.1 Welcome &amp; introduction 13.2 Introduction to CAPI 13.3 Questionnaire reading 13.4 Group practice 13.5 On-site respondents 13.6 Field practice 13.7 Role plays 13.8 Special tasks 13.9 Responding to inquiries 13.10 Final field test (often called pilot)", " Chapter 13 Conduct the Training This chapter provides details on how to deliver a successful interviewer training that facilitates learning. It is broken into subchapters corresponding to a different session type. In general, consider the following points for the training. Eliminate language barriers. Train in the language trainees are fluent in. Dont force them to speak your language if they do not fully understand it. If you dont speak the local language, either work with an interpreter, or have your local colleagues reiterate your points in the local language, e.g. the fieldwork managers. Support. Especially during the first few days, trainees often get lost in the CAPI or questionnaire and are unable to follow the sessions. Trainers not actively leading a session can walk around the class, check that trainees are in the right place and support those falling behind. Probe for understanding. It is wrong to assume things have been understood if you just ask Any questions? and there are none. There hardly ever are any. Probe using common examples or scenarios. Ask variations of your test questions. For example, think of working profiles/jobs that are typical in the surveyed population and probe trainees if these are wage employed or self-employed. Encourage questions. Often trainees do not dare to ask if they have questions. Make it clear to trainees from the beginning that asking questions is much better than not knowing something during the tests and that they should ask if they have any doubt. During sessions, they can raise their hand and be visited by a trainer. Involve trainees. Asking frequent questions. Let trainees read aloud questions and manuals. Ask them to correct one another. Dont always pick the most engaged trainees but try to involve everyone. Select a trainee using a random name picker such as https://wheelofnames.com, so it can be anybodys turn to answer your questions at any moment. They make it fun if displayed in front of class. Alternatively, select the person that is closest to where the paper ball lands. Select one trainee for a couple of questions to speed up things. Collect feedback. At the end of the day, use feedback forms to collect anonymous feedback from trainees on how clear different sessions or trainers were, topics they did not understand or questions they have. Review feedback forms prior to the next day and adjust your training schedule or methods if needed. Break up and mix seating arrangements. If you start noticing groups of trainees sitting together who are not paying attention or are in the bottom end of performance, distribute them across the classroom. Ask low performers to sit next to high performers, so they can help when falling behind. Essentials only. Only train interviewers on things that are relevant for them and that they are required to do. For example, there is no need for them to learn details about the analysis or experimental design. Likewise, if only supervisors collect GPS coordinates, this is better trained only to them. Confirm with designers/analysts. For solid results, the interpretation of a question must be aligned all the way from how it has been conceptualized to how it is fielded. This is not always the case. Note anything you are uncertain about or is underdefined and run it by designers/analysts on a daily basis. Ask them to be on standby so you receive timely responses. Make it fun! Let the last person entering the venue late and unexcused sing a song in front of the class to help with punctuality (Youtube is full of Karaoke videos). Play fun and exciting group games like Protect the egg or let trainees get to know each other in ice breakers. 13.1 Welcome &amp; introduction Before you start, make sure the training is fully set up LINK PREPARATION. At the beginning, introduce the survey, sketch out the training and field work plan and set the training rules. Keep it short so you can focus on content during this day. Introduction to organization, the survey, the project/study, etc. Do not go into too much detail here and limit it to the essential. Interviewers do not need to know about the details of the analysis plan or the full history of surveys in the country. You can provide more details in the manual and refer trainees to it. Training and field work plan. Run through the training plan, where to be when, when the selection will take place and where and when field work will take place. Importantly, this should be a refresher, NOT be the first time trainees learn details about their pay, other terms or the field work. Make sure all trainees have understood and signed a copy of the general terms prior to the training. Ask trainees to write their name on a name tag and carry it during the training. You have little time to learn who is who and being able to connect names with faces helps to get an overview quicker. Set training rules. You can also print/write them on a poster. Examples: Be on time. Trainees must be on time in the morning and after breaks. Trainers should respect the schedule and not run over time. No phones, no side activities. Trainees must pay attention and should not use their phones or have unnecessary side conversations. You could collect phones and keep them until the end of the day if someone is using them repeatedly. Trainees are responsible for their learning. Trainees can read the manual, practice outside the class, learn from peers etc to make sure they understand everything. It is no problem - and is actually encouraged - to ask questions during or after the training, but it will be a problem to not understand something and not say anything. There is a transparent selection process and they must pass it. Zero tolerance for cheating. This is a high quality survey that collects valid data. Anybody being caught making up data or purposefully breaking protocols will be immediately dismissed. There is nothing to fear as long as one follows what has been taught during training and communicates special cases to the survey management. 13.2 Introduction to CAPI You can train the questionnaire directly in CAPI if you are using Survey Solutions or another CAPI software that provides a good overview over the questionnaire, makes routing behavior clearly understandable and allows for free navigation. This has the advantage that trainees do not need to learn how to navigate in CAPI the questionnaire they learned in a different format. They only learn it in the format they will use in the field, and will practice CAPI along the way, making them much earlier confident in using it. Train CAPI just before trainees need to use it for the first time. If you are training the questionnaire on CAPI, introduce the CAPI software just before the questionnaire reading starts. If you are training the questionnaire on paper, train CAPI just before trainees practice for the first time on the tablets. This chapter is written keeping Survey Solutions in mind, but the same principles apply to other packages. CAPI, as any other software, is best learned by using it. Covering lots of functionality in an extensive theoretical session tends to be very ineffective as functionality that is not immediately put to practice is forgotten and needs to be retrained. It is more productive and a better use of time to initially only train the functionality that is necessary to proceed with the training, and to introduce additional functionality as required throughout the training. For example, trainees only need to learn how rejected interviews work once they need to deal with it in the latter part of the training. Limit the functionality you train to what is relevant from the perspective of an interviewer and what will be used in the actual survey. Trainees do not need to learn details about the supervisor interface or central server. The fewer slides you have in the CAPI training and the more demo and practice you do the better. Train CAPI functionality by demonstrating it on the big screen and asking trainees to follow on their own device. Make sure the class can follow and wait for those falling behind to catch up. Those lost should check with their neighbors or raise their hand, so one of the trainers can come to assist. If training the questionnaire directly in Survey Solutions, train on the following in the initial introduction to CAPI: A general overview, interviewer server, assignments, files, marking as complete, synchronizing to the server. Keep this part short. How to log-in to the Interviewer app. Make sure tablets and accounts have been set up prior to the training. How to create an interview file from their assignment. The general overview of the application and how to navigate through the questionnaire. The outline of a question, the question number, the question text (what to read and what not), the instructions, the answer options (different by type). Question enablement. It is sufficient to work on the first section of the questionnaire. The complete screen, marking an interview as complete. The started and completed screen on the dashboard, reopening an interview file, discarding an interview file. Having completed the above steps, trainees should be set-up to follow the questionnaire during the questionnaire reading sessions. The remaining functionality is best trained throughout the training on an as needed basis. Train: Different question types and roster forms during the questionnaire reading sessions, e.g. list questions in the household member module, yes/no questions in assets, etc. Trainees need to recognize and understand the difference between single select, multi-select, numeric, list and date questions, and understand the rosters. How to synchronize to receive assignments, create new interview files, review interviews on the complete screen, marking them as complete and synchronize to send a file. If you are conducting the tests LINK on Survey Solutions, a good moment to train this is just prior to the first test, as interviewers will have to synchronize to get the assignment and submit their test. How to add comments in the CAPI should be reviewed prior to group practice or practicing in the field, so that trainees can leave questions and comments directly in the questionnaire when practicing. How to receive rejected interviews, respond to comments, etc after the field test or the dress rehearsal, so trainees can practice it directly in interviews they have created. See details LINK TO SECTION. 13.3 Questionnaire reading During the training, all questionnaires must be read and discussed in full, including all related definitions and concepts. Questionnaire reading sessions can be extremely dry and a waste of time if implemented badly. Follow below points to ensure the sessions are fruitful: Allow for enough time. Do not underestimate the required time. Some survey firms, especially when coming from opinion surveys, only allocate a few hours for a single quick read of the questionnaire. This is not sufficient for complex socio-economic surveys where interviewers need to understand a wide range of concepts, definitions, and scenarios to correctly administer the questionnaire. Avoid long blocks of questionnaire reading. They are an information overload and make it impossible for trainees to internalize the content. Spread the questionnaire reading over several days and let trainees regularly practice the newly learned material in between. Apply the theory. Definitions for survey concepts such as household, wage employed, self-employed, parcel, etc. are very theoretical and hard for trainees to translate into the real world. Illustrate them with examples and scenarios to make them tangible for trainees. Keep the content together. Explain concepts and key definitions as they come up in the questionnaire, so trainees immediately learn how to apply them to the questionnaire and can easily remember them. Covering all definitions and concepts up front is abstract and out of context. Avoid chalk and talk, i.e. trainers speaking or presenting in front of the class without much interaction. Involve the trainees as much as possible for example by making the questionnaire reading a conversation. Select a trainee to read aloud the question, answer options and the manual entry. Ask them to explain in their own words what they have understood. Correct them if needed. Ask the rest of the class for input. Probe the trainee with an example or scenario. Train using the Interviewer Manual (LINK). You must get trainees used to consult the manual to get additional information or to clarify doubts, or the manual will never be used. Ask trainees to keep the manual open on paper or as softcopy on their device. After reading each question, ask trainees to check if there is a corresponding manual and read it. Assign one of the trainers to update the manual directly during the training if the entry is unclear or insufficient. Ask for input from the class if needed and confirm with them if the update is clear. Distribute updated soft copies of the manual to the team every morning/evening. Use your time effectively. There will always be a few rare cases in the population that the questionnaire does not cover well. Trainees love pointing those out, e.g. But what if .? Get a feeling if these are frequent enough that they require you to update the questionnaire. If not, tell trainees to leave comments in the questionnaire if the situation arises in the field, and move on with the training, so that you have enough time to focus on the important concepts. If longer discussions arise on a certain point, clarify and stop the discussion. If you are not sure how to answer, get back to them the next day after having read up or consulted with the design team. Train the questionnaire directly in CAPI without training on paper first, if you use Survey Solutions or another CAPI software that provides a good overview of the questionnaire, makes routing behavior clearly understandable for interviewers and allows for free navigation (Do NOT do this with software that displays one question per screen). Training first on paper requires trainees to learn the instrument in two formats and often delays trainees becoming confident with the CAPI version. By training directly in CAPI, trainees only learn the questionnaire in the interface used in the field and learn from the beginning how to use and navigate it. It also reduces the material that needs to be covered. If the questionnaire is administered in other languages than the training language, display the questionnaire in the training language and ask interviewers to look at it in the translated local languages. For each question, ask it to be read in the training and local languages. This serves to verify the translation and to familiarize trainees with the questionnaire in the local language. Update the translations if anything needs correction. Ask the class for inputs or a dedicated person to help if there are many issues slowing down the training. 13.4 Group practice Frequent group practice sessions are important for trainees to familiarize themselves with the questionnaire, practice the use of CAPI, learn from peers, and to break-up the monotony of questionnaire training. Hold group exercise every day after new theoretical content has been introduced, so trainees can put it to practice and internalize it better. On days that are theory heavy, such as the questionnaire reading days, do not wait until the end of the day or until the questionnaire has been covered fully. Instead hold 2 or 3 practice sessions per day at natural breaks (e.g. between sections) to practice the material/sections covered just prior. The change of training modality helps to maintain attention levels. Immediately repeating material helps trainees to understand and internalize it and to uncover any questions. Practicing more frequently becomes infeasible as each practice requires time to setup. Group exercise can be effective for pairs of trainees or small groups of 3-8 trainees. Change group composition between exercises. When allocating trainees to groups, mix trainees and break up clusters that formed in the class, so that low/high performers and those paying more/less attention are distributed. Try to be efficient and not to waste time when creating groups. For pairs, ask trainees to practice with their peers in the row in front/behind. For groups, work out the number of groups n needed, and repeatedly count loud from 1 to n, pointing at a trainee for each number. Each number corresponds to a group, and each number/group can meet in a different point of the training venue, e.g. All number 4s, meet at the blackboard. Ask groups to practice the questionnaire sections that have been covered since the last practicing session. If there is time, groups can also practice all sections learned to date. They should not work through parts of the questionnaire that have not been covered yet. Give scenarios for trainees to ensure they practice scenarios that are likely to occur in the local context and cover all parts of the questionnaire. For example, when practicing the labor section, ask to complete it for an employed individual, self-employed, supporting family worker, etc. Within each group, one trainee should read and ask the questions while another one answers them. After a few questions/sections the roles should be rotated, so each trainee had the opportunity to act as an interviewer or respondent. All trainees in a group should follow on their own tablet and record the answers, which at the end should be identical for all trainees within a group in theory. Ask groups to submit their interviews after the feedback session to motivate them to get it correct. Feedback after practicing sessions tends to be thin. Asking the class if there were any doubts, questions or comments often results in general silence. In order to receive better feedback, ask groups to actively come up with feedback points, note them on a sheet of paper including the trainees names, and to submit those to the trainers at the end of the exercise. Feedback points can be any question or doubt they have about questionnaire, definitions, scenarios, etc or any mistakes they have found. You can set a minimum number of feedback points, e.g. 3 during the beginning of the training when things are not clear yet, and 1 towards the end of the training. Trainers should observe groups that have been allocated to them, correct mistakes and help struggling trainees on the spot, but also take note of the issues they observed for the feedback session. They should consider any issues, including the way questions were read or answered, the intonation, misunderstandings, questionnaire navigation, CAPI use etc. While observing, trainers should also record trainees skill levels (see Chapter SKILLS) and give individual feedback where necessary. Debrief with the entire class immediately after the exercise. Ask groups for their written feedback and discuss and answer their questions in class involving input from other teams/trainees. Ask trainers to provide the feedback from their observations, explaining what was done wrongly and also how to do it correctly. Ask trainees who have been observed to do something well to demonstrate to the class (e.g. how to explain a concept/question), so that all can learn from good examples. 13.5 On-site respondents While group practicing is a good way to get trainees familiarized with the instrument, the scenarios and concepts remain hypothetical and theoretical until trainees learn to apply them with actual respondents. The first exposure to the real-world often is during field testing, which can have some logistical downsides: It is time consuming to move trainees to the field and allocate them to respondents willing to participate. Also it is impossible for trainers to observe more than a handful of trainees who are scattered across many homes. Practicing with real respondents on-site is a great way to give trainees more and early exposure to field conditions, and for trainers to observe all trainees conducting interviews. The best timing for an on-site practice is after the questionnaire has been covered completely and be practiced in group practicing, and prior to the first field test. Hire a group of respondents to visit the training venue or some other convenient location to be interviewed there. Depending on the number of trainees, some 5-10 respondents are usually enough. They should be similar to respondents in the surveyed population, e.g. cultivate land and keep livestock in agricultural surveys, or have jobs or small businesses in labor force surveys. Make sure to compensate respondents adequately, pay for the transport and refreshments or to invite them to the training lunch should it be provided. For the practicing, group trainees into as many groups as you have respondents, ideally not more then 4-6 interviewers per respondent. Each group should conduct a full interview with the respondent. Each trainee should follow on their own device and record answers, and they should take turns in asking the questions, e.g. a few questions or a section at a time. Groups should support each other if individuals fall behind, have questions or make mistakes. Ask groups to submit their interviews after the feedback session to motivate them to get it correct. If there is sufficient time, groups can switch respondents and conduct a second or third interview. Collect feedback, observe and debrief as described above in Chapter GROUP PRACTICE. 13.6 Field practice The purpose of field practice is for trainees to get first experience in implementing the questionnaire with real respondents. There is no need to completely mimic field conditions as one does in dress rehearsals. Rather, aim to maximize the number of interviews each trainee can conduct and the number of respondents and scenarios they are exposed to. Prepare them well, observe trainees and collect good feedback to make them a success. Field practice sessions should be held after the questionnaire has been trained and practiced in class, and before the final dress rehearsal at the end of the training. Often it also makes sense to conduct it prior to sessions on household tracking, receiving rejecting interviews, logistics etc, so that content is broken down more and the field test can focus on the understanding and implementation of the questionnaire only. If the questionnaire is really long, e.g. a LSMS-ISA style questionnaire with long household and agricultural questionnaires, you probably want to conduct more frequent field practices for different parts. Field practice often is implemented inefficiently with a lot of time being spent on logistics that could have been prepared in advance: Teams wait for transport, for community introduction, look for respondents, etc. It is not uncommon that a whole day is spent for trainees to conduct one 1 hour interview in the field or that trainees could not practice as they did not find respondents. Prepare well, so trainees can practice interviewing, not waiting. Visit communities that are not too far from the training venue and large enough to have enough respondents for all trainees. Communities should have similar characteristics to the surveyed communities, but be outside of the sample. Contact or visit the communities the day prior to the field practice, do the community introduction and arrange for respondents at the time of your expected arrival, e.g. with the help of a community leader. Compensating respondents for their participation often helps with their availability and willingness to participate and is totally fine during field practicing. Again, the aim is for trainees to practice the questionnaire as much as possible with respondents. Try to minimize other logistical issues that could cause delays. Make sure all tablets are fully charged, synchronized and have battery packs available if needed. Arrange for food and drinks for trainees in the field or ask them to bring it. Make sure your transport arrives on time, knows the way and does not need to stop for petrol. Try to arrive early in communities, as respondents tend to become unavailable during lunch hours. Group trainees in pairs of 2, trying to take skill levels into account and pairing high with low performers. Allocate each pair with a respondent. Agree on a meeting place for trainees to return to and a time by when they need to be back latest. Ideally, there is time for more than one interview per pair. Similar to the on-site group practices, trainees should take turns interviewing the respondent and both record all answers and submit the interview file at the end of the day. Ask them to write down any issues they experience while they conduct the interview. Working in pairs, one of them can take notes while the other one is interviewing. For each interview, each pair should list at least one question or doubt they had, and something about their interview or respondent that was noteworthy and is useful to share with the class. Try to observe as many of the pairs as possible, help and correct where necessary, mark their skills (LINK), and note down any issues you have observed as general feedback for the class. Focus on trainees that are borderline of being selected or that have not been marked yet. Collect as much feedback as possible from trainees before you leave the community. It becomes much harder later and the quality of the feedback decreases rapidly. As pairs of trainees arrive back at the meeting place, ask for their written notes, debrief quickly with each pair, clarify any issues they had, and record general feedback for the class. After each field test, hold a general debrief session with the entire class in which you respond to the written feedback from the team, answer any questions, and provide feedback from your observations. This might involve short retraining of concepts or questions that have been ill understood, practicing or role plays. Clarify all issues prior to the next field test. It is normal for quite a few things to go wrong in the first preptest and to improve quickly. Sometimes, you can debrief to some extent directly in the field in an empty classroom, a community centre, the shadow of a big tree, etc. This way interviews are still fresh in everyones head and there is no need to go back to the classroom after the transport back from the field. If you are required to debrief the same day in the classroom, make sure to leave the field in time and to allow for a long break, as everyone tends to be really tired after field practicing, especially after the transport back. Design your training schedule such that field practicing days are followed by classroom days, so you can have a detailed debrief session in the classroom with the big screen and other all required facilities. WARNING! Often survey managers assume that a question/module/protocol is working well and that the team is implementing it correctly because interviewers did not report any issues during feedback sessions. This assumption is wrong. Many issues cannot be identified by interviewers, are forgotten or not reported. Those reported are biased towards trainees concerns, e.g. they are quick to report something that caused extra effort from their side. Survey designers, analysts and trainers must observe interviews and compile their own feedback. To get meaningful feedback from trainees, trainees must write down issues or questions as they experience them and must be debriefed shortly after conducting interviews. 13.7 Role plays Trainees must master a range of interviewing skills that are crucial to collect good quality data. Examples are: The way interviewers introduce themselves, the survey and ask for participation is a big determinant of response rates and the perception of the survey and the organization in the population. Part of this is sometimes written up in consent statements, but in reality interviewers interact with respondents before they get to this point. It is important to get this interaction right and standardized across the team. Socio-economic surveys can be complex. Often respondents dont know the answer on top of their head, did not understand or misinterpreted the question or might digress and talk about something else. Interviewers need to learn how to inquire correctly to get the answers they need and lead the conversation. In long interviews respondents often respond fatigue kicks in. Interviewers need to learn how to keep respondents mentally engaged and prevent them from stopping the interview. Most of those skills are about the correct wording, attitude, intonation of voice, posture or gestures, etc. To learn, interviewers have to see and hear what they should and should not do, and learn from positive examples, e.g. from other trainees that are doing something very well. Interviewing skills are best learnt and practiced in role plays. For a role play, explain a scenario, what they can and cannot do, any tricks there are to do it well or any things they must avoid. Select a trainee (more on selection, see LINK SELECTION) to act as interviewer and if needed one to act as respondent. Often it is more useful for the trainer to act as respondent as it allows you to better drive the conversation. Ask selected trainees to come to the front or just stand up and speak towards the class, which is often faster. Put them into the context of the scenario and ask them to act as if you were a real respondent, speaking loudly so the entire class can hear. Give them feedback on what they have done well and what not. Ask the class for inputs. Not only does it make it more interactive and fun, but often the best responses depend on the culture and context. If what the trainee said could be improved, ask them to do it again, collect and give feedback and repeat it until the trainee does it well. Practice with as many trainees as possible or until the scenario generally works well. This can be quite a lot of fun and is a good way of breaking longer theoretical sessions. Some examples. To practice introduction, select one trainee, ask them to stand up, tell them that they just knocked at a door, and that they should introduce themselves and ask for participation. Check that they are giving all necessary information, that they say nothing wrong, how easy they are to understand, etc. Let the trainee repeat their introduction until it is correct in content and well presented. To practice convincing respondents to participate, ask interviewers to introduce as above, but this time act as a respondent giving responses that are commonly used by people not initially willing to participate, such as I am busy now or What do I get out of this?. With the class, find good ways to respond to each of them and practice with trainees how to respond to them and Practice difficult situations that come up in interviews, such as How much longer is this going to take? I really need to do , parts of the questionnaire that require inquiring, such as I dont know how many square meters one MELGA is or scenarios that you have observed in interviews during practicing or had feedback about. 13.8 Special tasks For some surveys interviewers need to learn a special task that requires repeated and structured practice until they can perform it correctly and on auto pilot. As a trainer, you need to identify trainees who are struggling with the task and be aware of scenarios that cause issues. Examples for special tasks are: Plot size measurement Collecting samples, e.g. to measure water quality Behavioral games Extracting information from registries, records, etc Anthropometry Typically, these tasks require interviewers to strictly implement a protocol, use additional tools or aids or understand differently structured documents. Explaining the theory and going through one example is not enough. You need to practice in a structured way and evaluate trainees, exposing them to different scenarios if necessary. The best way of doing this depends on the task at hand. One option is to run through scenarios together so all trainees practice the same thing at the same time. For example, if interviewers need to record details from school time tables, display a picture of an actual timetable from a school on the big screen, and ask trainees to record the required information on their tablets. Run through examples of different time table formats, so interviewers get used to different scenarios. Walk through the class and identify any trainees who are struggling. Provide feedback and discuss in the wider class if there is any confusion. If interviewers need to perform a task on automatic pilot, for example, operate testing equipment, ask the class to repeat it over and over until everyone can perform it correctly all the time. Another option is to set up workstations that interviewers need to go through. Set up different scenarios in or around the venue and staff some with trainers if necessary. Trainees go from station to station and complete the respective task. At the end, each trainee should have undergone the same scenarios and a general debrief can be held. Trainers can observe each trainee at their respective station, evaluate and provide personal feedback. Set up stations such that interviewers spend their time practicing, not queuing. Examples for workstations can be plots with different shapes and cultivation, trainers playing common scenarios in behavioral games, or different documents that need to be evaluated. 13.8.1 Specialized teams If a task is very complicated, requires special background knowledge or if there is not enough time to train it well to all interviewers during the training, field work model permitting, it is often best to specialize some fieldworkers to exclusively conduct the task and distribute them across the field teams. Examples where this tends to be useful are: Anthropometrics, especially when taking all measures, i.e. weight, height/length and middle upper arm circumference (MUAC) Learning outcome assessments used in school surveys that include reading, writing and math tests with strict administration protocols. Medical tests, such as taking blood pressure or measuring hemoglobin values. This often requires the use of specialized equipment and trained nurses. Training any of those tasks can take several days. For example, learning outcome assessments require trainees to handle test booklets, CAPI and a timer all at once while keeping a young child engaged. It usually takes days to practice until all trainees can conduct the assessment tests in a homogenous way, and not everyone is good at it. Train selected trainees in separate sessions in parallel to the interviewer training. Practice all components of the task, including the equipment use, measurement reading, and interaction with the respondent. Invite practicing respondents to the training venue to practice the human component and create closer to field conditions. For more details, see section Inhouse respondents. For anthropometrics, the components that need to be practiced include the correct setup of scales and height/length boards, taking the measurements, how to treat the children/babies, reading the measurements (careful, some are dislexic), and recording them. At the beginning, the measurements taken for the same person differ quite a bit between trainees. Practice until they converge after a few days. 13.9 Responding to inquiries In some surveys, interviewers receive feedback or inquiries about interviews they have submitted that require them to take some action or respond. In Survey Solutions, this is done using rejected interview files. This often goes wrong in surveys and needs to be covered in the training if implemented in the field. There is a big risk that interviewers fix interviews by simply changing answers to make issues go away, without actually addressing the underlying problem. Interviewers need to understand what they must and must not do. Train and practice in a dedicated session towards the end of the training. Ideally, trainees can practice with actual examples from interview files they have produced themselves. This makes it less theoretical and shows trainees that they made mistakes and what they were. You can use interview files from the field test (LINK) or even better, the final dress rehearsal (LINK), as interviews will generally be of better quality. This implies that the session needs to be held towards the very end of the survey. Prior to the session, carefully review submitted interviews and identify any issues. Normally, at the beginning there tends to be at least one per interview, make up some if there are none. Write comments and feedback as would be written during field work and reject the interviews to the interviewer. Try to do this with at least one file per trainee/pair. During the session, first give a quick general introduction to explain the rejected files tab on the interviewer dashboard, how to open a rejected file, how to read the rejection comment, how to see all comments by the supervisor, how to react to rejected files, respond to comments and resubmit them by marking them as complete and synchronizing. Then, project a rejected interview to the main screen and ask the respective trainee/pair to come to the front. They should explain what led to their mistake and what they should do if this occurred in the field. Ask them to respond to the issues and resubmit the file on screen. Ask the class to help and comment. Repeat this with as many files/trainees as possible and useful. Ask all trainees who did not solve their file on the big screen to solve it on their tablet and resubmit them. This exercise is also a great way to identify and streamline the rules around responding to rejected files, revisiting respondents, etc. For more information see chapter REJECTING FILES. 13.10 Final field test (often called pilot) In a dress rehearsal all survey questionnaires and protocols are tested under real-field conditions prior to field work. It is an important exercise to: Expose interviewers to real conditions and give them the opportunity to practice without affecting the sample Determine if the field team is ready for field work Identify and address any outstanding issues with questionnaire or field procedures Some surveys do not conduct a separate pilot prior to the training (link chapter PILOT) and refer to the dress rehearsal as pilot. In those cases, the dress rehearsal importantly serves as a final reality check for questionnaires and protocols, since they have not been tested in full in field conditions before. The dress rehearsal should be conducted at the end of the training and prior to the field work. Usually, 2-3 days of dress rehearsal are sufficient. If short debriefing/feedback sessions can be held in the late afternoon of each day, they can be on sequential days. If not, schedule debriefing/feedback days between field days. Allow for at least one class room day after the dress rehearsal for an extensive feedback session and to teach the class on how to deal with rejected files / inquiries from HQ. Additionally, you might need to allow for additional time for admin, rest days or travel to the field location. To create as close conditions to the field as possible, the dress rehearsal must be conducted with units that are similar to those in the survey sample, but crucially that are not part of the sample. For household surveys, this usually means conducting the dress rehearsal in un-selected communities within the survey areas and often requires the team to travel from the training location closer to the field. For surveys part of RCTs, this means practicing within the treatment arms. Ideally, locate the dress rehearsal such that the entire field team can be supervised and debriefed together in a central location. This ensures that all field workers have received the same inputs and reduces team effect. Split the dress rehearsal into different locations if there is a large variability in the sample that affects questionnaires or protocols. If doing so, try to streamline as much as possible the supervision and feedback given to all groups, e.g. by trainers exchanging notes before providing feedback to their respective group. Often it is easiest to select a set of units (e.g. communities) for the dress rehearsal during the sample design and have them undergo the same preparation steps as the sampled units, e.g. assignment of IDs, procurement of reference data, add them to the CAPI questionnaire, do community sensitisation, etc. For surveys with a separate listing exercise or panel surveys, conduct the dress rehearsal for all stages using the same units and keep the data, so you can properly prepare and test each stage of the survey. It can be very laborious and cumbersome to prepare separate data. One important aspect of dress rehearsal is for field workers to practice the selection, identification and tracking of survey units such as households, convincing respondents to participate and to learn how to make appointments and to manage time. Do not pre-arrange respondents as one would do during field practice. Equip interviewers with the same assignments or tracking sheet format they would receive during field work. If you do not have lists or names for the rehearsal units (e.g. you have the household names for sampled communities, but not for practicing communities), send someone to the community before the dress rehearsal to compile lists that you can use to produce the assignments/tracking lists. Allocate interviewers to their expected field teams and let the supervisors assume their management role, including the assignment of cases to interviewers. For the dress rehearsal, often it is beneficial for interviewers to conduct interviews in pairs. This reduces the number of cases required in any of the visited communities, increases the probability of finding available respondents and allows interviewers to help one another. Each pair can conduct multiple interviews per day and take turns in administering the questionnaire. As in the field tests, both interviewers should record all answers in their own tablet and submit the interview in the end. For the dress rehearsal you might need to relax replacement protocols and add additional replacement units, so interviewers can practice conducting some interviews even if nonresponse is high. Make sure field teams arrive early in the communities, so that interviewers have enough time to locate cases and conduct interviews before respondents start cooking/having lunch and become unavailable. As for the field practice, make all necessary logistical arrangements to prevent delays or hungry/thirsty field workers in the field. Collect feedback and debrief as described in chapter FIELD PRACTICE. Ask all interviewers to submit their interviews at the end of the day. Use the data to finalize your data system (link DATA SYSTEM) and quality control system, e.g. by updating the data checks. Review submitted interviews using the standard processes of the quality control system. In addition, visually review as many cases as possible and compile feedback for interviewers and the quality assurance team. The data collected during dress rehearsal is a great way to learn and practice receiving feedback with the team, see chapter feedback. "],["assessment-selection.html", "Chapter 14 Assessment &amp; Selection 14.1 Written Tests 14.2 Evaluating soft skills 14.3 Field worker selection", " Chapter 14 Assessment &amp; Selection 14.1 Written Tests Conduct written tests on a regular basis throughout the training, i.e. daily or every other day during class room days. On days with in-field practicing it is often not feasible to conduct tests, and there tends to be less new theoretical material that can be tested. 14.1.1 Designing tests Tests should probe the understanding of the questionnaire content and manual covered in the training so far, including protocols, definitions, answer options, discussed etc. but also for general skills like calculating the percentage or average if needed for the survey. If possible, tailored tests to the discussion of the (previous) day, so that they also probe for trainees attention. Repeat questions or topics from previous tests that a large share of trainees answered wrongly. Write questions clearly and that they can be clearly answered unambiguously. Good ways to ask questions in the test include: Describe a difficult scenario, followed by a question of the questionnaire, asking trainees to record or select the correct answer. Write statements and ask if they are true or false. List things and ask trainees to select all that apply, e.g. select all those people who would be household members according to a definition, all the jobs that are considered a self-employment, etc Avoid putting many questions with a simple yes/no or true/false answer. They can be answered 50% correctly by just randomly selecting an answer. Include numeric questions and multi-select questions that require a combination of answers to be selected to be correct. Dont make it too obvious. Sprinkle irrelevant or misleading information into the question text or scenarios. Add Dont have enough information as an answer option. For multi-select answer questions, make sure the wrong options are plausible, and occasionally use only correct or wrong options. Avoid open-ended questions, unless you have enough resources to review and score them for all trainees. See here for examples of tests that have been used in some training sessions. IFAD PACKAGE TEST, ANY OTHER? Aim for tests that take 15-30 minutes to answer so you get enough data points while not taking too long. That translates to around 10-15 difficult questions, some with multi-response answers. 14.1.2 Conducting the test Written tests have been conducted in the following ways in interviewer trainings: Test printed on paper. Do not print all tests up front, as they probably need to be updated each day. Print in the training venue or bring a printer. Questions on screen and note answers on a sheet of blank paper. Very flexible and useful for spontaneous tests as it allows last minute updates. Trainees note their name, question numbers and answers on a blank sheet of paper. Questions are read out or displayed on the screen. CAPI. Only works on CAPI software that is quick and easy to code. Write the test as a questionnaire in CAPI, assign it to trainees who complete one on their tablet and send it back. Often you can copy questions from the questionnaire and modify them. The answers are immediately available as data and can be corrected using code. Trainees learn to handle the CAPI tool and submission of interviews. Make sure to have an internet connection for all trainees to sync. Google forms or similar. There are several tools online that can be used to quickly write tests, deploy them on the trainees devices and make the results available. Most of them are online based and require good internet connectivity. To conduct the quiz, disperse trainees throughout the training venue, so that they cannot copy from one another nor talk. For electronic tests, trainees only need their tablet, so can do without a desk. Walk around and invigilate the test to make sure nobody. Help those that have technical issues and provide clarifications (to the entire class) if there is any general doubt about the quiz. Usually, you can allow trainees to refer to the manual or their notes during the test. This reinforces the manual as a reference and probes if trainees can find out the correct answer with the tools they have available during the field. Give sufficient time to trainees to complete the test, even if they have to refer to the manual a few times. Those that take a bit more time are not necessarily worse interviewers. You can set an exact time limit, or just stop the test once most trainees have handed in their test and once there is no more progress among the remaining ones. There are different advantages for different hours of the day to hold the test. If holding it in the morning, trainees are freshest and have had a chance to revise in the evening if they were falling behind or absent, but it is probably best to do the feedback session right afterwards. Holding the test at the end of the day makes good use of the end of the day that often is unproductive, and also stops trainees from disappearing during the day as it effectively is an attendance call. Trainers have the opportunity to mark in the evening and give feedback the following morning, which is a good way to repeat the material of the previous day. Also, you can ask trainees who have completed the test to leave the venue , so they do not have to wait for everyone to finish. 14.1.3 Marking tests There are several options to mark tests quickly. For tests conducted on paper, collect completed test papers and redistribute them to other trainees during the feedback session. Give clear guidance on how to mark each question as you go through the test with the class, e.g. ask to put a tick or x on the left of the question number, and at the end, count ticks and write the number in the top left corner. During the feedback sessions, ask those looking at a particularly wrong answer for a question to read it out together with the name. The trainee who answered wrong can then explain to the class their thinking that led to the answer and be corrected by the class. To get a feeling on how individual questions went, ask for a show of hands of those who marked it as correct or wrong. At the end, collect the test papers, cross check the marking and and record the score for each trainee. For tests conducted on CAPI or any other electronic tool, you can export the data and write a short script to mark the test. The script should for each trainee calculate the score and for each question, give the percentage of correct answers, tabulate wrong answers and list the names of trainees who answer it wrong. Have (a draft of) the script ready, so you can have the feedback session shortly afterwards. In some tools you can specify the correct answers and do the marking directly in the tool. If using Survey Solutions, this can be done by creating variables and displaying the score in supervisor level questions or questions conditional on supervisor level questions. 14.1.4 Providing feedback For each test, hold a feedback session, ideally shortly after the test, but no later than the following morning. Do better than just going through the correct answers. Actively involve trainees who got questions wrong to learn where misunderstandings are coming from and to ensure that the corrections are understood at the end. Involve the rest of the class to correct and explain in their own words. One way to achieve this is to project the test on the screen and work through it question by question. For each question, ask one of the trainees who answered it incorrectly to come to the front, read the question, and answer it in front of class, explaining their reasoning and cognitive steps. The class should not help the trainee. Once answered, ask the class if the answer was correct or wrong, the reasons why and to provide feedback. Step in if the class response is incorrect. Make sure the trainee in front understands why they answer the question incorrectly. Sometimes discussions arise if test questions were ambiguous or if trainees feel like they have been marked incorrectly. Do not let discussion get carried away. Intervene, repeat what is correct and why and move on. If there were any issues with the test question or the marking, you can not count the question towards the overall score. Announcing this usually helps to appease discussions. Make sure to correct any issues in the manual or questionnaire if they were the reasons for the misunderstanding, as they would likely cause confusion again if unaddressed. Keep an Excel sheet or other systematic record that for each test records the total number of points possible, and the points achieved by each trainee. Calculate a total score and ranking. With tests probably not being equally long or difficult between the days, you probably want to calculate the total score as the sum of points achieved over the sum of total points possible. In the sheet, also record marks and comments from observations, see the following chapter. 14.2 Evaluating soft skills Understanding the questionnaire alone does not make a good field worker. Other soft skills are equally important and must be observed during the training to inform a good field worker selection. Examples are the ability to use a tablet and CAPI, friendliness, seriousness, thoroughness, motivation, intonation of voice, etc. Observing those skills with all trainees and evaluating their qualification as fieldworkers during the training requires you to be systematic, produce comparable scores and make good use of all sessions. Doing it just like that without a systematic marking and non comprehensively is not fair and becomes quickly unfeasible to get a comprehensive picture if there are more than a few trainees. There is benefit in identifying as early as possible if trainees do not possess any of the key skills, so you can still address it in the training, e.g. by giving additional extra tablet/CAPI practice sessions for those not comfortable enough using the tablet or software. Want to do skill matrix, i.e. know which trainee has what skills can be used together with written exams for field worker selection Benefits: find right people Identify gaps early, so can do something about it How to create a skill matrix: 14.2.1 Identify key skills To produce a comparable and comprehensive ranking, identify a few key skills that matter most for the project and cannot easily be probed for in the written test. Examples are: CAPI and tablet use Sound of voice, intonation (especially for phone surveys) Reading questions, reading speed Getting respondents to participate Friendliness and likability Determinism (especially if high unit non-response rate is expected) Trustworthiness and reliability (especially if working alone without supervision) Thoroughness Team leading and organizing (if selecting supervisors from trainees) Aim to get a rating for each of the selected key skills for each trainee during the training. Be realistic, prioritize and select only the skills that matter most, max 3 - 5, so you actually manage to score all trainees against them. 14.2.2 Design a scoring system You need to use a common scoring system to evaluate skills across trainees, especially if more than one trainer is evaluating trainees. Prepare a detailed scoring system with descriptions of each score. Make sure trainers know it and refer to it during scoring. Below table shows an example scoring system ranging from 1 to 5: Score Rating Description 5 Excellent, exceptionally mastery Can be expected to perform extremely well. Can act as an exemplary role during training and field work. 4 Very good, above average More than adequate for effective performance. Possess high skill level. No counterproductive behavior or deficiencies. No major deficiencies. 3 Good, acceptable, average Should be adequate for performance requirements. Posses acceptable skill level. No major counterproductive behavior or deficiencies. 2 Weak, less than acceptable Insufficient for performance requirements. Does not possess sufficient skill level. Some counterproductive behavior or deficiencies. 1 Poor, unacceptable Significantly below performance requirements. Does not possess the skill. Counterproductive behavior. Many deficiencies. 14.2.3 Create a skill matrix Create a table with trainees in rows and skills as columns. Each cell should contain how the respective trainee scores in the relevant cell. Color coding helps to quickly generate an overview and identify generally weak skills or under-performing trainees. A quick and easy way of doing this is in a Spreadsheet. See an example here. There are different ways of filling the matrix: Trainers can record scores in their notes and copy them into the final matrix. This is easiest to set up, but cumbersome, difficult to keep track of and reconcile if trainees have been scored more than once for a skill. Print the empty skill matrix for trainers to record their scores. Copy the scores into the final matrix. Slightly better than above, but still cumbersome and difficult to reconcile. Record scores in a Google Form that contains a question to select the trainee name, one optional question for each skill, and a text field for comments. Trainers fill the form on an ongoing basis, rating only skills they have observed and recording comments. Answers are automatically stored in the Google Sheet, including the name of the scoring person and date time. Answers in the skill matrix can be calculated to be the average or last rating. Ideally, the skill matrix also contains the results for every test and the overall score, so you can get a quick overview of trainees performance. If needed, you can calculate an average score across all skills for each trainee and also combine it with the test result to obtain a single score only, but be aware that the individual components might be quite different and best not be aggregated, or might have to be weighted differently. Always also rely on the individual components if referring to the aggregate. 14.2.4 Rate A fieldworker training only provides you with a short amount of time to do a comprehensive rating of trainees skills, so start as early as possible and rate in all phases of the training. During the theory part of the training, engage as much as possible with trainees and score them against skills that you can rate based on the interaction, e.g. you can score their ability to read out questions or intonation of their voice easily during the questionnaire reading sessions. Some skills can also easily be observed during the classroom-based training by trainers not leading the class, e.g. the ability to use the CAPI software or tablets. During practicing sessions, in class or in the field, assign to each trainer a set of trainees to be observed and scored. Trainers should observe a trainee long enough to get a good enough impression to be able to score them against one or more skills, and should then move on to another trainee. Regularly check the skill matrix for completion, and target trainees that you have not evaluated yet. Rate If you can afford it, try that each trainee has been rated by more than one trainer to create a consensus decision. Give feedback to trainees. If you scored them low on a skill, explain why and that they should try to improve it throughout the training. Be aware of changes over time as trainees have (hopefully) acquired skills during the training. Scores from the beginning of the training for one trainee are not necessarily comparable to those for another at the end. Try that your scores are reflective as much as possible of the skill levels at the end of the training, as you are ultimately interested in trainees potential to perform in the field, not their learning curve throughout the training. Reconfirm extreme scores, especially bad ones towards the end of the training, and scores for trainees that are not clearly in or outside of the selection that is merging towards the end. 14.3 Field worker selection Field worker selection should happen at the end of the training and be skill-based, objective and transparent. 14.3.1 Timing Select field workers at the end of the training and prior to the start of field work. This could be before or after the final dress rehearsal and depends on the survey circumstances. Selecting after the address allows for more time to observe trainees and for trainees to show their strength in the field. Trainees that will not be selected and act as a replacement reserve will have had some field experience. On the other hand, selecting prior to the dress rehearsal allows you to form the actual field teams already, so they can practice in their field composition and so that you can make adjustments if necessary. 14.3.2 Criteria The fieldworker selection should only be based on trainees overall test results, skill ratings and other objective characteristics relevant for the project. Do not select based on subjective criteria such as personal liking or criteria that are irrelevant such as having or not having a degree. How you weigh individual criteria highly depends on the project and training outcome and has to be decided on a case by cases basis. Do not exclusively rely on the overall test scores by simply selecting the trainees with the best scores. Do not overvalue or favor experience, as experienced interviewers are not necessarily better. If you select survey teams based on capacity, over time the best interviewers will be the most experienced ones. See Chapter RECRUITMENT for more information. The selection should be comprehensible and justifiable using the applied criteria. For example, if one trainee has been selected over another one who performed better in the tests, there should be objective reasons, such as higher scores in relevant key skills. You can decide to make the selection and criteria public to the trainees after the selection. 14.3.3 Selection process The following selection algorithm tends to work well. You might have to adapt it a little to meet the requirements of your survey: First, discarding trainees with unacceptable performance or skill levels. They should not be used as field workers unless they receive special training after the training to bring them to acceptable levels. Select all special roles among those who score highest in the required special skills. For example, select data quality control officers among those who have well understood the material, are very reliable and thorough. Select supervisors among those who have good leadership and organizational skills, but do not necessarily need to be top of the class in the test results if they are not involved in the data monitoring (see chapter FIELD). Select anthropometrics specialists who did best in anthropometrics training. Rank the remaining trainees based on test scores, skill ratings, and other criteria you might have and select from top. Often you might find that you have too few or too many trainees with adequate skill levels. Ideally you can adjust your field work model accordingly. If there are too few, only select those capable and use fewer or smaller teams for longer, or extend the training to bring them up to speed. If more trainees qualify than required, you can think of using larger or more teams for a shorter amount of time, but be aware that larger field teams are harder to monitor. Allocate selected staff into teams, double check they balance well and adjust steps 2 and 3 if not. For team allocation, keep in mind any special requirements you might have, such as gender balance, languages, etc. Try to balance the skill levels within teams by mixing top and bottom of class, so stronger trainees can support and act as a reference for weaker trainees. Make sure teams know who should learn from whom, and that they should support one another. Also be aware of personal traits and group dynamics when allocating into teams. Keep trainees with adequate skill levels who have not been selected as reserve to replace fieldworkers who drop out during field work. Inform them that they are on a waiting list and might be called should others leave. "],["supervisor-training.html", "Chapter 15 Supervisor Training", " Chapter 15 Supervisor Training ljashd "],["introduction-2.html", "Introduction", " Introduction work in progress "],["listing.html", "Chapter 16 Listing", " Chapter 16 Listing work in progress "],["monitoring.html", "Chapter 17 Monitoring", " Chapter 17 Monitoring work in progress "],["effective-feedback.html", "Chapter 18 Effective Feedback", " Chapter 18 Effective Feedback work in progress "],["avoiding-nonresponse.html", "Chapter 19 Avoiding nonresponse", " Chapter 19 Avoiding nonresponse strong revisiting protocols make it hard for interviewers, remove any incentives, have to give phone numbers confirm, etc follow up on non-repsonse "],["introduction-3.html", "Introduction", " Introduction work in progress "],["data-editing.html", "Chapter 20 Data Editing", " Chapter 20 Data Editing work in progress ISCO codes, best done in batch by dedicated persons. "],["weighting.html", "Chapter 21 Weighting", " Chapter 21 Weighting "],["archiving.html", "Chapter 22 Archiving", " Chapter 22 Archiving work in progress "],["documentation.html", "Chapter 23 Documentation", " Chapter 23 Documentation work in progress - report on Standard errors - ideally "],["abbreviations-and-acronyms.html", "Abbreviations and Acronyms", " Abbreviations and Acronyms CAPI Computer Assissted Personal Interviewing CATI Computer Assissted Telephone Interviewing DK Dont Know LSMS Living Standard Measurement Survey NSO National Statistical Office PAPI Pen-and-Paper Personal Interviews TSE Total Survey Error TSQ Total Survey Quality "],["references.html", "References", " References "]]
