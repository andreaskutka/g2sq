
# Assessment & Selection

## Written Tests 

Conduct written tests  on a regular basis throughout the training, i.e. daily or every other day during class room days. On days with in-field practicing it is often not feasible to conduct tests, and there tends to be less new theoretical material that can be tested.  

Designing tests 

Tests should probe the understanding of the questionnaire content and manual covered in the training so far, including protocols, definitions, answer options, discussed etc. but also for general skills like calculating the percentage or average if needed for the survey.  If possible, tailored tests to the discussion of the (previous) day, so that they also probe for trainees’ attention. Repeat questions or topics from previous tests that a large share of trainees answered wrongly. 

Write questions clearly and that they can be clearly answered unambiguously. Good ways to ask questions in the test include: 

Describe a difficult scenario, followed by a question of the questionnaire, asking trainees to record or select the correct answer. 

Write statements and ask if they are true or false.  

List things and ask trainees to select all that apply, e.g. select all those people who would be household members according to a definition, all the jobs that are considered a self-employment, etc   

Avoid putting many questions with a simple yes/no or true/false answer. They can be answered 50% correctly by just randomly selecting an answer. Include numeric questions and multi-select questions that require a combination of answers to be selected to be correct.  

Don’t make it too obvious. Sprinkle irrelevant or misleading information into the question text or scenarios. Add “Don’t have enough information” as an answer option. For multi-select answer questions, make sure the wrong options are plausible, and occasionally use only correct or wrong options. Avoid open-ended questions, unless you have enough resources to review and score them for all trainees. 

See here for examples of tests that have been used in some training sessions. IFAD PACKAGE TEST, ANY OTHER? 

Aim for tests that take 15-30 minutes to answer so you get enough data points while not taking too long. That translates to around 10-15 difficult questions, some with multi-response answers.    

Conducting the test  

Written tests have been conducted in the following ways in interviewer trainings: 

Test printed on paper. Do not print all tests up front, as they probably need to be updated each day. Print in the training venue or bring a printer.   

Questions on screen and note answers on a sheet of blank paper. Very flexible and useful for spontaneous tests as it allows last minute updates. Trainees note their name, question numbers and answers on a blank sheet of paper. Questions are read out or displayed on the screen.  

CAPI. Only works on CAPI software that is quick and easy to code. Write the test as a questionnaire in CAPI, assign it to trainees who complete one on their tablet and send it back. Often you can copy questions from the questionnaire and modify them. The answers are immediately available as data and can be corrected using code. Trainees learn to handle the CAPI tool and submission of interviews. Make sure to have an internet connection for all trainees to sync.   

Google forms or similar. There are several tools online that can be used to quickly write tests, deploy them on the trainees’ devices and make the results available. Most of them are online based and require good internet connectivity. 

To conduct the quiz, disperse trainees throughout the training venue, so that they cannot copy from one another nor talk. For electronic tests, trainees only need their tablet, so can do without a desk. Walk around and invigilate the test to make sure nobody. Help those that have technical issues and provide clarifications (to the entire class) if there is any general doubt about the quiz.  

Usually, you can allow trainees to refer to the manual or their notes during the test. This reinforces the manual as a reference and probes if trainees can find out the correct answer with the tools they have available during the field.  

Give sufficient time to trainees to complete the test, even if they have to refer to the manual a few times. Those that take a bit more time are not necessarily worse interviewers. You can set an exact time limit, or just stop the test once most trainees have handed in their test and once there is no more progress among the remaining ones. 

There are different advantages for different hours of the day to hold the test. If holding it in the morning, trainees are freshest and have had a chance to revise in the evening if they were falling behind or absent, but it is probably best to do the feedback session right afterwards. Holding the test at the end of the day makes good use of the end of the day that often is unproductive, and also stops trainees from disappearing during the day as it effectively is an attendance call. Trainers have the opportunity to mark in the evening and give feedback the following morning, which is a good way to repeat the material of the previous day. Also, you can ask trainees who have completed the test to leave the venue , so they do not have to wait for everyone to finish. 

Marking tests 

There are several options to mark tests quickly.  

For tests conducted on paper, collect completed test papers and redistribute them to other trainees during the feedback session. Give clear guidance on how to mark each question as you go through the test with the class, e.g. ask to put a tick or x on the left of the question number, and at the end, count ticks and write the number in the top left corner. During the feedback sessions, ask those looking at a particularly wrong answer for a question to read it out together with the name. The trainee who answered wrong can then explain to the class their thinking that led to the answer and be corrected by the class. To get a feeling on how individual questions went, ask for a show of hands of those who marked it as correct or wrong. At the end, collect the test papers, cross check the marking and and record the score for each trainee.    

For tests conducted on CAPI or any other electronic tool, you can export the data and write a short script to mark the test. The script should for each trainee calculate the score and for each question, give the percentage of correct answers, tabulate wrong answers and list the names of trainees who answer it wrong. Have (a draft of) the script ready, so you can have the feedback session shortly afterwards. In some tools you can specify the correct answers and do the marking directly in the tool. If using Survey Solutions, this can be done by creating variables and displaying the score in supervisor level questions or questions conditional on supervisor level questions.       

Provide feedback 

For each test, hold a feedback session, ideally shortly after the test, but no later than the following morning. Do better than just going through the correct answers. Actively involve trainees who got questions wrong to learn where misunderstandings are coming from and to ensure that the corrections are understood at the end. Involve the rest of the class to correct and explain in their own words.  

One way to achieve this is to project the test on the screen and work through it question by question. For each question, ask one of the trainees who answered it incorrectly to come to the front, read the question, and answer it in front of class, explaining their reasoning and cognitive steps. The class should not help the trainee. Once answered, ask the class if the answer was correct or wrong, the reasons why and to provide feedback. Step in if the class response is incorrect. Make sure the trainee in front understands why they answer the question incorrectly. 

Sometimes discussions arise if test questions were ambiguous or if trainees feel like they have been marked incorrectly. Do not let discussion get carried away. Intervene, repeat what is correct and why and move on. If there were any issues with the test question or the marking, you can not count the question towards the overall score. Announcing this usually helps to appease discussions. Make sure to correct any issues in the manual or questionnaire if they were the reasons for the misunderstanding, as they would likely cause confusion again if unaddressed.   

Keep an Excel sheet or other systematic record that for each test records the total number of points possible, and the points achieved by each trainee. Calculate a total score and ranking. With tests probably not being equally long or difficult between the days, you probably want to calculate the total score as the sum of points achieved over the sum of total points possible. In the sheet, also record marks and comments from observations, see the following chapter. 

## Evaluating soft skills 

Understanding the questionnaire alone does not make a good field worker. Other “soft” skills are equally important and must be observed during the training to inform a good field worker selection. Examples are the ability to use a tablet and CAPI, friendliness, seriousness, thoroughness, motivation, intonation of voice, etc.  

Observing those skills with all trainees and evaluating their qualification as fieldworkers during the training requires you to be systematic, produce comparable scores and make good use of all sessions. Doing it “just like that” without a systematic marking and non comprehensively is not fair and becomes quickly unfeasible to get a comprehensive picture if there are more than a few trainees. There is benefit in identifying as early as possible if trainees do not possess any of the key skills, so you can still address it in the training, e.g. by giving additional extra tablet/CAPI practice sessions for those not comfortable enough using the tablet or software. 

Want to do skill matrix, i.e. know which trainee has what skills can be used together with written exams for field worker selection 

Benefits:  

find right people  

Identify gaps early, so can do something about it 

 

How to create a skill matrix: 

Identify key skills 

To produce a comparable and comprehensive ranking, identify a few key skills that matter most for the project and cannot easily be probed for in the written test. Examples are: 

CAPI and tablet use 

Sound of voice, intonation (especially for phone surveys) 

Reading questions, reading speed 

Getting respondents to participate 

Friendliness and likability 

Determinism (especially if high unit non-response rate is expected) 

Trustworthiness and reliability (especially if working alone without supervision) 

Thoroughness 

Team leading and organizing (if selecting supervisors from trainees) 

Aim to get a rating for each of the selected key skills for each trainee during the training. Be realistic, prioritize and select only the skills that matter most, max 3 - 5, so you actually manage to score all trainees against them. 

Design a scoring system 

You need to use a common scoring system to evaluate skills across trainees, especially if more than one trainer is evaluating trainees. Prepare a detailed scoring system with descriptions of each score. Make sure trainers know it and refer to it during scoring. Below table shows an example scoring system ranging from 1 to 5: 


```{r scoring_system, echo = FALSE, warning = FALSE, comment = FALSE, message = FALSE}
library(kableExtra)

score<-data.frame(
Score = c(5:1),
Rating= c(
"Excellent, exceptionally mastery",
"Very good, above average",
"Good, acceptable, average",
"Weak, less than acceptable",
"Poor, unacceptable"),
Description = c(
"Can be expected to perform extremely well. Can act as an exemplary role during training and field work.", 
"More than adequate for effective performance. Possess high skill level. No counterproductive behavior or deficiencies. No major deficiencies.",
"Should be adequate for performance requirements. Posses acceptable skill level. No major counterproductive behavior or deficiencies.", 
"Insufficient for performance requirements. Does not possess sufficient skill level. Some counterproductive behavior or deficiencies.",
"Significantly below performance requirements. Does not possess the skill. Counterproductive behavior. Many deficiencies.") 
)
score %>%
  kbl(align = "cll") %>%
  kable_styling() %>%
  column_spec(1, bold = T)

```

Create a skill matrix   

Create a table with trainees in rows and skills as columns. Each cell should contain how the respective trainee scores in the relevant cell. Color coding helps to quickly generate an overview and identify generally weak skills or under-performing trainees. A quick and easy way of doing this is in a Spreadsheet. See an example here.  

There are different ways of filling the matrix: 

Trainers can record scores in their notes and copy them into the final matrix. This is easiest to set up, but cumbersome, difficult to keep track of and reconcile if trainees have been scored more than once for a skill. 

Print the empty skill matrix for trainers to record their scores. Copy the scores into the final matrix. Slightly better than above, but still cumbersome and difficult to reconcile.  

Record scores in a Google Form that contains a question to select the trainee name, one optional question for each skill, and a text field for comments. Trainers fill the form on an ongoing basis, rating only skills they have observed and recording comments. Answers are automatically stored in the google sheet, including the name of the scoring person and date time. Answers in the skill matrix can be calculated to be the average or last rating.    

Ideally, the skill matrix also contains the results for every test and the overall score, so you can get a quick overview of trainees performance.  

If needed, you can calculate an average score across all skills for each trainee and also combine it with the test result to obtain a single score only, but be aware that the individual components might be quite different and best not be aggregated, or might have to be  weighted differently. Always also rely on the individual components if referring to the aggregate.  

Step 5: Rate 

A fieldworker training only provides you with a short amount of time to do a comprehensive rating of trainees skills, so start as early as possible and rate in all phases of the training.  

During the theory part of the training, engage as much as possible with trainees and score them against skills that you can rate based on the interaction, e.g. you can score their ability to read out questions or intonation of their voice easily during the questionnaire reading sessions. Some skills can also easily be observed during the classroom-based training by trainers not leading the class, e.g. the ability to use the CAPI software or tablets. 

During practicing sessions, in class or in the field, assign to each trainer a set of trainees to be observed and scored. Trainers should observe a trainee long enough to get a good enough impression to be able to score them against one or more skills, and should then move on to another trainee. 

Regularly check the skill matrix for completion, and target trainees that you have not evaluated yet. Rate If you can afford it, try that each trainee has been rated by more than one trainer to create a consensus decision. 

Give feedback to trainees. If you scored them low on a skill, explain why and that they should try to improve it throughout the training. 

Be aware of changes over time as trainees have (hopefully) acquired skills during the training. Scores from the beginning of the training for one trainee are not necessarily comparable to those for another at the end. Try that your scores are reflective as much as possible of the skill levels at the end of the training, as you are ultimately interested in trainees' potential to perform in the field, not their learning curve throughout the training.   

Reconfirm extreme scores, especially bad ones towards the end of the training, and scores for trainees that are not clearly in or outside of the selection that is merging towards the end.

## Field worker selection 

Field worker selection should happen at the end of the training and be skill-based, objective and transparent.  

Timing 

Select field workers at the end of the training and prior to the start of field work. This could be before or after the final dress rehearsal and depends on the survey circumstances. 

Selecting after the address allows for more time to observe trainees and for trainees to show their strength in the field. Trainees that will not be selected and act as a replacement reserve will have had some field experience. 

On the other hand, selecting prior to the dress rehearsal allows you to form the actual field teams already, so they can practice in their field composition and so that you can make adjustments if necessary.  

Criteria 

The fieldworker selection should only be based on trainees’ overall test results, skill ratings and other objective characteristics relevant for the project. Do not select based on subjective criteria such as personal liking or criteria that are irrelevant such as having or not having a degree.  

How you weigh individual criteria highly depends on the project and training outcome and has to be decided on a case by cases basis. Do not exclusively rely on the overall test scores by simply selecting the trainees with the best scores. 

Do not overvalue or favor experience, as experienced interviewers are not necessarily better. If you select survey teams based on capacity, over time the best interviewers will be the most experienced ones. See Chapter RECRUITMENT for more information. 

The selection should be comprehensible and justifiable using the applied criteria. For example, if one trainee has been selected over another one who performed better in the tests, there should be objective reasons, such as higher scores in relevant key skills. You can decide to make the selection and criteria public to the trainees after the selection. 

Selection process 

The following selection algorithm tends to work well. You might have to adapt it a little to meet the requirements of your survey: 

First, discarding trainees with unacceptable performance or skill levels. They should not be used as field workers unless they receive special training after the training to bring them to acceptable levels. 

Select all special roles among those who score highest in the required special skills. For example, select data quality control officers among those who have well understood the material, are very reliable and thorough. Select supervisors among those who have good leadership and organizational skills, but do not necessarily need to be top of the class in the test results if they are not involved in the data monitoring (see chapter FIELD). Select anthropometrics specialists who did best in anthropometrics training.  

Rank the remaining trainees based on test scores, skill ratings, and other criteria you might have and select from top. Often you might find that you have too few or too many trainees with adequate skill levels. Ideally you can adjust your field work model accordingly. If there are too few, only select those capable and use fewer or smaller teams for longer, or extend the training to bring them up to speed. If more trainees qualify than required, you can think of using larger or more teams for a shorter amount of time, but be aware that larger field teams are harder to monitor.     

Allocate selected staff into teams, double check they balance well and adjust steps 2 and 3 if not. For team allocation, keep in mind any special requirements you might have, such as gender balance, languages, etc. Try to balance the skill levels within teams by mixing top and bottom of class, so stronger trainees can support and act as a reference for weaker trainees. Make sure teams know who should learn from whom, and that they should support one another. Also be aware of personal traits and group dynamics when allocating into teams.     
Keep trainees with adequate skill levels who have not been selected as reserve to replace fieldworkers who drop out during field work. Inform them that they are on a waiting list and might be called should others leave.   
